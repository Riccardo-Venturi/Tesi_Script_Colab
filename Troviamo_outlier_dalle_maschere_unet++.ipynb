{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1_5CEkINfJDSmytqBVJiCHOzHGqBc3FWp",
      "authorship_tag": "ABX9TyM1qnz50Thx2ufsbjDythQX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riccardo-Venturi/Tesi_Script_Colab/blob/main/Troviamo_outlier_dalle_maschere_unet%2B%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas tqdm"
      ],
      "metadata": {
        "id": "YoTysUO8oVqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDg4qj0NoOIH"
      },
      "outputs": [],
      "source": [
        "#@title check utlier da rifinire a mano\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIGURAZIONE ---\n",
        "PRED_DIR = Path(\"/content/drive/MyDrive/UNETPPMaschereInferenza\")\n",
        "# Scala di riferimento per i calcoli fisici\n",
        "SCALA_PX_MM = 35.8\n",
        "\n",
        "def screening_anomalie_totale():\n",
        "    pred_files = sorted(list(PRED_DIR.glob(\"*.png\")))\n",
        "    analisi = []\n",
        "\n",
        "    print(f\"üïµÔ∏è Analisi eurisitica su {len(pred_files)} maschere UNet++...\")\n",
        "\n",
        "    for f in tqdm(pred_files):\n",
        "        img = cv2.imread(str(f), 0)\n",
        "        if img is None: continue\n",
        "\n",
        "        # Estrazione metriche \"pure\" dalla maschera predetta\n",
        "        area_foro_px = np.sum(img == 1)\n",
        "        area_danno_px = np.sum(img == 2)\n",
        "        area_tot_mm2 = (area_foro_px + area_danno_px) / (SCALA_PX_MM**2)\n",
        "        ratio_danno_foro = (area_danno_px / area_foro_px) if area_foro_px > 0 else 0\n",
        "\n",
        "        # Calcolo circolarit√† del foro (per vedere se l'AI ha segmentato bene il cerchio)\n",
        "        contours, _ = cv2.findContours((img == 1).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        circ_foro = 0\n",
        "        if contours:\n",
        "            cnt = max(contours, key=cv2.contourArea)\n",
        "            area = cv2.contourArea(cnt)\n",
        "            perim = cv2.arcLength(cnt, True)\n",
        "            if perim > 0:\n",
        "                circ_foro = 4 * np.pi * (area / (perim * perim))\n",
        "\n",
        "        analisi.append({\n",
        "            'File': f.name,\n",
        "            'Area_Danno_mm2': round(area_danno_px / (SCALA_PX_MM**2), 2),\n",
        "            'Ratio_Danno_Foro': round(ratio_danno_foro, 3),\n",
        "            'Circolarita_Foro': round(circ_foro, 3),\n",
        "            'Hole_Present': 1 if area_foro_px > 500 else 0\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(analisi)\n",
        "\n",
        "    # --- DEFINIZIONE ANOMALIE (LOGICA DA INGEGNERE) ---\n",
        "    # 1. Filtro: Fori senza foro (Rilevamento fallito totalmente)\n",
        "    fail_detect = df[df['Hole_Present'] == 0]\n",
        "\n",
        "    # 2. Filtro: Circolarit√† pessima (AI ha confuso sfondo/danno con il foro)\n",
        "    fail_geom = df[(df['Hole_Present'] == 1) & (df['Circolarita_Foro'] < 0.7)]\n",
        "\n",
        "    # 3. Filtro: Esplosione del danno (Area > 50mm2 √® quasi certamente un errore sui bordi)\n",
        "    fail_area = df[df['Area_Danno_mm2'] > 50.0]\n",
        "\n",
        "    print(\"\\n\" + \"!\"*40)\n",
        "    print(f\"üìâ SCREENING COMPLETATO\")\n",
        "    print(f\"‚ùå Rilevamenti Falliti: {len(fail_detect)}\")\n",
        "    print(f\"üìê Errori Geometrici (Foro storto): {len(fail_geom)}\")\n",
        "    print(f\"üå™Ô∏è Possibili Allucinazioni (Danno eccessivo): {len(fail_area)}\")\n",
        "    print(\"!\"*40)\n",
        "\n",
        "    # Crea un DataFrame degli \"indiziati\" per QuPath\n",
        "    anomali = pd.concat([fail_detect, fail_geom, fail_area]).drop_duplicates().sort_values(by='Area_Danno_mm2', ascending=False)\n",
        "\n",
        "    return anomali\n",
        "\n",
        "df_anomalie = screening_anomalie_totale()\n",
        "\n",
        "# Visualizza i primi 50 casi strani\n",
        "display(df_anomalie.head(50))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec3282ae"
      },
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Define the root directory where the raw radiographs are stored (including subfolders)\n",
        "RAW_PATCHES_DIR = Path(\"/content/drive/MyDrive/Radio_Patches_Normalized\")\n",
        "\n",
        "# Define the expected extension for the raw images (assuming .png like your masks)\n",
        "RAW_IMG_EXTENSION = \".jpg\" # <--- ADJUST THIS EXTENSION if different (e.g., .jpg, .tiff)\n",
        "\n",
        "# Ensure the source directory exists\n",
        "if not RAW_PATCHES_DIR.exists():\n",
        "    print(f\"Error: The specified raw image directory '{RAW_PATCHES_DIR}' does not exist. Please check the path.\")\n",
        "else:\n",
        "    print(f\"Copying corresponding raw image files from '{RAW_PATCHES_DIR}' to '{RAW_IMAGES_DIR}'...\")\n",
        "\n",
        "    copied_count = 0\n",
        "    not_found_count = 0\n",
        "\n",
        "    for index, row in df_anomalie.iterrows():\n",
        "        mask_filename = row['File']\n",
        "        # Extract the base name (e.g., 'H064', 'H574') from the mask filename\n",
        "        base_name_to_match = Path(mask_filename).stem.split('_')[0] # Assuming 'HXXX' is always the first part\n",
        "\n",
        "        found_raw_image = False\n",
        "        # Search recursively for the corresponding raw image\n",
        "        for raw_file_path in RAW_PATCHES_DIR.rglob(f\"*{base_name_to_match}*{RAW_IMG_EXTENSION}\"):\n",
        "            # A more robust check might be needed if multiple files match the pattern\n",
        "            # For now, we'll take the first one found.\n",
        "            source_raw_image_path = raw_file_path\n",
        "            destination_raw_image_path = RAW_IMAGES_DIR / raw_file_path.name # Keep original filename\n",
        "\n",
        "            if source_raw_image_path.exists():\n",
        "                shutil.copy(source_raw_image_path, destination_raw_image_path)\n",
        "                copied_count += 1\n",
        "                found_raw_image = True\n",
        "                # print(f\"Copied: {raw_file_path.name}\") # Uncomment for verbose output\n",
        "                break # Stop after finding the first match\n",
        "\n",
        "        if not found_raw_image:\n",
        "            not_found_count += 1\n",
        "            print(f\"Warning: Corresponding raw image file not found for mask beginning with '{base_name_to_match}' (mask: {mask_filename}) in '{RAW_PATCHES_DIR}'.\")\n",
        "\n",
        "    print(f\"\\nCopying process complete: {copied_count} raw images copied, {not_found_count} not found.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1942801f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Define the colormap once\n",
        "colors = ['black', 'blue', 'red']\n",
        "cmap = mcolors.ListedColormap(colors)\n",
        "bounds = [-0.5, 0.5, 1.5, 2.5] # Define boundaries for each color\n",
        "norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "# Define a directory to save the visualized images\n",
        "VISUALIZED_MASKS_DIR = Path(\"/content/drive/MyDrive/Segmentation_to_study_special_cases/visualized_anomalous_masks\")\n",
        "VISUALIZED_MASKS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Saving visualized masks to: {VISUALIZED_MASKS_DIR}\")\n",
        "\n",
        "# Loop through the first 13 anomalous files in the DataFrame\n",
        "for index, row in df_anomalie.head(13).iterrows():\n",
        "    example_file = row['File']\n",
        "    example_mask_path = PRED_DIR / example_file\n",
        "\n",
        "    # Load the mask image\n",
        "    mask_img = cv2.imread(str(example_mask_path), 0)\n",
        "\n",
        "    if mask_img is not None:\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        ax.imshow(mask_img, cmap=cmap, norm=norm)\n",
        "        ax.set_title(f'Visualizing Mask: {example_file}')\n",
        "        fig.colorbar(ax.imshow(mask_img, cmap=cmap, norm=norm), ticks=[0, 1, 2], label='Pixel Value: 0=Background, 1=Hole, 2=Damage')\n",
        "\n",
        "        # Define the output path for the saved visualized image\n",
        "        output_filename = example_file.replace('.png', '_visualized.png')\n",
        "        output_path = VISUALIZED_MASKS_DIR / output_filename\n",
        "\n",
        "        # Save the figure\n",
        "        plt.savefig(output_path, bbox_inches='tight')\n",
        "        plt.close(fig) # Close the figure to avoid displaying all of them in the notebook\n",
        "    else:\n",
        "        print(f\"Could not load image: {example_mask_path}\")\n",
        "\n",
        "print(\"Visualized masks saved to Google Drive.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m0js0MALt2eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IY1IJ2zX6YVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CACCIATORE DI ALIENI V3.0 - RELAXED & RANKED\n",
        "# Ti restituisce solo i 25 file pi√π \"brutti\" basandosi su un punteggio di errore.\n",
        "# ==============================================================================\n",
        "def screening_peggiori_25():\n",
        "    pred_files = sorted(list(PRED_DIR.glob(\"*.png\")))\n",
        "    analisi = []\n",
        "\n",
        "    for f in tqdm(pred_files):\n",
        "        img = cv2.imread(str(f), 0)\n",
        "        if img is None: continue\n",
        "\n",
        "        mask_foro = (img == 1).astype(np.uint8)\n",
        "        mask_danno = (img == 2).astype(np.uint8)\n",
        "\n",
        "        # 1. Frammentazione (Punta al rumore \"a sale e pepe\")\n",
        "        num_labels, _, _, _ = cv2.connectedComponentsWithStats(mask_danno)\n",
        "        n_fragments = num_labels - 1\n",
        "\n",
        "        # 2. Circolarit√† (Punta ai fori che non sono fori)\n",
        "        contours, _ = cv2.findContours(mask_foro, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        circ_score = 1.0 # default buono\n",
        "        if contours:\n",
        "            cnt = max(contours, key=cv2.contourArea)\n",
        "            area = cv2.contourArea(cnt)\n",
        "            perim = cv2.arcLength(cnt, True)\n",
        "            if perim > 0: circ_score = (4 * np.pi * area) / (perim**2)\n",
        "\n",
        "        # 3. Area del danno sospetta (troppo grande)\n",
        "        area_danno_mm2 = np.sum(mask_danno) / (SCALA_PX_MM**2)\n",
        "\n",
        "        # --- CALCOLO PUNTEGGIO DI ANOMALIA (Pi√π √® alto, pi√π √® un outlier) ---\n",
        "        # Un foro pessimo (<0.6 circ) + Troppi frammenti (>50) + Area sospetta (>30mm2)\n",
        "        score = (n_fragments / 100) + (1.0 - circ_score)\n",
        "        if area_danno_mm2 > 40: score += 5  # Esplosione di area = quasi sempre errore\n",
        "        if area_danno_mm2 < 0.5: score += 1  # Danno nullo = sospetto\n",
        "\n",
        "        analisi.append({\n",
        "            'File': f.name,\n",
        "            'Area_Danno': area_danno_mm2,\n",
        "            'Frammenti': n_fragments,\n",
        "            'Circ_Foro': round(circ_score, 2),\n",
        "            'Anomaly_Score': score\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(analisi)\n",
        "\n",
        "    # Prendi solo i 25 col punteggio di anomalia pi√π alto\n",
        "    top_outliers = df.sort_values(by='Anomaly_Score', ascending=False).head(25)\n",
        "\n",
        "    return top_outliers\n",
        "\n",
        "df_indiziati = screening_peggiori_25()\n",
        "print(\"\\nüî• LISTA DEI 25 PEGGOIRI FILE DA SISTEMARE CON GIMP/PENNA üî•\")\n",
        "display(df_indiziati)"
      ],
      "metadata": {
        "id": "gC0_Jqmp6YSs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}