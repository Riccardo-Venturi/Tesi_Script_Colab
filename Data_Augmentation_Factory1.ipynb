{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1gpfEaALqhvAjzv_SOk8VIOurp-4wBfD3",
      "authorship_tag": "ABX9TyOri+8yEZKp5czZJckpiNKm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riccardo-Venturi/Tesi_Script_Colab/blob/main/Data_Augmentation_Factory1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0KdZ9l3QJnFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perché la Cella 2 è lenta\n",
        "a) Copia da Drive\n",
        "\n",
        "Il file‑system FUSE di Drive ha throughput ballerino (spesso 5–20 MB/s) e latenza alta\n",
        "GitHub\n",
        ". Copiare centinaia di PNG da 512 px richiede parecchi secondi – ed è lineare con il numero di file. SUGGERIMENTO: esegui la cella una sola volta, poi lascia il dataset ridimensionato su disco locale di Colab o, meglio, su una cartella Drive già preridimensionata e salta la copia con la “cella 2.5” che hai citato\n",
        "Reddit\n",
        ".\n",
        "b) cv2.resize in loop Python\n",
        "\n",
        "cv2.resize è veloce in C++, ma fare 1000 chiamate dal loop Python resta costoso; è normale perdere 0.3–0.6 s/immagine\n",
        "Stack Overflow\n",
        ". Se devi rifarlo spesso, usa joblib.Parallel o una semplice lista di path e multiprocessing (l’I/O in cache lo regge) per saturare la CPU.\n",
        "\n",
        "questa cella fa' quello\n",
        "# ===================================================================\n",
        "# CELLA 1: SETUP E CARICAMENTO DATI 512px\n",
        "# ===================================================================\n",
        "print(\"--- [1/4] Setup e Caricamento Dati 512px ---\")\n",
        "!pip install -q segmentation-models-pytorch==0.3.3 albumentations torchinfo\n",
        "import torch, cv2, json, pathlib, numpy as np\n",
        "import segmentation_models_pytorch as smp\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"✅ Librerie pronte.\")\n",
        "\n",
        "# Percorso del dataset GIÀ RIDIMENSIONATO a 512px su Drive\n",
        "DATA_512_DRIVE_PATH = \"/content/drive/MyDrive/Colab_Datasets/Augmented_Dataset_55\" # Assumo sia questo\n",
        "LOCAL_DATA_PATH = pathlib.Path(\"/content/dataset_512\")\n",
        "\n",
        "if not LOCAL_DATA_PATH.exists():\n",
        "    print(f\"Copiando dati da '{DATA_512_DRIVE_PATH}'...\")\n",
        "    !cp -r \"{DATA_512_DRIVE_PATH}\" \"{LOCAL_DATA_PATH}\"\n",
        "    print(\"✅ Copia completata.\")\n",
        "else:\n",
        "    print(f\"ℹ️ Dataset già presente in '{LOCAL_DATA_PATH}'.\")"
      ],
      "metadata": {
        "id": "iCjJ9scyJnl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #### Cella 1 Riferimento: Import e Definizione della Funzione VIENE USATO PER MODULARIZZARE IL TUTTO NEGLI ALtrI SCRIPT COLAB\n",
        "\n",
        "#Questa cella conterrà **solo** le definizioni, senza eseguire nulla.\n",
        "\n",
        "# ===================================================================\n",
        "# CELLA 1: MODULO DI AUGMENTATION OFFLINE\n",
        "# ===================================================================\n",
        "# Questo notebook, quando importato, espone la funzione 'create_augmented_dataset'\n",
        "\n",
        "import cv2, json, pathlib, numpy as np\n",
        "import albumentations as A\n",
        "from tqdm.notebook import tqdm\n",
        "import shutil\n",
        "\n",
        "print(\"Modulo 'Data_Augmentation_Factory' caricato.\")\n",
        "\n",
        "def create_augmented_dataset(\n",
        "    original_zip_path: str,\n",
        "    drive_save_path: str,\n",
        "    n_augmentations_per_image: int = 15,\n",
        "    image_size: int = 512,\n",
        "    train_split_ratio: float = 0.75,\n",
        "    val_split_ratio: float = 0.15,\n",
        "    random_seed: int = 42\n",
        "):\n",
        "    \"\"\"\n",
        "    Funzione completa per creare un dataset augmentato offline.\n",
        "    Prende uno zip, lo processa e salva il risultato su Google Drive.\n",
        "\n",
        "    Args:\n",
        "        original_zip_path (str): Percorso del file .zip con i dati originali.\n",
        "        drive_save_path (str): Percorso su Google Drive dove salvare il dataset finale.\n",
        "        ... (altri parametri con valori di default)\n",
        "\n",
        "    Returns:\n",
        "        pathlib.Path: Il percorso su Drive dove è stato salvato il dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Definisci i percorsi locali temporanei\n",
        "    original_data_local = pathlib.Path(\"/content/original_dataset_temp\")\n",
        "    aug_data_local = pathlib.Path(\"/content/augmented_dataset_temp\")\n",
        "\n",
        "    # Pulisci le cartelle temporanee se esistono da una run precedente\n",
        "    if original_data_local.exists():\n",
        "        shutil.rmtree(original_data_local)\n",
        "    if aug_data_local.exists():\n",
        "        shutil.rmtree(aug_data_local)\n",
        "\n",
        "    # --- 1. PREPARA I DATI ORIGINALI ---\n",
        "    original_data_local.mkdir()\n",
        "    print(f\"Scompatto '{original_zip_path}'...\")\n",
        "    !unzip -q -o \"{original_zip_path}\" -d \"{original_data_local}\"\n",
        "    print(\"✅ Dati originali pronti.\")\n",
        "\n",
        "    original_img_dir = original_data_local / \"images\"\n",
        "    original_msk_dir = original_data_local / \"masks\"\n",
        "\n",
        "    # --- 2. DEFINISCI LE PIPELINE DI AUGMENTATION ---\n",
        "    # ... (il codice per definire train_aug_pipeline e val_test_aug_pipeline rimane identico) ...\n",
        "    train_aug_pipeline = A.Compose([...]) # Inserisci la pipeline completa qui\n",
        "    val_test_aug_pipeline = A.Compose([A.Resize(image_size, image_size)])\n",
        "\n",
        "    # --- 3. SPLIT DEI DATI ORIGINALI ---\n",
        "    # ... (il codice per lo split rimane identico) ...\n",
        "    all_stems = sorted([...])\n",
        "    np.random.seed(random_seed)\n",
        "    # ... etc ...\n",
        "\n",
        "    # --- 4. GENERAZIONE DEL NUOVO DATASET ---\n",
        "    # ... (tutta la logica di generazione delle immagini rimane identica) ...\n",
        "\n",
        "    # --- 5. SALVA IL NUOVO SPLIT FILE ---\n",
        "    final_split = {\"train\": new_train_ids, \"val\": val_ids, \"test\": test_ids}\n",
        "    with open(aug_data_local / \"split.json\", \"w\") as f:\n",
        "        json.dump(final_split, f, indent=4)\n",
        "\n",
        "    # --- 6. SALVA SU GOOGLE DRIVE ---\n",
        "    drive_save_path = pathlib.Path(drive_save_path)\n",
        "    drive_save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if drive_save_path.exists():\n",
        "        shutil.rmtree(drive_save_path) # Rimuovi la vecchia versione per evitare conflitti\n",
        "\n",
        "    print(f\"Salvataggio del dataset augmentato su Google Drive in: '{drive_save_path}'...\")\n",
        "    !cp -r \"{aug_data_local}\" \"{drive_save_path}\"\n",
        "    print(\"✅ Salvataggio completato!\")\n",
        "\n",
        "    # Pulisci lo spazio locale di Colab\n",
        "    shutil.rmtree(original_data_local)\n",
        "    shutil.rmtree(aug_data_local)\n",
        "\n",
        "    return drive_save_path\n"
      ],
      "metadata": {
        "id": "mwUYHAfQShl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fs7AXtxwI7qC"
      },
      "outputs": [],
      "source": [
        "#@title cella autodefinita codice 0.1 adessio si modula#### Cella 1: Setup e Parametri\n",
        "'''\n",
        "#Qui definiamo tutti i parametri del processo: percorsi, numero di augmentations, ecc.\n",
        "\n",
        "# ===================================================================\n",
        "# CELLA 1: SETUP E PARAMETRI DELLA FABBRICA DI DATI\n",
        "# ===================================================================\n",
        "print(\"--- Fabbrica di Dati Attivata ---\")\n",
        "\n",
        "# --- Installazioni e Import ---\n",
        "!pip install -q albumentations opencv-python-headless tqdm\n",
        "!pip install unzip\n",
        "import cv2, json, pathlib, numpy as np\n",
        "import albumentations as A\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"✅ Librerie pronte.\")\n",
        "\n",
        "# --- PARAMETRI DI CONFIGURAZIONE ---\n",
        "\n",
        "# 1. Percorsi dei dati originali (da dove leggere)\n",
        "ORIGINAL_ZIP_PATH = \"/content/drive/MyDrive/DatasetUnet/unet_dataset_multiclasse(1).zip\"\n",
        "!unzip -q -o \"/content/drive/MyDrive/DatasetUnet/unet_dataset_multiclasse(1)\" -d \"/content/original_dataset\"\n",
        "\n",
        "ORIGINAL_DATA_LOCAL = pathlib.Path(\"/content/original_dataset\")\n",
        "\n",
        "# 2. Percorsi del nuovo dataset augmentato (dove salvare)\n",
        "AUG_DATA_LOCAL = pathlib.Path(\"/content/augmented_dataset_v2AUG\")\n",
        "DRIVE_SAVE_PATH = pathlib.Path(\"/content/drive/MyDrive/Colab_Datasets/Augmented_Dataset_v2AUG\")\n",
        "\n",
        "# 3. Parametri di Augmentation e Split\n",
        "N_AUGMENTATIONS_PER_IMAGE = 15  # Aumentiamo ancora un po'\n",
        "IMAGE_SIZE = 704                # Dimensione finale delle immagini\n",
        "TRAIN_SPLIT_RATIO = 0.75        # 75% dei dati originali per il training\n",
        "VAL_SPLIT_RATIO = 0.15          # 15% per la validazione\n",
        "# Il restante 10% sarà per il test\n",
        "RANDOM_SEED = 42                # Per la riproducibilità dello split\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #### Cella 2(VECCHIA): Lo Script di Generazione\n",
        "\n",
        "#Questa è l'unica altra cella. Fa tutto il lavoro sporco.\n",
        "#\n",
        "#```python\n",
        "# ===================================================================\n",
        "# CELLA 2: ESECUZIONE DELLA GENERAZIONE\n",
        "# ===================================================================\n",
        "\n",
        "# --- 1. PREPARA I DATI ORIGINALI ---\n",
        "if not ORIGINAL_DATA_LOCAL.exists():\n",
        "    ORIGINAL_DATA_LOCAL.mkdir()\n",
        "    print(f\"Scompatto '{ORIGINAL_ZIP_PATH}'...\")\n",
        "    !unzip -q -o \"{ORIGINAL_ZIP_PATH}\" -d \"{ORIGINAL_DATA_LOCAL}\"\n",
        "    print(\"✅ Dati originali pronti.\")\n",
        "else:\n",
        "    print(f\"ℹ️ Dati originali già presenti.\")\n",
        "\n",
        "ORIGINAL_IMG_DIR = ORIGINAL_DATA_LOCAL / \"images\"\n",
        "ORIGINAL_MSK_DIR = ORIGINAL_DATA_LOCAL / \"masks\"\n",
        "\n",
        "# --- 2. DEFINISCI LE PIPELINE DI AUGMENTATION ---\n",
        "# Pipeline aggressiva per il training\n",
        "train_aug_pipeline = A.Compose([\n",
        "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.15, rotate_limit=25, p=0.8),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.4),\n",
        "])\n",
        "# Pipeline semplice (solo resize) per validazione e test\n",
        "val_test_aug_pipeline = A.Compose([A.Resize(IMAGE_SIZE, IMAGE_SIZE)])\n",
        "\n",
        "# --- 3. SPLIT DEI DATI ORIGINALI ---\n",
        "all_stems = sorted([p.stem for p in ORIGINAL_IMG_DIR.glob(\"*.png\")])\n",
        "np.random.seed(RANDOM_SEED)\n",
        "np.random.shuffle(all_stems)\n",
        "n = len(all_stems)\n",
        "n_val = int(n * VAL_SPLIT_RATIO)\n",
        "n_train = int(n * TRAIN_SPLIT_RATIO)\n",
        "train_ids = all_stems[:n_train]\n",
        "val_ids = all_stems[n_train : n_train + n_val]\n",
        "test_ids = all_stems[n_train + n_val :]\n",
        "print(f\"Split originale: {len(train_ids)} train, {len(val_ids)} val, {len(test_ids)} test.\")\n",
        "\n",
        "# --- 4. GENERAZIONE DEL NUOVO DATASET ---\n",
        "(AUG_DATA_LOCAL / \"images\").mkdir(parents=True, exist_ok=True)\n",
        "(AUG_DATA_LOCAL / \"masks\").mkdir(parents=True, exist_ok=True)\n",
        "new_train_ids = []\n",
        "\n",
        "# Processa il training set\n",
        "for stem in tqdm(train_ids, desc=\"Augmenting Train Set\"):\n",
        "    img = cv2.imread(str(ORIGINAL_IMG_DIR / f\"{stem}.png\"))\n",
        "    msk = cv2.imread(str(ORIGINAL_MSK_DIR / f\"{stem}.png\"), cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "    # Salva la versione originale (ridimensionata)\n",
        "    resized = val_test_aug_pipeline(image=img, mask=msk)\n",
        "    new_stem_orig = f\"{stem}_orig\"\n",
        "    cv2.imwrite(str(AUG_DATA_LOCAL / \"images\" / f\"{new_stem_orig}.png\"), resized['image'])\n",
        "    cv2.imwrite(str(AUG_DATA_LOCAL / \"masks\" / f\"{new_stem_orig}.png\"), resized['mask'])\n",
        "    new_train_ids.append(new_stem_orig)\n",
        "\n",
        "    # Crea e salva le versioni augmentate\n",
        "    for i in range(N_AUGMENTATIONS_PER_IMAGE):\n",
        "        augmented = train_aug_pipeline(image=img, mask=msk)\n",
        "        new_stem_aug = f\"{stem}_aug_{i}\"\n",
        "        cv2.imwrite(str(AUG_DATA_LOCAL / \"images\" / f\"{new_stem_aug}.png\"), augmented['image'])\n",
        "        cv2.imwrite(str(AUG_DATA_LOCAL / \"masks\" / f\"{new_stem_aug}.png\"), augmented['mask'])\n",
        "        new_train_ids.append(new_stem_aug)\n",
        "\n",
        "# Processa validation e test set\n",
        "for stem in tqdm(val_ids + test_ids, desc=\"Processing Val/Test Sets\"):\n",
        "    img = cv2.imread(str(ORIGINAL_IMG_DIR / f\"{stem}.png\"))\n",
        "    msk = cv2.imread(str(ORIGINAL_MSK_DIR / f\"{stem}.png\"), cv2.IMREAD_UNCHANGED)\n",
        "    processed = val_test_aug_pipeline(image=img, mask=msk)\n",
        "    cv2.imwrite(str(AUG_DATA_LOCAL / \"images\" / f\"{stem}.png\"), processed['image'])\n",
        "    cv2.imwrite(str(AUG_DATA_LOCAL / \"masks\" / f\"{stem}.png\"), processed['mask'])\n",
        "\n",
        "# --- 5. SALVA IL NUOVO SPLIT FILE ---\n",
        "final_split = {\"train\": new_train_ids, \"val\": val_ids, \"test\": test_ids}\n",
        "with open(AUG_DATA_LOCAL / \"split.json\", \"w\") as f:\n",
        "    json.dump(final_split, f, indent=4)\n",
        "print(f\"✅ Dataset augmentato creato in: '{AUG_DATA_LOCAL}' con {len(new_train_ids)} immagini di training.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sYoZnsypJOct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Cella 2: Esempio di Utilizzo (per testare)\n",
        "#Questa cella ti permette di testare la funzione direttamente nel notebook della \"fabbrica\".\n",
        "# ===================================================================\n",
        "# CELLA 2: ESECUZIONE DELLA FUNZIONE\n",
        "# ===================================================================\n",
        "\n",
        "# Questo codice viene eseguito solo se esegui questo notebook direttamente\n",
        "if __name__ == '__main__':\n",
        "    # Definisci i tuoi percorsi\n",
        "    ZIP_FILE = \"/content/drive/MyDrive/DatasetUnet/unet_dataset_multiclasse(1).zip\"\n",
        "    SAVE_DESTINATION = \"/content/drive/MyDrive/Colab_Datasets/Augmented_Dataset_v3\"\n",
        "\n",
        "    # Chiama la funzione\n",
        "    final_path = create_augmented_dataset(\n",
        "        original_zip_path=ZIP_FILE,\n",
        "        drive_save_path=SAVE_DESTINATION,\n",
        "        n_augmentations_per_image=20 # Aumentiamo ancora per la prova!\n",
        "    )\n",
        "\n",
        "    print(f\"\\nProcesso terminato. Dataset disponibile in: {final_path}\")"
      ],
      "metadata": {
        "id": "Tnk1se_gTQYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir \"/content/drive/MyDrive/Colab_Datasets\""
      ],
      "metadata": {
        "id": "Vpv4WcgYQ4K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. SALVA SU GOOGLE DRIVE ---\n",
        "print(f\"Salvataggio del dataset augmentato su Google Drive in: '{DRIVE_SAVE_PATH}'...\")\n",
        "!cp -r \"{AUG_DATA_LOCAL}\" \"{DRIVE_SAVE_PATH}\"\n",
        "print(\"✅ Lavoro completato. La fabbrica si spegne.\")"
      ],
      "metadata": {
        "id": "jE0TqZCIJXTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}