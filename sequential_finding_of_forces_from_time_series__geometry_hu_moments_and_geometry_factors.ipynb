{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "x09_LO3w5x3Z"
      ],
      "mount_file_id": "1TDmXfcm8MK6ndUybSpdnCQ57mpqqwFH4",
      "authorship_tag": "ABX9TyO8wUqQjoXU1D/E+d8mboee",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riccardo-Venturi/Tesi_Script_Colab/blob/main/sequential_finding_of_forces_from_time_series__geometry_hu_moments_and_geometry_factors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUesta nuova cella dimostra che lgbm ha overfittato, quindi si preferisce prendere il modello MLP come utput con dati_forze_imputate"
      ],
      "metadata": {
        "id": "W0gZ7frDuazP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title questo predispone si sia fatto tutto il resto sotto come test; risulta un diario scientifico sperimentale\n",
        "##prende il csv creato in precedenza con i valori mancanti delle forze e applica la lgbm che si Ã¨ dimostrata quella con errore sperimentale\n",
        "#piÃ¹ basso e migiore aderenza ai dati\n",
        "# =============================================================================\n",
        "# SNIPPET FINALE PER IMPUTAZIONE FORZE MANCANTI CON MODELLO OTTIMALE\n",
        "# Metodo: LightGBM addestrato solo su 'NumeroForo'\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. Caricamento del dataset con i buchi ---\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/dataset_master_finalissimo.csv\")\n",
        "    print(\"Dataset 'dataset_master_finalissimo.csv' caricato.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERRORE: Assicurati che il file 'dataset_master_finalissimo.csv' sia presente.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Preparazione Dati per l'Imputazione ---\n",
        "# Converti colonna in numerico, gestendo i valori mancanti\n",
        "df['Forza_N'] = pd.to_numeric(df['Forza_N'], errors='coerce')\n",
        "\n",
        "# Separa i dati per addestrare il nostro modello di imputazione\n",
        "df_known = df[df['Forza_N'].notna()].copy()\n",
        "\n",
        "# Dati di addestramento: usiamo TUTTI i dati noti per addestrare il modello finale\n",
        "X_train_final = df_known[['NumeroForo']]\n",
        "y_train_final = df_known['Forza_N']\n",
        "\n",
        "# Dati da imputare: le righe dove la Forza_N Ã¨ mancante\n",
        "X_to_impute = df[df['Forza_N'].isna()][['NumeroForo']]\n",
        "\n",
        "# --- 3. Addestramento del Modello Vincente e Imputazione ---\n",
        "print(\"\\nAddestramento del modello di imputazione finale (LGBM-Base)...\")\n",
        "# Usiamo i parametri di default, che si sono dimostrati efficaci\n",
        "imputation_model = lgb.LGBMRegressor(random_state=42, verbosity=-1)\n",
        "\n",
        "# Addestriamo il modello su TUTTI i dati di forza disponibili\n",
        "imputation_model.fit(X_train_final, y_train_final)\n",
        "\n",
        "print(\"Predizione dei valori mancanti...\")\n",
        "# Eseguiamo la predizione\n",
        "predicted_forces = imputation_model.predict(X_to_impute)\n",
        "\n",
        "# --- 4. Creazione e Salvataggio del Dataset Definitivo ---\n",
        "# Crea una copia del dataframe originale per non modificarlo\n",
        "df_imputed_lgbm = df.copy()\n",
        "\n",
        "# Riempi i valori NaN con le nostre predizioni\n",
        "df_imputed_lgbm.loc[df_imputed_lgbm['Forza_N'].isna(), 'Forza_N'] = predicted_forces\n",
        "\n",
        "# Salva il file CSV che userai per tutte le analisi future\n",
        "output_filename = 'dataset_LGBM_imputato.csv'\n",
        "df_imputed_lgbm.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Dataset definitivo salvato come '{output_filename}'\")\n",
        "print(\"\\nPrime righe con i valori imputati:\")\n",
        "display(df_imputed_lgbm.loc[df['Forza_N'].isna()].head())\n",
        "\n",
        "# --- 5. Visualizzazione del Risultato (Highlight per la tesi) ---\n",
        "print(\"\\nGenerazione del grafico di confronto finale...\")\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "# Dati reali originali\n",
        "ax.scatter(df_known['NumeroForo'], df_known['Forza_N'],\n",
        "           label='Dato Reale', s=15, c='royalblue', zorder=3)\n",
        "\n",
        "# Dati che abbiamo appena imputato con il modello LGBM-Base\n",
        "ax.scatter(X_to_impute['NumeroForo'], predicted_forces,\n",
        "           label='Dato Imputato (LGBM-Base)', s=60, c='red', marker='X', zorder=5, edgecolor='black')\n",
        "\n",
        "ax.set_title(\"Dataset Finale con Forze Imputate tramite Regressione su Usura\", fontsize=18)\n",
        "ax.set_xlabel(\"Numero del Foro (Sequenza Esperimento)\", fontsize=14)\n",
        "ax.set_ylabel(\"Forza (N)\", fontsize=14)\n",
        "ax.legend(fontsize=12, title=\"Stato del Dato\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k70yGCVqkyAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cella di confronto visuale rapido\n",
        "\n",
        "df_mlp = pd.read_csv('dati_con_forze_imputate.csv')\n",
        "df_lgbm = pd.read_csv('dataset_LGBM_imputato.csv')\n",
        "\n",
        "# Estrai solo i valori imputati da entrambi\n",
        "indici_mancanti = df_lgbm['NumeroForo'].isin(range(169, 177)) # Esempio per il primo blocco\n",
        "forze_imputate_mlp = df_mlp[indici_mancanti]\n",
        "forze_imputate_lgbm = df_lgbm[indici_mancanti]\n",
        "\n",
        "# Grafico\n",
        "plt.figure(figsize=(20, 10))\n",
        "# Dati reali circostanti per contesto\n",
        "plt.plot(df_lgbm['NumeroForo'], df_lgbm['Forza_N'], 'o', color='lightgray', label='Dati Reali')\n",
        "\n",
        "# Dati imputati\n",
        "plt.plot(forze_imputate_mlp['NumeroForo'], forze_imputate_mlp['Forza_N'], 's-r', label='Imputato con MLP (complesso)')\n",
        "plt.plot(forze_imputate_lgbm['NumeroForo'], forze_imputate_lgbm['Forza_N'], 'X-g', label='Imputato con LGBM-Base (semplice/ottimale)')\n",
        "\n",
        "plt.title('Confronto Visivo delle Imputazioni', fontsize=16)\n",
        "plt.xlabel('NumeroForo')\n",
        "plt.ylabel('Forza (N)')\n",
        "plt.legend()\n",
        "plt.xlim(160, 185) # Zoom sulla zona di interesse\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L9v4vV7doknY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tabella dei Dati Essenziali e Costanti del Progetto**\n",
        "\n",
        "| Parametro / Dato | Valore | UnitÃ  | Fonte / Scopo | Note |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **PARAMETRI FISICI & GEOMETRICI (Ground Truth)** | | | | |\n",
        "| Diametro Nominale Foro (`D_nom`) | **6.0** | mm | Tesi / **Calibrazione** | La nostra \"stele di Rosetta\". Ãˆ la misura reale che useremo per calibrare ogni patch. |\n",
        "| Area Nominale Foro (`A_nom`) | **28.274** | mmÂ² | Calcolata (`Ï€*rÂ²`) | Serve per calcolare `Adel` dai dati `Atot` umani e per lo `Shape Factor`. |\n",
        "| Spessore Laminato | ~2.0 | mm | Tesi | Non usato nei modelli attuali, ma fondamentale per future simulazioni FEM. |\n",
        "| **PARAMETRI DELLA SCANSIONE ORIGINALE** | | | | |\n",
        "| Risoluzione Scansione | **600** | DPI | Tesi / Specifiche | Utilizzata per il calcolo della scala teorica. |\n",
        "| Scala Teorica Originale | **23.62** | pixel/mm | Calcolata (`600/25.4`) | **Valida SOLO sulla scansione intera, non sulle patch ridimensionate.** |\n",
        "| Diametro Foro in Pixel (Teorico) | **~142** | pixel | Calcolato (`6 * 23.62`) | Utile come controllo di sanitÃ  sui primi script di rilevamento. |\n",
        "| **PARAMETRI DELLA PIPELINE ATTUALE (YOLO -> UNet)** | | | | |\n",
        "| Dimensione Patch di Input (UNet) | **512 x 512** | pixel | Tuo script di crop | **Questo ridimensionamento ha causato la perdita della scala originale.** |\n",
        "| **Scala Empirica Misurata su Patch** | **~35.8** | pixel/mm | Nostra misura manuale | **Valore indicativo** che conferma la discrepanza e la necessitÃ  di una calibrazione per patch. |\n",
        "| **CLASSI NELLE MASCHERE UNet** | | | | |\n",
        "| Sfondo | **0** | (valore pixel) | UNet script | Classe da ignorare nei calcoli. |\n",
        "| Foro | **1** | (valore pixel) | UNet script | Maschera del foro. La UNet la separa dal danno. |\n",
        "| Delaminazione | **2** | (valore pixel) | UNet script | Maschera del danno. **Ãˆ questa che dobbiamo misurare.** |\n",
        "\n",
        "---\n",
        "\n",
        "### **Confronto Dati: Umano vs. Nostro (Il Problema da Risolvere)**\n",
        "\n",
        "Questa tabella evidenzia la discrepanza che dobbiamo risolvere. Prendo il Foro 1 come esempio lampante:\n",
        "\n",
        "| Metrica | Dato Umano (Tesi) | Dato Nostro (CSV sballato) | Stato |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Adel** | **3.08 mmÂ²** | **6.14 mmÂ²** | ðŸ”´ **Sbagliato!** (Errore del ~100%) |\n",
        "| **DMAX** | **6.22 mm** | **15.74 mm** | ðŸ”´ **Sbagliato!** (Errore del ~150%) |\n",
        "| **Forza** | **89.50 N** | (da predire) | ðŸ”œ Obiettivo Futuro |\n",
        "\n",
        "### **Cosa Facciamo Adesso (Il Piano Pratico Immediato)**\n",
        "\n",
        "Come hai detto tu, lavoriamo ripartendo dalle patch e dalle maschere che hai giÃ .\n",
        "\n",
        "1.  **Azione #1: Calibrazione Automatica (ORA).**\n",
        "    *   Prendiamo la cartella con le tue **patch radiografiche** (`.jpg`, 512x512).\n",
        "    *   Eseguiamo lo **script di calibrazione automatica** che ti ho fornito. Questo script misurerÃ  il diametro del foro in pixel in ogni patch e creerÃ  il file `calibrazione_scale_patch.csv`.\n",
        "    *   **Risultato:** Un file che associa ogni patch alla sua scala `px/mm` corretta.\n",
        "\n",
        "2.  **Azione #2: Estrazione Features Corretta.**\n",
        "    *   Modifichiamo lo script di estrazione features. Per ogni maschera:\n",
        "        *   Legge il nome del file (es. `H001_..._pred.png`).\n",
        "        *   Cerca la scala per `H001` nel file `calibrazione_scale_patch.csv`.\n",
        "        *   Misura area e DMAX in pixel dalla maschera.\n",
        "        *   **Usa la scala corretta per convertire i valori in mm e mmÂ².**\n",
        "    *   **Risultato:** Un nuovo file `features_corrette_e_verificate.csv` con dati finalmente confrontabili con quelli del tesista.\n",
        "\n",
        "Iniziamo con l'**Azione #1**. Prepara la cartella con le patch `.jpg` e il codice di calibrazione che ti ho passato. Ãˆ il passo piÃ¹ importante per sbloccare tutto il resto.\n"
      ],
      "metadata": {
        "id": "mPWnKqfmeLf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Fase 1: La Scansione Originale (Il Mondo Reale in Pixel)**\n",
        "<details>\n",
        "*   **Immagine di Riferimento:** `T0_90_1A_ingresso.jpg` (e le altre scansioni complete).\n",
        "*   **Scala Teorica (DPI):** 600 DPI (dots per inch).\n",
        "    *   Conversione: 1 pollice = 25.4 mm\n",
        "    *   **Scala Originale (px/mm):** `600 / 25.4 = **23.62 pixel per mm**`.\n",
        "*   **Dimensione Fisica Riferimento:** Diametro nominale del foro `D_nom = **6.0 mm**`.\n",
        "*   **Dimensione in Pixel (Teorica):** Sulla scansione originale, un foro da 6 mm dovrebbe avere un diametro di `6.0 mm * 23.62 px/mm â‰ˆ **142 pixel**`.\n",
        "\n",
        "#### **Fase 2: Dal Rilevamento YOLO al Ritaglio della Patch**\n",
        "\n",
        "*   **Script di Riferimento:** `yolorilevazionefori.ipynb` e gli script successivi di ordinamento e crop.\n",
        "*   **Processo:**\n",
        "    1.  YOLO rileva i fori sulla scansione intera (`imgsz=1280` o `(H,W)` nativa).\n",
        "    2.  Le coordinate delle bounding box vengono salvate e ordinate con K-Means (l'ordinamento bustrofedico).\n",
        "    3.  Uno script di crop prende il centro `(cx, cy)` di ogni foro rilevato.\n",
        "    4.  Viene ritagliata una patch quadrata attorno a quel centro. Hai usato `HS = 350` (patch 700x700) e `HS = 256` (patch 512x512).\n",
        "    5.  **IL PASSAGGIO CRUCIALE:** In uno degli script, c'Ã¨ `patch = cv2.resize(patch, (TARGET_SIZE, TARGET_SIZE), ...)` dove `TARGET_SIZE` Ã¨ `512`.\n",
        "\n",
        "#### **Fase 3: La Patch Radiografica (L'Input per la UNet)**\n",
        "\n",
        "*   **Immagine di Riferimento:** L'immagine della patch singola che mi hai mostrato.\n",
        "*   **Dimensione:** **512x512 pixel**.\n",
        "*   **Il Problema:** Questa immagine Ã¨ il risultato del `resize` della Fase 2. La sua scala in `pixel/mm` **NON Ã¨ piÃ¹ 23.62**. Ãˆ una nuova scala che dobbiamo calcolare.\n",
        "\n",
        "#### **Fase 4: La Maschera Predetta (L'Output della UNet)**\n",
        "\n",
        "*   **Immagine di Riferimento:** L'immagine della maschera in bianco e nero che mi hai mostrato.\n",
        "*   **Dimensione:** **512x512 pixel** (la stessa della patch di input).\n",
        "*   **Contenuto:** I valori dei pixel rappresentano le classi (es. 0=Sfondo, 1=Foro, 2=Danno).\n",
        "*   **Obiettivo:** Misurare l'area e il DMAX del danno (pixel di classe 2) su questa maschera e convertirli in `mmÂ²` e `mm` usando la scala corretta.\n"
      ],
      "metadata": {
        "id": "eQnAoHCEIt-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    *   **Mediana vs Media:** La tua intuizione sulla mediana Ã¨ acuta. Se la distribuzione delle forze per un dato `NumeroForo` non Ã¨ simmetrica, predire la media (come fa la MSE loss) potrebbe non essere la scelta migliore. I punti potrebbero essere \"spiaccicati\" (con una maggiore dispersione), e il nostro modello non lo cattura.\n"
      ],
      "metadata": {
        "id": "50zKc6tz3u_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analisi piÃ¹ Rigorosa e Piano d'Azione Rivisto\n",
        "\n",
        "Dato che l'obiettivo finale Ã¨ definire un limite di danno accettabile, dobbiamo essere piÃ¹ critici. Invece di accettare ciecamente l'output della MLP, usiamolo come uno strumento esplorativo per rispondere a una domanda piÃ¹ profonda.\n",
        "\n",
        "**Nuovo Obiettivo:** Stabilire se le *caratteristiche della forma del danno* (i momenti di Hu) contengono abbastanza informazione per distinguere tra un processo a \"bassa forza\" e uno ad \"alta forza\", al netto dell'effetto dominante dell'usura (`NumeroForo`).\n",
        "<details>\n",
        "Ecco un approccio piÃ¹ solido e un codice rivisto che si concentra su questa domanda.\n",
        "\n",
        "#### Piano d'Azione (Versione 2.0 - piÃ¹ mirata)\n",
        "1.  **Analisi di Correlazione:** Prima di addestrare un modello complesso, visualizziamo la correlazione tra le nostre features (soprattutto i momenti di Hu) e la `Forza_N`. Questo ci dirÃ  quali descrittori sono piÃ¹ promettenti.\n",
        "2.  **Feature Importance con un Modello Robusto:** Invece di una MLP (che Ã¨ una \"scatola nera\"), usiamo un modello come **Gradient Boosting (es. LightGBM o XGBoost)**. Questi modelli ci forniscono una metrica diretta di **\"Feature Importance\"**, dicendoci quali variabili hanno usato di piÃ¹ per fare le loro predizioni. Questo Ã¨ un passo diagnostico fondamentale.\n",
        "3.  **Confronto Modelli:** Addestriamo due modelli:\n",
        "    *   **Modello A (Baseline):** Predice la `Forza_N` usando **solo** `NumeroForo`. Questo ci dice qual Ã¨ la performance che otteniamo basandoci solo sull'usura.\n",
        "    *   **Modello B (Completo):** Predice la `Forza_N` usando `NumeroForo` + tutte le features del danno.\n",
        "4.  **Valutazione:** Se il Modello B Ã¨ **significativamente migliore** del Modello A, allora abbiamo la prova che **le forme del danno contengono informazione utile sulla forza, al di lÃ  del semplice effetto di usura.**\n"
      ],
      "metadata": {
        "id": "3x_LYEGC5BSS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZAJqEP8IrAF"
      },
      "outputs": [],
      "source": [
        "### **Script di Calibrazione Automatica della Scala**\n",
        "'''\n",
        "Questo script fa una sola cosa, ma la fa bene:\n",
        "1.  Legge ogni immagine `.jpg` da una cartella di input (le tue patch radiografiche).\n",
        "2.  Usa l'algoritmo di visione artificiale `HoughCircles` per trovare il foro da 6 mm.\n",
        "3.  Calcola la scala `pixel/mm` per quella specifica immagine.\n",
        "4.  Salva tutti i risultati in un file CSV chiamato `calibrazione_scale_patch.csv`.\n",
        "\n",
        "```python\n",
        "'''# =============================================================================\n",
        "# SCRIPT DI CALIBRAZIONE AUTOMATICA DELLA SCALA PER OGNI PATCH\n",
        "# =============================================================================\n",
        "\n",
        "# --- 1. INSTALLAZIONI E IMPORT NECESSARI ---\n",
        "# (Esegui questa cella una sola volta all'inizio)\n",
        "!pip install -q opencv-python pandas matplotlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files # Per scaricare il file CSV alla fine\n",
        "\n",
        "# --- 2. IMPOSTAZIONI (LE UNICHE COSE DA MODIFICARE) ---\n",
        "\n",
        "# Diametro reale del foro in millimetri. Questa Ã¨ la nostra \"veritÃ \" per la calibrazione.\n",
        "DIAMETRO_REALE_FORO_MM = 6.0\n",
        "\n",
        "# Imposta il percorso alla cartella che contiene le TUE patch radiografiche (.jpg).\n",
        "# Esempio per Google Drive:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# RADIO_PATCHES_DIR = Path(\"/content/drive/MyDrive/PATCHES_RADIO_Dataset\")\n",
        "\n",
        "# Per un test iniziale, puoi caricare le immagini manualmente in una cartella in Colab.\n",
        "RADIO_PATCHES_DIR = Path(\"/content/drive/MyDrive/PATCHES_RADIO_Dataset\")#\"/content/immagini_da_calibrare\")\n",
        "RADIO_PATCHES_DIR.mkdir(exist_ok=True) # Crea la cartella se non esiste\n",
        "print(f\"ATTENZIONE: Lo script cercherÃ  le immagini in '{RADIO_PATCHES_DIR}'\")\n",
        "print(\"Carica le tue patch .jpg in quella cartella.\")\n",
        "\n",
        "\n",
        "# --- 3. FUNZIONE DI CALIBRAZIONE ---\n",
        "\n",
        "def get_scale_from_radiograph(radiograph_path: Path, visualize=False):\n",
        "    \"\"\"\n",
        "    Carica un'immagine radiografica, trova il foro centrale e calcola la scala pixel/mm.\n",
        "    \"\"\"\n",
        "    if not radiograph_path.exists():\n",
        "        print(f\"Attenzione: Immagine non trovata a {radiograph_path}\")\n",
        "        return None, None\n",
        "\n",
        "    # Carica l'immagine in scala di grigi\n",
        "    image = cv2.imread(str(radiograph_path), cv2.IMREAD_GRAYSCALE)\n",
        "    if image is None:\n",
        "        return None, None\n",
        "\n",
        "    # Applica un leggero blur per ridurre il rumore e aiutare l'algoritmo\n",
        "    blurred_image = cv2.GaussianBlur(image, (9, 9), 2)\n",
        "\n",
        "    # Parametri per l'algoritmo HoughCircles. Potrebbero richiedere aggiustamenti.\n",
        "    # dp: Rapporto inverso della risoluzione dell'accumulatore. 1.2 Ã¨ un buon punto di partenza.\n",
        "    # minDist: Distanza minima tra i centri dei cerchi. Mettiamola grande per trovare solo il foro principale.\n",
        "    # param1: Soglia superiore per l'edge detector di Canny interno.\n",
        "    # param2: Soglia per il centro del cerchio. PiÃ¹ Ã¨ basso, piÃ¹ \"falsi\" cerchi trova.\n",
        "    # minRadius, maxRadius: Limiti per il raggio del cerchio. FONDAMENTALI!\n",
        "\n",
        "    # Stimiamo un range ragionevole per il raggio basandoci sulle dimensioni dell'immagine\n",
        "    img_height, img_width = image.shape\n",
        "    min_r = int(img_height * 0.15) # min raggio 15% dell'altezza\n",
        "    max_r = int(img_height * 0.40) # max raggio 40% dell'altezza (piÃ¹ flessibile)\n",
        "\n",
        "    circles = cv2.HoughCircles(blurred_image, cv2.HOUGH_GRADIENT, dp=1.2, minDist=img_width,\n",
        "                               param1=70, param2=45, minRadius=min_r, maxRadius=max_r)\n",
        "\n",
        "    if circles is not None:\n",
        "        # Trovato almeno un cerchio!\n",
        "        circles = np.round(circles[0, :]).astype(\"int\")\n",
        "        (x, y, r) = circles[0] # Prendiamo il primo (e unico) cerchio trovato\n",
        "\n",
        "        diametro_pixel = r * 2\n",
        "        scala_px_per_mm = diametro_pixel / DIAMETRO_REALE_FORO_MM\n",
        "\n",
        "        if visualize:\n",
        "            output_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "            cv2.circle(output_image, (x, y), r, (0, 255, 0), 4) # Cerchio verde\n",
        "            cv2.circle(output_image, (x, y), 5, (0, 0, 255), -1) # Punto rosso al centro\n",
        "            testo = f\"Scala: {scala_px_per_mm:.2f} px/mm\"\n",
        "            cv2.putText(output_image, testo, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "            return scala_px_per_mm, output_image\n",
        "\n",
        "        return scala_px_per_mm, None\n",
        "    else:\n",
        "        # Nessun cerchio trovato automaticamente\n",
        "        return None, image if visualize else None\n",
        "\n",
        "# --- 4. BLOCCO DI ESECUZIONE ---\n",
        "\n",
        "# Prendi tutte le immagini .jpg nella cartella\n",
        "all_radio_paths = sorted(list(RADIO_PATCHES_DIR.glob(\"*.jpg\")))\n",
        "if not all_radio_paths:\n",
        "    all_radio_paths = sorted(list(RADIO_PATCHES_DIR.glob(\"*.png\"))) # Prova anche i png\n",
        "\n",
        "if not all_radio_paths:\n",
        "    print(f\"âŒ ERRORE: Nessuna immagine .jpg o .png trovata nella cartella '{RADIO_PATCHES_DIR}'.\")\n",
        "    print(\"Assicurati di aver caricato le immagini prima di eseguire questa cella.\")\n",
        "else:\n",
        "    print(f\"Trovate {len(all_radio_paths)} patch. Inizio calibrazione...\")\n",
        "\n",
        "    # Visualizziamo un esempio per un controllo di qualitÃ \n",
        "    print(\"\\n--- TEST DI VISUALIZZAZIONE SULLA PRIMA IMMAGINE ---\")\n",
        "    scala_test, img_test = get_scale_from_radiograph(all_radio_paths[0], visualize=True)\n",
        "    if scala_test:\n",
        "        print(f\"Scala calcolata per '{all_radio_paths[0].name}': {scala_test:.2f} px/mm\")\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(cv2.cvtColor(img_test, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(\"Visualizzazione Rilevamento Cerchio\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Rilevamento fallito sull'immagine di test. Prova a modificare i parametri di HoughCircles (param2, minRadius, maxRadius).\")\n",
        "\n",
        "    # Eseguiamolo su tutte le immagini per creare il nostro file di calibrazione\n",
        "    print(\"\\n--- CALIBRAZIONE DI MASSA SULL'INTERO DATASET ---\")\n",
        "    calibration_data = []\n",
        "    for path in tqdm(all_radio_paths, desc=\"Calibrazione Immagini\"):\n",
        "        scala, _ = get_scale_from_radiograph(path, visualize=False)\n",
        "\n",
        "        entry = {'filename': path.name}\n",
        "        if scala:\n",
        "            entry['scala_px_per_mm'] = scala\n",
        "        else:\n",
        "            entry['scala_px_per_mm'] = np.nan # Mettiamo NaN se il rilevamento fallisce\n",
        "        calibration_data.append(entry)\n",
        "\n",
        "    # Converti i risultati in un DataFrame di Pandas\n",
        "    df_calibrazione = pd.DataFrame(calibration_data)\n",
        "\n",
        "    output_filename = \"calibrazione_scale_patch.csv\"\n",
        "    df_calibrazione.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(\"\\n--- RISULTATI CALIBRAZIONE ---\")\n",
        "    print(df_calibrazione.head())\n",
        "    print(f\"\\nâœ… File di calibrazione '{output_filename}' creato con successo!\")\n",
        "\n",
        "    # Controlliamo se ci sono stati fallimenti\n",
        "    fallimenti = df_calibrazione['scala_px_per_mm'].isna().sum()\n",
        "    if fallimenti > 0:\n",
        "        print(f\"\\nâš ï¸ ATTENZIONE: {fallimenti} immagini non sono state calibrate correttamente.\")\n",
        "    else:\n",
        "        print(\"\\nðŸŽ‰ OTTIMO: Tutte le immagini sono state calibrate con successo.\")\n",
        "\n",
        "    # Analizziamo la distribuzione delle scale\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    df_calibrazione['scala_px_per_mm'].hist(bins=30, edgecolor='black')\n",
        "    plt.title(\"Distribuzione delle Scale Calcolate (px/mm)\")\n",
        "    plt.xlabel(\"Scala (pixel/mm)\")\n",
        "    plt.ylabel(\"Numero di Patch\")\n",
        "    plt.show()\n",
        "\n",
        "    # Offri il download del file CSV\n",
        "    files.download(output_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ora si applica alle maschere della unet"
      ],
      "metadata": {
        "id": "r9XhTzIZhXsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SCRIPT DI ESTRAZIONE FEATURES \"INTELLIGENTE\" CON SCALA CORRETTA\n",
        "# =============================================================================\n",
        "\n",
        "# --- 1. INSTALLAZIONI E IMPORT NECESSARI ---\n",
        "!pip install -q opencv-python pandas\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# --- 2. IMPOSTAZIONI (CONTROLLA QUESTI PERCORSI) ---\n",
        "\n",
        "# Percorso al file CSV con le scale che hai appena generato\n",
        "CALIBRATION_FILE_PATH = Path(\"/content/calibrazione_scale_patch.csv\")\n",
        "\n",
        "# Percorso alla cartella che contiene le TUE maschere predette dalla UNet (.png)\n",
        "MASKS_DIR = Path(\"/content/drive/MyDrive/PATCHES_RADIO_Dataset\")\n",
        "MASKS_DIR.mkdir(exist_ok=True) # Crea la cartella se non esiste\n",
        "print(f\"ATTENZIONE: Lo script cercherÃ  le maschere in '{MASKS_DIR}'\")\n",
        "print(\"Carica le tue maschere .png in quella cartella.\")\n",
        "\n",
        "# --- 3. CARICAMENTO DATI DI CALIBRAZIONE ---\n",
        "try:\n",
        "    df_calibrazione = pd.read_csv(CALIBRATION_FILE_PATH)\n",
        "    # Creiamo un dizionario per una ricerca super veloce della scala\n",
        "    # Assumiamo che il filename della maschera derivi da quello della patch\n",
        "    # Es: H001_h001_..._pred.png -> H001_h001_....jpg\n",
        "    # Quindi creiamo una chiave pulita\n",
        "    def get_clean_key(filename):\n",
        "        return filename.replace('.png', '.jpg')\n",
        "\n",
        "    scale_dict = {row['filename']: row['scala_px_per_mm'] for _, row in df_calibrazione.iterrows()}\n",
        "    print(f\"âœ… File di calibrazione caricato con successo. Contiene {len(scale_dict)} scale.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ ERRORE: File di calibrazione non trovato in '{CALIBRATION_FILE_PATH}'. Assicurati che sia presente.\")\n",
        "    raise\n",
        "\n",
        "# --- 4. FUNZIONE DI ESTRAZIONE FEATURES (AGGIORNATA) ---\n",
        "\n",
        "def extract_calibrated_features(mask_path, scale_lookup):\n",
        "    \"\"\"\n",
        "    Estrae le features da una maschera usando la scala di calibrazione corretta.\n",
        "    \"\"\"\n",
        "    # Ricava il nome del file della radiografia originale per cercare la scala\n",
        "    original_radio_filename = mask_path.name.replace('.png', '.jpg')\n",
        "\n",
        "    # Prendi la scala corretta dal nostro dizionario\n",
        "    scala = scale_lookup.get(original_radio_filename)\n",
        "\n",
        "    if scala is None or pd.isna(scala):\n",
        "        # Se non troviamo una scala per questa immagine, la saltiamo\n",
        "        return None\n",
        "\n",
        "    # Carica la maschera\n",
        "    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None:\n",
        "        return None\n",
        "\n",
        "    # Isola solo la maschera della delaminazione (classe 2)\n",
        "    delam_mask = (mask == 2).astype(np.uint8)\n",
        "\n",
        "    # --- Calcolo Area ---\n",
        "    area_delam_px = cv2.countNonZero(delam_mask)\n",
        "    area_delam_mm2 = area_delam_px / (scala ** 2)\n",
        "\n",
        "    # --- Calcolo DMAX ---\n",
        "    contours, _ = cv2.findContours(delam_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    d_max_px = 0\n",
        "    if contours:\n",
        "        # Unisci tutti i punti di tutti i contorni in un unico array\n",
        "        all_points = np.concatenate(contours, axis=0)\n",
        "        # Troviamo il rettangolo rotato con area minima che contiene tutti i punti\n",
        "        rect = cv2.minAreaRect(all_points)\n",
        "        # La diagonale di questo rettangolo Ã¨ una buona stima di DMAX\n",
        "        box = cv2.boxPoints(rect)\n",
        "        dist1 = np.linalg.norm(box[0] - box[2])\n",
        "        dist2 = np.linalg.norm(box[1] - box[3])\n",
        "        d_max_px = max(dist1, dist2)\n",
        "\n",
        "    d_max_mm = d_max_px / scala\n",
        "\n",
        "    # (Opzionale, puoi aggiungere qui i Momenti di Hu o altri calcoli se servono)\n",
        "\n",
        "    return {\n",
        "        'FileMaschera': mask_path.name,\n",
        "        'AreaDelaminata_mm2': area_delam_mm2,\n",
        "        'Dmax_mm': d_max_mm,\n",
        "        'Scala_Usata': scala\n",
        "    }\n",
        "\n",
        "# --- 5. BLOCCO DI ESECUZIONE ---\n",
        "all_mask_paths = sorted({\n",
        "    *MASKS_DIR.rglob(\"*.png\"),\n",
        "    *MASKS_DIR.rglob(\"*.jpg\"),\n",
        "    *MASKS_DIR.rglob(\"*.jpeg\"),\n",
        "})\n",
        "if not all_mask_paths:\n",
        "    print(f\"âŒ ERRORE: Nessuna maschera .png trovata in '{MASKS_DIR}'.\")\n",
        "else:\n",
        "    print(f\"\\nTrovate {len(all_mask_paths)} maschere. Inizio estrazione features corrette...\")\n",
        "\n",
        "    features_list = []\n",
        "    for path in tqdm(all_mask_paths, desc=\"Estrazione Features Calibrate\"):\n",
        "        feats = extract_calibrated_features(path, scale_dict)\n",
        "        if feats:\n",
        "            features_list.append(feats)\n",
        "\n",
        "    df_features_corrette = pd.DataFrame(features_list)\n",
        "\n",
        "    output_filename = \"features_corrette_e_verificate.csv\"\n",
        "    df_features_corrette.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(\"\\n--- RISULTATI ESTRAZIONE CORRETTA ---\")\n",
        "    print(df_features_corrette.head())\n",
        "    print(f\"\\nâœ… File con features corrette '{output_filename}' creato con successo!\")\n",
        "\n",
        "    # Offri il download\n",
        "    files.download(output_filename)"
      ],
      "metadata": {
        "id": "ZW0yem-WhbZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crezione del file csv dei dati di lavoro definitivo, mi fa merge di quelle maschere unet e del file di origine di Melis"
      ],
      "metadata": {
        "id": "I84eyFhTmNaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### **Notebook Definitivo: Creazione del Dataset Master Comparativo**\n",
        "\n",
        "#### **CELLA 1: Setup, Caricamento Dati e Costanti**\n",
        "'''*Questa cella imposta l'ambiente, definisce le costanti del nostro universo e carica tutti i file di input necessari.*\n",
        "\n",
        "```python'''\n",
        "# =============================================================================\n",
        "# CELLA 1: SETUP, CARICAMENTO DATI E COSTANTI\n",
        "# =============================================================================\n",
        "\n",
        "# --- 1. Installazioni e Import ---\n",
        "!pip install -q opencv-python pandas\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import re\n",
        "from google.colab import files\n",
        "\n",
        "print(\"--- Ambiente Pronto ---\")\n",
        "\n",
        "# --- 2. Costanti Fisiche e di Progetto ---\n",
        "D_NOMINALE_MM = 6.0\n",
        "AREA_NOMINALE_MM2 = np.pi * (D_NOMINALE_MM / 2)**2\n",
        "print(f\"Costanti definite: D_nom = {D_NOMINALE_MM} mm, A_nom = {AREA_NOMINALE_MM2:.3f} mm^2\")\n",
        "\n",
        "\n",
        "# --- 3. Percorsi dei File di Input (CORRETTI) ---\n",
        "CALIBRATION_FILE_PATH = Path(\"/content/calibrazione_scale_patch.csv\")\n",
        "\n",
        "\n",
        "# QUESTI SONO I LINK AI FILE \"RAW\" (Grezzi)\n",
        "FORZA_URL_RAW = \"https://github.com/Riccardo-Venturi/DatiBuchi/raw/main/forze%20T_0_90.xlsx\"\n",
        "GEOMETRIA_MELIS_URL_RAW = \"https://github.com/Riccardo-Venturi/DatiBuchi/raw/main/Aree%20Delaminazioni%20T_0_90_rev1.xlsx\"\n",
        "\n",
        "\n",
        "# --- 4. Caricamento e Preparazione dei Dati di Input ---\n",
        "try:\n",
        "    # Dati di Calibrazione (da file locale)\n",
        "    df_calibrazione = pd.read_csv(CALIBRATION_FILE_PATH)\n",
        "    scale_dict = {row['filename']: row['scala_px_per_mm'] for _, row in df_calibrazione.iterrows()}\n",
        "    print(f\"âœ… File di calibrazione caricato ({len(scale_dict)} voci).\")\n",
        "\n",
        "    # Dati di Forza di Maurizio (da URL raw usando pd.read_excel)\n",
        "    df_forza_maurizio = pd.read_excel(FORZA_URL_RAW, engine='openpyxl',header=1)\n",
        "    # Puliamo i dati, prendendo solo le colonne che ci servono\n",
        "    df_forza_maurizio = df_forza_maurizio[['Foro', 'Fcorretta']].rename(columns={'Foro': 'NumeroForo', 'Fcorretta': 'Forza_N'})\n",
        "    print(f\"âœ… File delle forze di Maurizio caricato ({len(df_forza_maurizio)} voci).\")\n",
        "#    # Dati Geometrici di Maurizio\n",
        "#    df_geometria_maurizio = pd.read_excel(GEOMETRIA_MELIS_URL_RAW,engine='openpyxl')\n",
        "#    print(f\"âœ… File di geometria di Maurizio caricato ({len(df_geometria_maurizio)} voci).\")\n",
        "#\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ERRORE: Qualcosa Ã¨ andato storto nel caricamento dei file. Dettagli: {e}\")\n",
        "    raise\n"
      ],
      "metadata": {
        "id": "z5emrEnrmVtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(MASKS_DIR)"
      ],
      "metadata": {
        "id": "H3MBpQwbKoJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# CELLA 3: ESTRAZIONE FEATURES GEOMETRICHE CALIBRATE (UNET)\n",
        "# =============================================================================\n",
        "\n",
        "def extract_hole_number(path):\n",
        "    match = re.search(r'H(\\d+)', path.stem)\n",
        "    return int(match.group(1)) if match else -1\n",
        "\n",
        "def extract_geometric_features(mask_path, scale_lookup):\n",
        "    original_radio_filename = mask_path.name.replace('.png', '.jpg')\n",
        "    scala = scale_lookup.get(original_radio_filename)\n",
        "    if scala is None or pd.isna(scala): return None\n",
        "\n",
        "    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None: return None\n",
        "\n",
        "    numero_foro = extract_hole_number(mask_path)\n",
        "    delam_mask = (mask == 2).astype(np.uint8)\n",
        "\n",
        "    # Calcoli geometrici\n",
        "    area_delam_px = cv2.countNonZero(delam_mask)\n",
        "    area_delam_mm2 = area_delam_px / (scala ** 2)\n",
        "\n",
        "    d_max_mm = 0\n",
        "    contours, _ = cv2.findContours(delam_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if contours:\n",
        "        all_points = np.concatenate(contours, axis=0)\n",
        "        if len(all_points) > 0:\n",
        "            rect = cv2.minAreaRect(all_points)\n",
        "            box = cv2.boxPoints(rect)\n",
        "            d_max_px = max(np.linalg.norm(box[0] - box[2]), np.linalg.norm(box[1] - box[3]))\n",
        "            d_max_mm = d_max_px / scala\n",
        "\n",
        "    # Feature ingegnerizzate\n",
        "    df_diametro = d_max_mm / D_NOMINALE_MM\n",
        "    area_cerchio_max_mm2 = np.pi * (d_max_mm / 2)**2\n",
        "    denominatore_sf = area_cerchio_max_mm2 - AREA_NOMINALE_MM2\n",
        "    shape_factor = area_delam_mm2 / denominatore_sf if denominatore_sf > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'NumeroForo': numero_foro,\n",
        "        'AreaDelaminata_unet': area_delam_mm2,\n",
        "        'Dmax_unet': d_max_mm,\n",
        "        'DF_unet': df_diametro,\n",
        "        'ShapeFactor_unet': shape_factor,\n",
        "        'FileMaschera': mask_path.name,\n",
        "    }\n",
        "\n",
        "# Eseguiamo l'estrazione\n",
        "all_mask_paths = sorted({\n",
        "    *MASKS_DIR.rglob(\"*.png\"),\n",
        "    *MASKS_DIR.rglob(\"*.jpg\"),\n",
        "    *MASKS_DIR.rglob(\"*.jpeg\"),\n",
        "})\n",
        "if not all_mask_paths:\n",
        "    print(f\"âŒ ERRORE: Nessuna maschera trovata in '{MASKS_DIR}'.\")\n",
        "else:\n",
        "    geometric_features_list = []\n",
        "    for path in tqdm(all_mask_paths, desc=\"Estraggo Geometria UNet\"):\n",
        "        feats = extract_geometric_features(path, scale_dict)\n",
        "        if feats:\n",
        "            geometric_features_list.append(feats)\n",
        "\n",
        "    df_unet_geometric = pd.DataFrame(geometric_features_list)\n",
        "    print(\"\\n--- Dataset Geometrico (UNet) Calibrato ---\")\n",
        "    display(df_unet_geometric.head())\n",
        "\n",
        "#### **CELLA 4: Calcolo Momenti di Hu (Cella Separata)**\n",
        "'''*Come richiesto, una cella dedicata a calcolare i 7 Momenti di Hu, che sono ottimi descrittori di forma.*\n",
        "\n",
        "```python'''\n",
        "# =============================================================================\n",
        "# CELLA 4: CALCOLO MOMENTI DI HU\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_hu_moments(mask_path):\n",
        "    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None: return None\n",
        "\n",
        "    numero_foro = extract_hole_number(mask_path)\n",
        "    delam_mask = (mask == 2).astype(np.uint8)\n",
        "\n",
        "    moments = cv2.moments(delam_mask)\n",
        "    hu_moments = cv2.HuMoments(moments).flatten()\n",
        "    hu_log = np.sign(hu_moments) * np.log10(np.abs(hu_moments) + 1e-7) # Stabilizzazione logaritmica\n",
        "\n",
        "    return {\n",
        "        'NumeroForo': numero_foro,\n",
        "        'Hu_1': hu_log[0], 'Hu_2': hu_log[1], 'Hu_3': hu_log[2], 'Hu_4': hu_log[3],\n",
        "        'Hu_5': hu_log[4], 'Hu_6': hu_log[5], 'Hu_7': hu_log[6],\n",
        "    }\n",
        "\n",
        "# Eseguiamo l'estrazione\n",
        "hu_features_list = []\n",
        "for path in tqdm(all_mask_paths, desc=\"Calcolo Momenti di Hu\"):\n",
        "    hu_feats = calculate_hu_moments(path)\n",
        "    if hu_feats:\n",
        "        hu_features_list.append(hu_feats)\n",
        "\n",
        "df_unet_hu = pd.DataFrame(hu_features_list)\n",
        "print(\"\\n--- Dataset Momenti di Hu (UNet) ---\")\n",
        "display(df_unet_hu.head())"
      ],
      "metadata": {
        "id": "XEozVj7l3QQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELLA 5: UNIONE FINALE E SALVATAGGIO (senza geometrie umane)  âœ…\n",
        "# =============================================================================\n",
        "print(\"--- Unione di UNet (geometria+Hu) con le Forze ---\")\n",
        "\n",
        "# 0) Protezione: tieni solo fori riconosciuti (>0) dalle maschere\n",
        "df_unet_geometric = df_unet_geometric[df_unet_geometric[\"NumeroForo\"].fillna(-1).astype(int) > 0]\n",
        "df_unet_hu        = df_unet_hu[df_unet_hu[\"NumeroForo\"].fillna(-1).astype(int) > 0]\n",
        "\n",
        "# 1) UNET: merge interno tra geometria e Hu\n",
        "df_unet_completo = df_unet_geometric.merge(df_unet_hu, on=\"NumeroForo\", how=\"inner\")\n",
        "\n",
        "# 2) FORZE: normalizza chiave e deduplica\n",
        "df_forza_maurizio = (\n",
        "    df_forza_maurizio\n",
        "      .assign(NumeroForo=(df_forza_maurizio[\"NumeroForo\"]\n",
        "                          .astype(str).str.extract(r\"(\\d+)\").squeeze()))\n",
        "      .dropna(subset=[\"NumeroForo\"])\n",
        ")\n",
        "df_forza_maurizio[\"NumeroForo\"] = df_forza_maurizio[\"NumeroForo\"].astype(\"int64\")\n",
        "df_forza_maurizio = df_forza_maurizio.drop_duplicates(subset=[\"NumeroForo\"], keep=\"first\")\n",
        "\n",
        "# 3) MERGE finale: tieni tutti i fori che hanno features UNet, attacca la Forza se câ€™Ã¨\n",
        "#    (on deve essere colonna presente in ENTRAMBI i DataFrame)\n",
        "df_master = df_unet_completo.merge(\n",
        "    df_forza_maurizio[[\"NumeroForo\",\"Forza_N\"]],\n",
        "    on=\"NumeroForo\", how=\"left\"\n",
        ")\n",
        "\n",
        "# 4) Ordina/riordina colonne e salva\n",
        "df_master = df_master.sort_values(\"NumeroForo\").reset_index(drop=True)\n",
        "colonne_ordinate = [\n",
        "    \"NumeroForo\", \"Forza_N\",\n",
        "    \"AreaDelaminata_unet\", \"Dmax_unet\", \"DF_unet\", \"ShapeFactor_unet\",\n",
        "    \"Hu_1\",\"Hu_2\",\"Hu_3\",\"Hu_4\",\"Hu_5\",\"Hu_6\",\"Hu_7\",\n",
        "    \"FileMaschera\"\n",
        "]\n",
        "df_master = df_master[[c for c in colonne_ordinate if c in df_master.columns]]\n",
        "output_filename = \"dataset_master_finalissimo.csv\"\n",
        "df_master.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Salvato: {output_filename}\")\n",
        "print(f\"Righe: {len(df_master)} | Forze mancanti: {df_master['Forza_N'].isna().sum()}\")\n",
        "\n",
        "display(df_master.head(10))\n"
      ],
      "metadata": {
        "id": "h27-DF06-NcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questo Ã¨ un classico problema di **regressione** e di **imputazione dei dati tramite modello**. Procediamo passo dopo passo.\n",
        "si passa alla parte di chiussura del dataset\n",
        "1) attiva mlp per forze da geometria\n",
        "2) attiva ltsm per danno"
      ],
      "metadata": {
        "id": "HgCNC37bboCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ecco il codice Python che realizza esattamente questo. Utilizzeremo `pandas` per la manipolazione dei dati, `scikit-learn` per il pre-processing e la divisione dei dati, e `tensorflow/keras` per costruire e addestrare la nostra MLP.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import re # Per estrarre il tipo di materiale dal nome del file\n"
      ],
      "metadata": {
        "id": "0WUD4CHwb1Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# 1. Caricamento e Preparazione dei Dati\n",
        "# --------------------------------------------\n",
        "# Assicurati che il file si chiami 'dati_completi.csv' o modifica il nome\n",
        "# Ho ipotizzato che il file sia nella stessa cartella dello script.\n",
        "try:\n",
        "    df = pd.read_csv('/content/dataset_master_finalissimo.csv') # Sostituisci con il nome del tuo file\n",
        "except FileNotFoundError:\n",
        "    print(\"Errore: File non trovato. Assicurati che il nome del file sia corretto e che si trovi nella giusta directory.\")\n",
        "    exit()\n",
        "\n",
        "print(\"Dataset caricato con successo.\")\n",
        "print(f\"Dimensioni del dataset: {df.shape}\")\n",
        "\n",
        "# Funzione per estrarre il tipo di materiale\n",
        "def get_material_type(filename):\n",
        "    # Cerca un pattern tipo \"Carbon Textile XY\"\n",
        "    match = re.search(r'Carbon Textile \\d[A-Z]', str(filename))\n",
        "    if match:\n",
        "        return match.group(0)\n",
        "    return \"Unknown\" # In caso non trovi il pattern\n",
        "\n",
        "# Crea la colonna 'TipoMateriale' che Ã¨ una feature FONDAMENTALE\n",
        "df['TipoMateriale'] = df['FileMaschera'].apply(get_material_type)\n",
        "\n",
        "print(\"\\nValori unici in 'TipoMateriale':\")\n",
        "print(df['TipoMateriale'].value_counts())\n"
      ],
      "metadata": {
        "id": "wscL4jHYb2zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Divisione del Dataset\n",
        "# --------------------------------------------\n",
        "# Dividiamo il dataframe in due: uno con le forze note (per addestrare)\n",
        "# e uno con le forze mancanti (su cui predire).\n",
        "\n",
        "df_con_forza = df[df['Forza_N'].notna()].copy()\n",
        "df_senza_forza = df[df['Forza_N'].isna()].copy()\n",
        "\n",
        "print(f\"\\nNumero di campioni con forza nota (per training/validation): {len(df_con_forza)}\")\n",
        "print(f\"Numero di campioni con forza mancante (per predizione): {len(df_senza_forza)}\")\n"
      ],
      "metadata": {
        "id": "d9sP4DUAb2_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Definizione delle Feature e del Target\n",
        "# --------------------------------------------\n",
        "# Definiamo le colonne che useremo come input (features) e output (target)\n",
        "# Il target Ã¨ la forza, le features sono tutto il resto che descrive il danno.\n",
        "\n",
        "features_numeriche = [\n",
        "    'AreaDelaminata_unet', 'Dmax_unet', 'DF_unet', 'ShapeFactor_unet',\n",
        "    'Hu_1', 'Hu_2', 'Hu_3', 'Hu_4', 'Hu_5', 'Hu_6', 'Hu_7'\n",
        "]\n",
        "features_categoriche = ['TipoMateriale']\n",
        "\n",
        "# Variabili indipendenti (X) e dipendente (y)\n",
        "X = df_con_forza[features_numeriche + features_categoriche]\n",
        "y = df_con_forza['Forza_N']\n"
      ],
      "metadata": {
        "id": "PTT2WLHMcw59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Creazione della Pipeline di Pre-processing\n",
        "# --------------------------------------------\n",
        "# Questa pipeline gestirÃ  la standardizzazione delle feature numeriche\n",
        "# e la codifica one-hot di quelle categoriche in modo automatico e sicuro.\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), features_numeriche),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), features_categoriche)\n",
        "    ])"
      ],
      "metadata": {
        "id": "rpvN_IMdc09G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Divisione del set di dati noti in Training e Validation\n",
        "# -------------------------------------------------------------\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nDimensioni del set di addestramento: {X_train.shape}\")\n",
        "print(f\"Dimensioni del set di validazione: {X_val.shape}\")\n"
      ],
      "metadata": {
        "id": "Lnfr6xRQc8c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verifica Quantitativa dell'Importanza delle Features (il test decisivo):**\n",
        "    *   **Testo:** \"Per quantificare oggettivamente l'importanza predittiva di ciascuna variabile, sono stati confrontati due modelli Gradient Boosting (LightGBM): un modello 'Baseline' addestrato solo su `NumeroForo` e un modello 'Completo' addestrato su tutte le features.\"\n",
        "    <details>*   **Snippet di Codice:**\n",
        "        ```python\n",
        "        # Snippet 2: Confronto Modelli LGBM\n",
        "        import lightgbm as lgb\n",
        "        from sklearn.metrics import mean_squared_error\n",
        "        import numpy as np\n",
        "\n",
        "        df_known_force = df[df['Forza_N'].notna()]\n",
        "        X = df_known_force[features]\n",
        "        y = df_known_force[target]\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Modello Completo\n",
        "        lgbm_full = lgb.LGBMRegressor(random_state=42, verbosity=-1) # verbosity=-1 per pulire i warning\n",
        "        lgbm_full.fit(X_train, y_train)\n",
        "        rmse_full = np.sqrt(mean_squared_error(y_test, lgbm_full.predict(X_test)))\n",
        "\n",
        "        # Modello Baseline\n",
        "        lgbm_base = lgb.LGBMRegressor(random_state=42, verbosity=-1)\n",
        "        lgbm_base.fit(X_train[['NumeroForo']], y_train)\n",
        "        rmse_base = np.sqrt(mean_squared_error(y_test, lgbm_base.predict(X_test[['NumeroForo']])))\n",
        "\n",
        "        print(f\"RMSE Modello Completo: {rmse_full:.2f} N\")\n",
        "        print(f\"RMSE Modello Baseline: {rmse_base:.2f} N\")\n",
        "        ```\n",
        "    *   **Risultato:** Inserisci il grafico della **Feature Importance del modello completo**.\n",
        "    *   **Commento:** \"I risultati sono stati inequivocabili. Il modello Baseline ha ottenuto un RMSE di 6.11 N, mentre il modello Completo ha ottenuto un RMSE di 6.61 N, indicando un **peggioramento della performance del 8.13%**. Il grafico di Feature Importance (Figura X) conferma che le features geometriche del danno forniscono un contributo predittivo trascurabile una volta che l'effetto dell'usura Ã¨ noto.\"\n"
      ],
      "metadata": {
        "id": "bPPpJOleiMUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Assumo che 'full_data.csv' sia disponibile con i tuoi dati\n",
        "try:\n",
        "    df = pd.read_csv('/content/dataset_master_finalissimo.csv', sep=',')\n",
        "except FileNotFoundError:\n",
        "    print(\"ERRORE: Assicurati che il file '/content/dataset_master_finalissimo.csv' sia nella stessa cartella.\")\n",
        "    # Fallback su dati di esempio se non trova il file\n",
        "    df = pd.DataFrame({\n",
        "        'NumeroForo': range(1, 11),\n",
        "        'Forza_N': np.linspace(90, 110, 10),\n",
        "        'AreaDelaminata_unet': np.random.rand(10) * 0.1,\n",
        "        'Dmax_unet': np.random.rand(10) * 2 + 8,\n",
        "        'DF_unet': np.random.rand(10) * 0.5 + 1,\n",
        "        'ShapeFactor_unet': np.random.rand(10) * 0.005,\n",
        "        'Hu_1': np.random.rand(10) * 2, 'Hu_2': np.random.rand(10) * 4, 'Hu_3': np.random.rand(10) * 6,\n",
        "        'Hu_4': np.random.rand(10) * 6, 'Hu_5': np.random.rand(10) * 20 - 10, 'Hu_6': np.random.rand(10) * 20 - 10,\n",
        "        'Hu_7': np.random.rand(10) * 20 - 10\n",
        "    })\n",
        "\n",
        "# --- PASSO 0: Preparazione ---\n",
        "df['Forza_N'] = pd.to_numeric(df['Forza_N'], errors='coerce')\n",
        "df_known_force = df[df['Forza_N'].notna()].copy()\n",
        "\n",
        "features = [\n",
        "    'NumeroForo',\n",
        "    'AreaDelaminata_unet', 'Dmax_unet', 'DF_unet', 'ShapeFactor_unet',\n",
        "    'Hu_1', 'Hu_2', 'Hu_3', 'Hu_4', 'Hu_5', 'Hu_6', 'Hu_7'\n",
        "]\n",
        "target = 'Forza_N'\n",
        "\n",
        "X = df_known_force[features]\n",
        "y = df_known_force[target]\n",
        "\n",
        "# --- PASSO 1: Analisi di Correlazione ---\n",
        "correlation_matrix = df_known_force[features + [target]].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\",\n",
        "            annot_kws={\"size\": 8})\n",
        "plt.title('Matrice di Correlazione tra Features e Forza', fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCorrelazione delle features con la Forza:\")\n",
        "print(correlation_matrix[target].sort_values(ascending=False))\n",
        "\n",
        "\n",
        "# --- PASSO 2: Feature Importance con LightGBM ---\n",
        "\n",
        "# Divisione dati\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Modello B (Completo)\n",
        "lgbm_full = lgb.LGBMRegressor(random_state=42)\n",
        "lgbm_full.fit(X_train, y_train)\n",
        "\n",
        "# Valutazione Modello B\n",
        "preds_full = lgbm_full.predict(X_test)\n",
        "rmse_full = np.sqrt(mean_squared_error(y_test, preds_full))\n",
        "print(f\"\\nRMSE del Modello Completo (con tutte le features): {rmse_full:.4f} N\")\n",
        "\n",
        "# Visualizzazione Feature Importance\n",
        "lgb.plot_importance(lgbm_full, figsize=(10, 8), title=\"Feature Importance (Modello Completo)\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- PASSO 3: Confronto con Modello Baseline ---\n",
        "\n",
        "# Modello A (Baseline - solo usura)\n",
        "X_train_base = X_train[['NumeroForo']]\n",
        "X_test_base = X_test[['NumeroForo']]\n",
        "\n",
        "lgbm_base = lgb.LGBMRegressor(random_state=42)\n",
        "lgbm_base.fit(X_train_base, y_train)\n",
        "\n",
        "# Valutazione Modello A\n",
        "preds_base = lgbm_base.predict(X_test_base)\n",
        "rmse_base = np.sqrt(mean_squared_error(y_test, preds_base))\n",
        "print(f\"RMSE del Modello Baseline (solo con NumeroForo): {rmse_base:.4f} N\")\n",
        "\n",
        "# --- PASSO 4: Discussione ---\n",
        "improvement = (rmse_base - rmse_full) / rmse_base * 100\n",
        "print(f\"\\nAggiungere le features del danno ha migliorato la predizione del {improvement:.2f}%.\")\n",
        "\n",
        "if improvement > 5: # Soglia arbitraria\n",
        "    print(\"\\nCONCLUSIONE: SÃ¬, le caratteristiche della forma del danno (momenti, area, etc.)\")\n",
        "    print(\"contengono informazioni preziose per stimare la forza, anche dopo aver considerato l'usura.\")\n",
        "    print(\"Questo supporta l'idea di poter definire un 'limite di danno accettabile' basato sulla sua forma.\")\n",
        "else:\n",
        "    print(\"\\nCONCLUSIONE: L'effetto dominante Ã¨ l'usura ('NumeroForo').\")\n",
        "    print(\"Le caratteristiche del danno aggiungono poche informazioni aggiuntive per predire la forza.\")\n",
        "    print(\"SarÃ  piÃ¹ difficile definire un limite di danno basandosi solo sulla sua forma, indipendentemente dall'usura.\")"
      ],
      "metadata": {
        "id": "UgF4PZ-x58UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##:fine sopra preparazione dati\n",
        "\n",
        "# **Sott::o inizio settaggio modello**!!:"
      ],
      "metadata": {
        "id": "DThyIJjgdGau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#modello MLP usato all'inizio, ma mancano picchi, sembra seguire medie, usaimo un modello diverso che Ã¨ piÃ¹ centrato sulla relazione fra metriche per capire le correlazioni fra forza e danno"
      ],
      "metadata": {
        "id": "x09_LO3w5x3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Costruzione del Modello MLP (Multi-Layer Perceptron)\n",
        "# --------------------------------------------------------\n",
        "# Creiamo una pipeline che prima processa i dati e poi li passa al modello\n",
        "# Definiamo un'architettura MLP semplice ma efficace.\n",
        "\n",
        "# Trasformiamo i dati di training e validation usando il preprocessor\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_val_processed = preprocessor.transform(X_val)\n",
        "\n",
        "# Definiamo il modello Keras\n",
        "model = tf.keras.Sequential([\n",
        "    # Il numero di neuroni nel primo strato Ã¨ basato sulla forma dei dati processati\n",
        "    tf.keras.layers.Input(shape=(X_train_processed.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2), # Dropout per ridurre l'overfitting\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1) # Strato di output con 1 neurone per la regressione\n",
        "])\n",
        "\n",
        "# Compiliamo il modello\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='mean_squared_error', # Ottima loss per la regressione\n",
        "    metrics=['mean_absolute_error'] # Metrica piÃ¹ interpretabile (errore medio in N)\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Aggiungiamo un early stopping per fermare l'addestramento quando il modello smette di migliorare\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=15, restore_best_weights=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "5Lr14Sk5dLQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Addestramento del Modello\n",
        "# --------------------------------------------------------\n",
        "print(\"\\nInizio addestramento MLP...\")\n",
        "history = model.fit(\n",
        "    X_train_processed, y_train,\n",
        "    epochs=200, # Numero massimo di epoche\n",
        "    validation_data=(X_val_processed, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Addestramento completato.\")\n"
      ],
      "metadata": {
        "id": "Uwp7pQSGdlmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Valutazione del Modello\n",
        "# --------------------------------------------------------\n",
        "# Visualizziamo le curve di apprendimento per assicurarci che non ci sia overfitting\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss del Modello')\n",
        "plt.xlabel('Epoca')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mean_absolute_error'], label='Training MAE')\n",
        "plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n",
        "plt.title('Errore Assoluto Medio (MAE)')\n",
        "plt.xlabel('Epoca')\n",
        "plt.ylabel('Forza (N)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uCGEwgfLdmq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Predizione delle Forze Mancanti\n",
        "# --------------------------------------------------------\n",
        "# Usiamo il modello addestrato per predire i valori sul set df_senza_forza\n",
        "X_da_predire = df_senza_forza[features_numeriche + features_categoriche]\n",
        "\n",
        "# Processiamo i dati da predire con lo stesso preprocessor\n",
        "X_da_predire_processed = preprocessor.transform(X_da_predire)\n",
        "\n",
        "# Eseguiamo la predizione\n",
        "forze_predette = model.predict(X_da_predire_processed).flatten() # .flatten() per avere un array 1D\n"
      ],
      "metadata": {
        "id": "P3q_C6n0dtE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Creazione del DataFrame Finale\n",
        "# ---------------------------------------------------------\n",
        "# Riempiamo i valori NaN nel dataframe originale con le nostre predizioni\n",
        "df_finale = df.copy()\n",
        "df_finale.loc[df_finale['Forza_N'].isna(), 'Forza_N'] = forze_predette\n",
        "\n",
        "# Visualizziamo alcuni dei valori imputati per controllo\n",
        "print(\"\\nDataFrame originale con forze mancanti (prime 5 righe affette):\")\n",
        "print(df[df['Forza_N'].isna()].head())\n",
        "\n",
        "print(\"\\nDataFrame finale con forze imputate (stesse 5 righe):\")\n",
        "print(df_finale.loc[df_senza_forza.index].head())\n",
        "\n",
        "# Salviamo il nuovo CSV completo\n",
        "output_filename = 'dati_con_forze_imputate.csv'\n",
        "df_finale.to_csv(output_filename, index=False)\n",
        "print(f\"\\nDataset completo salvato come '{output_filename}'\")"
      ],
      "metadata": {
        "id": "ZBYhGiUId25r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##NEW\n",
        "# =============================================================================\n",
        "# CELLA 5: STUDIO ESPLORATIVO + ESPORTAZIONE/DOWNLOAD\n",
        "# =============================================================================\n",
        "# Requisiti: df_unet_geometric, df_unet_hu giÃ  creati (CELLE 3â€“4)\n",
        "# Opzionale (se hai giÃ  fatto il merge forza altrove): df_forza_maurizio\n",
        "\n",
        "import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "\n",
        "plt.close('all')\n",
        "sns.set(context=\"notebook\", style=\"whitegrid\")\n",
        "\n",
        "# ---- 1) Build preview dataframe (join geom + Hu; attacca Forza se disponibile) ----\n",
        "df_unet = pd.merge(df_unet_geometric, df_unet_hu, on=\"NumeroForo\", how=\"inner\")\n",
        "\n",
        "if \"df_forza_maurizio\" in globals() and isinstance(df_forza_maurizio, pd.DataFrame):\n",
        "    tmp_forze = df_forza_maurizio.copy()\n",
        "    if \"NumeroForo\" in tmp_forze.columns:\n",
        "        tmp_forze[\"NumeroForo\"] = (\n",
        "            tmp_forze[\"NumeroForo\"].astype(str).str.extract(r\"(\\d+)\").astype(float)\n",
        "        )\n",
        "        tmp_forze[\"NumeroForo\"] = tmp_forze[\"NumeroForo\"].astype(\"Int64\")\n",
        "    df_master_preview = df_unet.merge(\n",
        "        tmp_forze[[\"NumeroForo\",\"Forza_N\"]].drop_duplicates(\"NumeroForo\"),\n",
        "        on=\"NumeroForo\", how=\"left\"\n",
        "    )\n",
        "else:\n",
        "    df_master_preview = df_unet.copy()\n",
        "    if \"Forza_N\" not in df_master_preview.columns:\n",
        "        df_master_preview[\"Forza_N\"] = np.nan  # slot per coerenza viste\n",
        "\n",
        "# Colonne numeriche principali da studiare (adatta liberamente)\n",
        "num_cols = [c for c in [\n",
        "    \"AreaDelaminata_unet\",\"Dmax_unet\",\"DF_unet\",\"ShapeFactor_unet\",\n",
        "    \"Hu_1\",\"Hu_2\",\"Hu_3\",\"Hu_4\",\"Hu_5\",\"Hu_6\",\"Hu_7\",\"Forza_N\"\n",
        "] if c in df_master_preview.columns]\n",
        "\n",
        "print(f\"Rows in preview: {len(df_master_preview)} | numeric cols: {len(num_cols)}\")\n",
        "\n",
        "# ---- 2) Distributions (hist+KDE) ----\n",
        "for c in num_cols:\n",
        "    fig = plt.figure(figsize=(6,4))\n",
        "    ax = sns.histplot(df_master_preview[c].dropna(), kde=True)\n",
        "    ax.set_title(f\"Distribution â€” {c}\")\n",
        "    ax.set_xlabel(c); ax.set_ylabel(\"count\")\n",
        "    plt.show()\n",
        "\n",
        "# ---- 3) 2-D pairwise (quick) ----\n",
        "# Hint: subset per velocitÃ  se vuoi: vars=num_cols[:6]\n",
        "if len(num_cols) >= 2:\n",
        "    g = sns.pairplot(df_master_preview[num_cols].dropna(), corner=True, diag_kind=\"hist\")\n",
        "    g.fig.suptitle(\"Pairwise (subset numeric)\", y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "# ---- 4) Correlation heatmap ----\n",
        "if len(num_cols) >= 2:\n",
        "    corr = df_master_preview[num_cols].corr(numeric_only=True)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    ax = sns.heatmap(corr, annot=False, cmap=\"viridis\", square=True)\n",
        "    ax.set_title(\"Correlation (numeric)\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# ---- 5) â€œTime seriesâ€ semplice indicizzata per foro (giusto per scorrere valori) ----\n",
        "if \"Forza_N\" in df_master_preview.columns:\n",
        "    df_plot = df_master_preview.sort_values(\"NumeroForo\")\n",
        "    plt.figure(figsize=(8,3.5))\n",
        "    plt.plot(df_plot[\"NumeroForo\"], df_plot[\"Forza_N\"], marker=\".\", linestyle=\"-\")\n",
        "    plt.title(\"Forza_N vs NumeroForo\")\n",
        "    plt.xlabel(\"NumeroForo\"); plt.ylabel(\"Forza_N\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# ---- 6) Salvataggi + download ----\n",
        "OUT_DIR = Path(\"/content\"); OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "p_geom = OUT_DIR/\"unet_geometria.csv\"\n",
        "p_hu   = OUT_DIR/\"unet_hu.csv\"\n",
        "p_prev = OUT_DIR/\"dataset_master_preview.csv\"\n",
        "\n",
        "df_unet_geometric.to_csv(p_geom, index=False)\n",
        "df_unet_hu.to_csv(p_hu, index=False)\n",
        "df_master_preview.to_csv(p_prev, index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" -\", p_geom)\n",
        "print(\" -\", p_hu)\n",
        "print(\" -\", p_prev)\n",
        "\n",
        "# trigger download (Colab)\n",
        "for p in [p_geom, p_hu, p_prev]:\n",
        "    try:\n",
        "        files.download(str(p))\n",
        "    except Exception as e:\n",
        "        print(f\"(Info) Download programmatico non riuscito per {p.name}. Scaricalo dal pannello Files. Dettagli: {e}\")\n",
        "\n",
        "# ---- 7) (Opzionale) Report HTML EDA con ydata-profiling ----\n",
        "# NB: generare il report puÃ² richiedere un po' di tempo per molte colonne/righe\n",
        "try:\n",
        "    %pip -q install ydata-profiling\n",
        "    from ydata_profiling import ProfileReport\n",
        "    profile = ProfileReport(df_master_preview[num_cols + [\"NumeroForo\"]], title=\"UNet + Forze â€” Profiling\")\n",
        "    html_path = OUT_DIR/\"eda_report_unet_forze.html\"\n",
        "    profile.to_file(html_path)\n",
        "    print(\"EDA report:\", html_path)\n",
        "    try:\n",
        "        files.download(str(html_path))\n",
        "    except Exception as e:\n",
        "        print(f\"(Info) Download HTML non auto-avviato: {e}\")\n",
        "except Exception as e:\n",
        "    print(\"(Opzionale) ydata-profiling non installato/errore:\", e)\n"
      ],
      "metadata": {
        "id": "uQfdfF6gfMtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Analisi Grafica della Coerenza dei Valori di Forza Imputati\n",
        "'''\n",
        "Per questa analisi, utilizzerÃ² il file originale e quello nuovo, `/content/dati_con_forze_imputate.csv.csv`.\n",
        "\n",
        "```python'''\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Impostazioni grafiche per una migliore leggibilitÃ \n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# 1. Caricamento dei due dataset per il confronto\n",
        "# ----------------------------------------------------\n",
        "try:\n",
        "    df_original = pd.read_csv('/content/dataset_master_finalissimo.csv')\n",
        "    df_imputato = pd.read_csv('/content/dati_con_forze_imputate.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Assicurati che entrambi i file '/content/dataset_master_finalissimo.csv' e '/content/dati_con_forze_imputate.csv.csv' siano presenti.\")\n",
        "    exit()\n",
        "\n",
        "# Aggiungiamo un flag per distinguere i dati originali, imputati e reali\n",
        "df_imputato['Stato'] = 'Reale'\n",
        "# Identifichiamo gli indici dove la forza era mancante nel file originale\n",
        "indici_mancanti = df_original[df_original['Forza_N'].isna()].index\n",
        "df_imputato.loc[indici_mancanti, 'Stato'] = 'Imputato'\n",
        "\n",
        "print(\"Dati caricati e pronti per la visualizzazione.\")\n",
        "\n",
        "# 2. Grafico 1: Andamento Generale della Forza\n",
        "# ----------------------------------------------------\n",
        "# Questo grafico ci mostra la sequenza completa delle forze, evidenziando i punti che abbiamo predetto.\n",
        "# Ci aspettiamo che i punti rossi (imputati) si inseriscano in modo fluido nell'andamento generale.\n",
        "\n",
        "plt.figure(figsize=(18, 8))\n",
        "sns.scatterplot(\n",
        "    data=df_imputato,\n",
        "    x='NumeroForo',\n",
        "    y='Forza_N',\n",
        "    hue='Stato',\n",
        "    palette={'Reale': 'blue', 'Imputato': 'red'},\n",
        "    s=20, # Dimensione dei punti\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title('Andamento Generale della Forza (Valori Reali vs Imputati)', fontsize=16)\n",
        "plt.xlabel('Numero del Foro (Sequenza Esperimento)', fontsize=12)\n",
        "plt.ylabel('Forza (N)', fontsize=12)\n",
        "plt.legend(title='Stato del Dato')\n",
        "plt.show()\n",
        "\n",
        "# 3. Grafico 2: Confronto delle Distribuzioni (Violin Plot)\n",
        "# -----------------------------------------------------------------\n",
        "# Questo grafico Ã¨ FONDAMENTALE. Mostra la distribuzione dei valori di forza per ogni tipo di materiale.\n",
        "# Vogliamo vedere se la distribuzione dei valori imputati (in rosso) si sovrappone bene a quella\n",
        "# dei valori reali (in blu) all'interno dello stesso gruppo di materiale.\n",
        "\n",
        "# Ordiniamo i materiali per una visualizzazione logica\n",
        "material_order = sorted(df_imputato['TipoMateriale'].unique())\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "sns.violinplot(\n",
        "    data=df_imputato,\n",
        "    x='TipoMateriale',\n",
        "    y='Forza_N',\n",
        "    hue='Stato',\n",
        "    split=True, # Divide il violino a metÃ  per un confronto diretto\n",
        "    inner='quart', # Mostra i quartili all'interno\n",
        "    palette={'Reale': 'skyblue', 'Imputato': 'salmon'},\n",
        "    order=material_order\n",
        ")\n",
        "plt.title('Distribuzione delle Forze per Tipo di Materiale', fontsize=16)\n",
        "plt.xlabel('Tipo di Materiale', fontsize=12)\n",
        "plt.ylabel('Forza (N)', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Stato del Dato', loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Grafico 3: Relazione Forza vs. Area Delaminata\n",
        "# -----------------------------------------------------------------\n",
        "# Verifichiamo se i punti imputati seguono la stessa \"nuvola\" di punti dei dati reali quando\n",
        "# plottiamo la forza contro una delle feature piÃ¹ importanti.\n",
        "\n",
        "plt.figure(figsize=(18, 8))\n",
        "sns.scatterplot(\n",
        "    data=df_imputato,\n",
        "    x='AreaDelaminata_unet',\n",
        "    y='Forza_N',\n",
        "    hue='Stato',\n",
        "    style='TipoMateriale', # Usiamo stili diversi per ogni materiale\n",
        "    palette={'Reale': 'black', 'Imputato': 'red'},\n",
        "    s=50,\n",
        "    alpha=0.8\n",
        ")\n",
        "plt.title('Correlazione Forza vs. Area Delaminata (Valori Reali vs Imputati)', fontsize=16)\n",
        "plt.xlabel('Area Delaminata (mmÂ²)', fontsize=12)\n",
        "plt.ylabel('Forza (N)', fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9UFLWmQEfMYR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}