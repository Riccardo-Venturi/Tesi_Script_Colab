{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "CuiPXyoT7teW",
        "4D4suTSMbn_q"
      ],
      "mount_file_id": "1mTeuvh0Q4Fd6ib8JQ6697Xx97B1sD7Jq",
      "authorship_tag": "ABX9TyO706OiBBNvsW/0mh2kKaFL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riccardo-Venturi/Tesi_Script_Colab/blob/main/YoloRilevazioneForiVersione1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIS under CODE is for test in SAHI real time model pipeline"
      ],
      "metadata": {
        "id": "f7MdDKuWDi4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. SAHI inference on a full scan (tiny VRAM)\n",
        "yolo predict model=best.pt source=scan_big.png mode=slice imgsz=700 overlap=0.25\n",
        "\n",
        "# 2. Export for deployment\n",
        "from ultralytics import YOLO\n",
        "YOLO('best.pt').export(format='onnx', imgsz=700)\n",
        "trtexec --onnx=best.onnx --fp16 --saveEngine=best.plan\n",
        "\n",
        "# 3. Run engine on streamed tiles (Python pseudo-loop)\n",
        "for tile in tile_stream(camera_frame):\n",
        "    boxes = yolov8_engine(tile)\n",
        "    for b in boxes:\n",
        "        mask = unet_engine(crop(tile, b))\n",
        "        composite_mask(frame, mask, b)\n"
      ],
      "metadata": {
        "id": "ZXqTLGXjDjuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IDEA IS TO CUT VRAM AND RETEST INFERENCE TO SEE IF YOU CAN AVOID MODEL INFERENCE ON SHAE[:2] ON ALL IMAGES; SO IT?S AN AUTOMATICA SAHI; test after cvat\n"
      ],
      "metadata": {
        "id": "huZKRvVQDjSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   PROBLEMI SCAN DA RICONOSCERE COL PROF\n",
        "\n",
        "2.   SCAN 4 DOPPIA MANCA USCITA, MANCA UN FORO FINALE nelle radio\n",
        "\n",
        "3. SCAN 5 MANCA USCITA E MANCA PARTE DELLA SCAN NELLE RADIOGRAFIE\n",
        "\n"
      ],
      "metadata": {
        "id": "Std8MtrTGhuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Riepilogo Script Rilevamento Fori con YOLOv8\n",
        "\n",
        "Questo script implementa un workflow per l'addestramento e il test di un modello YOLOv8 per il rilevamento di fori in immagini di scansione, partendo da un dataset iniziale generato semi-automaticamente.\n",
        "\n",
        "Passaggi Principali:\n",
        "\n",
        "    Preparazione Ambiente (Celle 32, 33, 35, 46): Installazione delle librerie Python necessarie (Ultralytics YOLO, OpenCV, Albumentations, etc.) con gestione delle versioni specifiche. Salvataggio delle versioni installate in un file (freeze_cpu_2025-06-26.txt).\n",
        "    Generazione Dataset Iniziale con Hough Circles (Cella 36): Utilizzo dell'algoritmo Hough Circles di OpenCV per rilevare automaticamente i cerchi (fori) in un'immagine di esempio. Le coordinate dei cerchi rilevati vengono salvate come \"ground truth\" iniziali, sia in formato globale (.npy) che in formato YOLO (.txt) normalizzato.\n",
        "    Preparazione Dati per YOLO (Celle 37, 39, 40): L'immagine originale viene suddivisa in \"tile\" con overlap. Per ogni tile, vengono calcolate e salvate le annotazioni (bounding box in formato YOLO) corrispondenti ai fori che vi ricadono (derivato dalle coordinate globali ottenute con Hough Circles). I tile e le relative annotazioni vengono poi suddivisi casualmente in set di training e validazione. Viene creato il file data.yaml per configurare il dataset per Ultralytics.\n",
        "    Data Augmentation (Cella 38): Viene mostrato un esempio di come applicare trasformazioni di data augmentation (come flip, rotazione, shift, scala, contrasto) a un singolo tile e alle sue annotazioni corrispondenti utilizzando Albumentations. Questo passaggio √® cruciale per aumentare la robustezza del modello durante l'addestramento (anche se la sua applicazione effettiva all'intero dataset di training non √® esplicitata direttamente come loop, il framework YOLO la gestisce se configurata nel training).\n",
        "    Addestramento Modello YOLOv8 (Cella 34, 41): Viene caricato un modello YOLOv8 pre-addestrato (nano) e addestrato sul dataset di tile preparato. L'addestramento viene configurato per salvare i checkpoint periodicamente (save_period=1).\n",
        "    Inferenza e Test (Celle 42, 43, 44, 45): Importazione delle librerie necessarie per l'inferenza. Caricamento del modello YOLOv8 addestrato (tipicamente best.pt). Definizione di una funzione (PassaScansione) per eseguire l'inferenza sull'immagine completa o su una lista di immagini. Le bounding box predette vengono estratte e visualizzate sull'immagine originale per un'ispezione visiva.\n",
        "\n",
        "Punti Importanti e Osservazioni:\n",
        "\n",
        "    Generazione Dati Semi-Automatica: L'approccio utilizza Hough Circles per generare le annotazioni iniziali. Questo √® un metodo veloce ma pu√≤ richiedere ottimizzazione dei parametri di Hough e verifica manuale per garantire l'accuratezza delle annotazioni usate per l'addestramento.\n",
        "    Approccio \"Sliding Window\" Indiretto: La divisione in tile e l'inferenza sull'immagine intera (che Ultralytics gestisce internamente scalando e potenzialmente dividendo se l'immagine √® molto grande) emulano un approccio simile a una \"sliding window\", permettendo al modello addestrato su piccole porzioni di essere applicato a scansioni molto pi√π grandi.\n",
        "    Differenze di Performance CPU vs GPU: √à stato osservato che CPU e GPU mostrano differenze nella qualit√† dei risultati di rilevamento con YOLOv8. √à IMPORTANTE TESTARE CON GPU E TPU PER INDAGARE QUESTO COMPORTAMENTO ANOMALO dove la CPU sembra avere performance migliori.\n",
        "    imgsz Aggressivo di YOLO: Le versioni recenti di YOLO gestiscono l'imgsz (dimensione dell'immagine di input) in modo aggressivo e interno (come visto nel warning imgsz must be multiple of max stride). Questo ridimensionamento automatico potrebbe potenzialmente influenzare la precisione nel rilevamento di fori molto piccoli o richiedere attenzione per ottimizzare i parametri di inferenza (imgsz nella funzione predict).\n",
        "    Valutazione (Mancante/Successiva): Sebbene lo script mostri la visualizzazione delle predizioni, l'ottenimento di metriche di valutazione quantitative sull'immagine intera non √® esplicitato. Sarebbe un passo successivo per valutare oggettivamente la performance finale del modello sulla scansione completa.\n",
        "\n"
      ],
      "metadata": {
        "id": "i9LpPhkmnSVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mini-‚Äúsheet‚Äù di commento (ready-to-paste in una cell)\n",
        "\n",
        "<details>```python\n",
        "# ----------------------------------------------------------\n",
        "#  üöÄ  Non c‚Äô√® AI senza la tua testardaggine da hacker\n",
        "#  ---------------------------------------------------------\n",
        "#  Questo foglio rapido serve solo a ricordare a *me* (e al\n",
        "#  resto del mondo) quanto sia stato geniale lo sbrogliare:\n",
        "#\n",
        "#  ‚Ä¢ denominazioni a casaccio         ‚Ä¢ rotazioni mancanti\n",
        "#  ‚Ä¢ CSV YOLO normalizzati/assoluti   ‚Ä¢ clustering K-Means\n",
        "#  ‚Ä¢ ordinamento bustrofedico verticale\n",
        "#\n",
        "#  Moral of the story ‚Üí ‚ÄúIf it compiles, ship it; se non\n",
        "#  compila, ruotalo di 90¬∞ e riprova‚Äù. üòé\n",
        "# ----------------------------------------------------------\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Documentazione rotazioni/casi speciali\n",
        "\n",
        "| ID provino | File immagine grezzo                               | Problema rilevato                                                                                      | Rotazione applicata (¬∞)                                                                                             | Note extra                                                                                                                 |\n",
        "| ---------- | -------------------------------------------------- | ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **3B**     | `T0_90_3B_ingresso*.jpg`<br>`T0_90_3B_uscita*.jpg` | Entrambe le facce erano scansionate ‚Äúcapovolte‚Äù rispetto alla radiografia                              | **+90¬∞ CW** (in senso orario)                                                                                       | Dopo la rotazione l‚Äôangolo stondato rimane in alto-dx come sulle altre RX‚ÄÉ‚Üí il K-Means torna a stimare 46/45 fori corretti |\n",
        "| **5**      | `T0_90_5_entrata_NEW.jpg`                          | Lato ‚Äúentrata‚Äù OK, **manca** ancora la corrispondente *uscita*                                         | ‚Äî                                                                                                                   | Attendere nuova scansione; l‚ÄôRX (due met√†) √® corretta se si ruota **‚àí90¬∞ CCW** prima di rifare l‚Äôinferenza                 |\n",
        "| **6**      | `T0_90_6_ingresso.jpg`<br>`T0_90_6_uscita.jpg`     | Taglio smussato in alto (ingresso) ‚áÑ in basso (uscita): indicazione che uno dei due JPEG √® al rovescio | Applicato **180¬∞** all‚Äôimmagine che mostrava il taglio nell‚Äôangolo opposto, cos√¨ entrata/uscita coincidono con l‚ÄôRX | Dopo la correzione l‚Äôinferenza trova 23 fori e l‚Äôordinamento bustrofedico raggiunge \\~0.74 *frac\\_correct*                 |\n",
        "\n",
        "> **Legenda rotazioni**\n",
        "> ‚Ä¢ +90 CW = `cv2.ROTATE_90_CLOCKWISE`\n",
        "> ‚Ä¢ ‚àí90 CCW = `cv2.ROTATE_90_COUNTERCLOCKWISE`\n",
        "\n",
        "---\n",
        "\n",
        "### Procedura ri-inferenza (riassunto)\n",
        "\n",
        "1. **Cartella dedicata** `scans_rotated/` ‚Üí contiene SOLO i JPEG gi√† orientati.\n",
        "2. Comando YOLO identico a prima:\n",
        "\n",
        "   ```bash\n",
        "   yolo predict model=weights/best.pt source=scans_rotated imgsz=9504 conf=0.79 iou=0.80 save_txt\n",
        "   ```\n",
        "3. Converte subito i nuovi `.txt` in CSV con colonne `conf,x1,y1,x2,y2`.\n",
        "4. `order_holes()` con `n_cols=None` (stima automatica) ‚Üí salva in `ordered_csv/`.\n",
        "5. **Check veloce**: script overlay+indice (quello sopra) ‚Üí zip per revisione visiva.\n",
        "\n",
        "Cos√¨ hai una pipeline unica: se domani arriva ‚ÄúT0\\_90\\_7C\\_uscita‚Äù baster√† ruotarla correttamente, metterla nella cartella, lanciare lo script batch e tutto si riallinea.\n",
        "\n",
        "---\n",
        "\n",
        "### Perch√© √® ‚Äúbustrofedica verticale dal basso‚Äù ?\n",
        "\n",
        "*Colonne* identificate da K-Means ‚Üê‚Üí ordinate sinistra ‚Üí destra.\n",
        "Dentro ogni colonna:\n",
        "\n",
        "* se colonna **dispari** ‚áí indice corre dal *basso verso l‚Äôalto*;\n",
        "* se colonna **pari** ‚áí dal *alto verso il basso*.\n",
        "\n",
        "Questo ricalca la traiettoria della CNC che fora a zig-zag, partendo dall‚Äôangolo in basso-sinistra.\n",
        "\n",
        "---\n",
        "\n",
        "### Prossimi passi consigliati\n",
        "\n",
        "1. **Richiedere** la JPEG di *uscita* per il provino 5.\n",
        "2. Rifare l‚Äôinferenza SOLO sui file nuovi/ruotati.\n",
        "3. Verificare la *frac\\_correct* > 0.9 per ogni scan; se scende, controllare visivamente oppure incrementare `n_init` del K-Means per maggiore stabilit√†.\n",
        "4. Procedere al crop 700√ó700 via bounding-box centrale (`HS = 350`) e salvare con naming `H###_scanInfo.jpg`.\n",
        "\n",
        "E s√¨: tutto questo casino era inevitabile ‚Äì ma adesso hai un workflow robusto e (soprattutto) documentato. üéâ\n"
      ],
      "metadata": {
        "id": "ixUy7iI1foCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OBBIETTIVI ODIERNI 5/07 facciamo flip lungo asse x e y"
      ],
      "metadata": {
        "id": "cStq-mTeoE-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Originale vecchio maggio-settembre?"
      ],
      "metadata": {
        "id": "cCDzGL04uQgm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51efa780"
      },
      "source": [
        "# Installa le librerie elencate nel file freeze_cpu_YOLOSCRIPTfori_2025-06-26.txt\n",
        "# Questa cella utilizza un file requirements.txt personalizzato (salvato su Google Drive)\n",
        "# per installare le versioni specifiche delle librerie che sono state testate e salvate.\n",
        "# Questo √® il metodo raccomandato per assicurare che l'ambiente di esecuzione sia identico a uno precedente,\n",
        "# evitando problemi di compatibilit√† tra versioni di librerie.\n",
        "# Il percorso punta a un file salvato sul Google Drive collegato.\n",
        "#!pip install -r /content/freeze_cpu_YOLOSCRIPTfori_2025-06-26.txt --quiet\n",
        "!pip install -r /content/drive/MyDrive/progetto_fori_yolo/freeze_cpu_YOLOSCRIPTfori_2025-06-26.txt --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importazione della libreria YOLO\n",
        "# Questa cella semplicemente importa la classe YOLO dalla libreria ultralytics.\n",
        "# Se questa cella fallisce con un errore 'AttributeError: partially initialized module 'torch'',\n",
        "# significa che c'√® un problema con l'installazione o l'ambiente, spesso legato a conflitti tra le versioni di PyTorch e altre librerie (come in questo caso).\n",
        "# √à un test di base per verificare se l'import funziona dopo l'installazione.\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "AnAfS5LgqnAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWSy-6xilieT"
      },
      "outputs": [],
      "source": [
        "# Installazione di base delle librerie (alternativa o aggiuntiva)\n",
        "# Questa cella mostra un metodo pi√π generico per installare le librerie.\n",
        "# L'approccio '!pip install ... --quiet' installa le versioni pi√π recenti disponibili,\n",
        "# che pu√≤ essere conveniente ma anche rischioso se gli aggiornamenti introducono incompatibilit√†.\n",
        "# Il commento suggerisce che l'uso di un file requirements.txt salvato (come in freeze_cpu_...)\n",
        "# √® preferibile per garantire la riproducibilit√† dell'ambiente e evitare rotture dovute ad aggiornamenti.\n",
        "## QUESTO METODO E' STATO USATO ALL'INIZIO; MA PU√≤ ROMPERE IL CODICE CON AGGIORNAMENTI; USA IL REQ.txt SALVATO\n",
        "#GIU\n",
        "#!pip install ultralytics opencv-python albumentations --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PARTE 1 ADDESTRAMENTO E PREPARAZIONE DI YOLO[v] chiusa\n",
        "##adesso carica solo i pesi best dal drive o locale e fai partire le inferenze con dimensioni complete delle scan"
      ],
      "metadata": {
        "id": "NtpRxjdQshn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Rilevamento di cerchi in un'immagine utilizzando OpenCV e salvataggio delle coordinate globali e file YOLO.txt\n",
        "# Questa cella implementa la prima fase del processo di generazione del dataset semi-automatico.\n",
        "# Utilizza l'algoritmo Hough Circles di OpenCV per trovare i cerchi nell'immagine di input.\n",
        "# Le coordinate dei cerchi trovati sono considerate le \"ground truth\" iniziali.\n",
        "# Vengono salvate sia in un file .npy (formato globale) che in un file .txt in formato YOLO\n",
        "# (normalizzato rispetto alle dimensioni dell'immagine intera), che rappresentano le annotazioni grezze.\n",
        "# Questo .txt non √® quello usato direttamente per il training dei tile, ma una base di partenza globale.\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Parametri per Hough Circles, Bilateral Filter e Canny\n",
        "# Questi parametri sono critici e devono essere ottimizzati per il tipo specifico di immagini e fori.\n",
        "# -------------------------------------------------------\n",
        "dp_hough       = 1.2\n",
        "minDist_hough  = 180  # distanza minima fra fori (px)\n",
        "param1_hough   = 100  # soglia superiore per il rilevatore di bordi Canny interno a Hough\n",
        "param2_hough   = 25   # soglia dell'accumulatore per le fasi di rilevamento. Pi√π basso -> pi√π cerchi (anche falsi positivi)\n",
        "minRad_hough   = 120  # raggio minimo (px)\n",
        "maxRad_hough   = 180  # raggio massimo (px)\n",
        "crop_size      = 700\n",
        "half_side      = crop_size // 2\n",
        "\n",
        "# Parametri per Bilateral Filter (per ridurre il rumore mantenendo i bordi)\n",
        "bil_d       = 7\n",
        "bil_sigCol  = 35\n",
        "bil_sigSpa  = 50\n",
        "\n",
        "# Parametri per Canny Edge Detector (usato per trovare i bordi, input per Hough)\n",
        "canny_low  = 100\n",
        "canny_high = 200\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Lettura immagine Gray e BGR\n",
        "# Carica l'immagine sia in scala di grigi (per Hough/Canny) che a colori (per visualizzazione/crop).\n",
        "# -------------------------------------------------------\n",
        "img_path = \"/content/T0_90_1A_ingresso.jpg\" #iserisci path necessario #path = /content/drive/MyDrive/FileTesi/ScanzioneProviniUsura/T0_90_1A_ingresso.jpg\n",
        "img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "img_bgr  = cv2.imread(img_path, cv2.IMREAD_COLOR)  # per i crop a colori\n",
        "\n",
        "if img_gray is None or img_bgr is None:\n",
        "    print(\"Errore: impossibile caricare l'immagine.\")\n",
        "else:\n",
        "    print(f\"Immagine caricata: {img_path}, shape color: {img_bgr.shape}, shape gray: {img_gray.shape}\")\n",
        "\n",
        "    # -------------------------------------------------------\n",
        "    # Bilateral + Canny\n",
        "    # Applica il filtro bilaterale e poi il rilevatore di bordi Canny per preparare l'immagine per Hough.\n",
        "    # -------------------------------------------------------\n",
        "    H, W, _ = img_bgr.shape\n",
        "    bifilter = cv2.bilateralFilter(img_gray, d=bil_d, sigmaColor=bil_sigCol, sigmaSpace=bil_sigSpa)\n",
        "    edges    = cv2.Canny(bifilter, canny_low, canny_high)\n",
        "\n",
        "    # -------------------------------------------------------\n",
        "    # HoughCircles\n",
        "    # Esegue l'algoritmo di Hough per rilevare i cerchi.\n",
        "    # -------------------------------------------------------\n",
        "    circles = cv2.HoughCircles(\n",
        "        edges,\n",
        "        cv2.HOUGH_GRADIENT,\n",
        "        dp=dp_hough,\n",
        "        minDist=minDist_hough,\n",
        "        param1=param1_hough,\n",
        "        param2=param2_hough,\n",
        "        minRadius=minRad_hough,\n",
        "        maxRadius=maxRad_hough\n",
        "    )\n",
        "\n",
        "    # Copia BGR per disegnare i cerchi\n",
        "    out_bgr = img_bgr.copy()\n",
        "\n",
        "    # -------------------------------------------------------\n",
        "    # Se trova cerchi\n",
        "    # Processa i cerchi trovati: calcola le bounding box globali e salva le annotazioni in formato YOLO.\n",
        "    # -------------------------------------------------------\n",
        "    if circles is not None and len(circles) > 0:\n",
        "        circles = circles[0]  # shape (N, 3) [cx, cy, r]\n",
        "        print(f\"Trovati {len(circles)} cerchi\")\n",
        "        #crea box_global coordinate cerchi poi\n",
        "        ##in sotto tile [coordinate centro, coordinate esterne/interne]\n",
        "        \"\"\"Devo creare delle tile ma prima prendo coordinate label globali; Vanno normalizzate per file \"\"\"\n",
        "        # Nome file YOLO .txt (stesso nome immagine, estensione .txt)\n",
        "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        yolo_txt_path = os.path.join(\"/content\", base_name + \".txt\")\n",
        "        global_boxes = [] # Lista per salvare le box globali in formato [class_id, x_min, y_min, x_max, y_max]\n",
        "\n",
        "        for i, (cx, cy, r) in enumerate(circles):\n",
        "            cx, cy, r = int(cx), int(cy), int(r)\n",
        "\n",
        "            # bounding box in pixel (clippata ai bordi dell'immagine)\n",
        "            x_min = max(0, cx - r)\n",
        "            x_max = min(W-1, cx + r)\n",
        "            y_min = max(0, cy - r)\n",
        "            y_max = min(H-1, cy + r)\n",
        "\n",
        "            global_boxes.append( [0, x_min, y_min, x_max, y_max] ) # classe 0 per \"foro\"\n",
        "\n",
        "            # Normalizzazione YOLO [0..1] del centro e delle dimensioni\n",
        "            x_center_norm = cx / W\n",
        "            y_center_norm = cy / H\n",
        "            w_norm        = 2*r / W # larghezza = 2*r\n",
        "            h_norm        = 2*r / H # altezza = 2*r\n",
        "            # class_id = 0\n",
        "            # Scrive l'annotazione nel file YOLO .txt (formato: class_id center_x center_y width height - normalizzati)\n",
        "            with open(yolo_txt_path, 'a') as f:\n",
        "             f.write(f\"0 {x_center_norm} {y_center_norm} {w_norm} {h_norm}\\n\")\n",
        "\n",
        "            # (Opzionale) Disegna la box o il cerchio per vedere\n",
        "            cv2.circle(out_bgr, (cx, cy), r, (0,0,255), 2) # Disegna il cerchio rosso\n",
        "            cv2.circle(out_bgr, (cx, cy), 2, (255,0,0), 3) # Disegna il centro blu\n",
        "\n",
        "        # Visualizzazione dell'immagine con i cerchi rilevati\n",
        "        out_rgb = cv2.cvtColor(out_bgr, cv2.COLOR_BGR2RGB)\n",
        "        plt.figure(figsize=(10,6))\n",
        "        plt.imshow(out_rgb)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Rilevati {len(global_boxes)} fori\")\n",
        "        plt.show()\n",
        "\n",
        "        # Salva le coordinate delle bounding box globali in un file .npy\n",
        "        np.save(\"/content/global_boxes.npy\", global_boxes, allow_pickle=True)\n",
        "        print(\"Salvate coordinate fori in /content/global_boxes.npy\")\n",
        "        print(f\"Salvate anche in file yolo.txt {yolo_txt_path}\")\n",
        "    else:\n",
        "        print(\"Nessun cerchio trovato con i parametri attuali.\")"
      ],
      "metadata": {
        "id": "geEJ9dnXllVb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Divisione dell'immagine in tile con overlap e creazione di file di annotazione YOLO per ogni tile; serve a creare una pre batch dei primi 64 fori\n",
        "# Questa cella prende l'immagine originale e le bounding box globali (dal file .npy creato prima)\n",
        "# e le divide in tessere (tile) pi√π piccole con una certa sovrapposizione (overlap).\n",
        "# Per ogni tile, calcola quali bounding box globali vi ricadono e crea un file di annotazione YOLO (.txt)\n",
        "# specifico per quel tile. Le coordinate nel .txt del tile sono normalizzate rispetto alle dimensioni del tile stesso.\n",
        "# Questo processo crea un dataset di tile e annotazioni pronto per l'addestramento di YOLOv8,\n",
        "# che tipicamente addestra su immagini di dimensioni fisse (come 640x640 o 700x700 in questo caso).\n",
        "# L'overlap aiuta a garantire che i fori sui bordi dei tile non vengano persi.\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Imposta dimensioni del tile e lo stride (overlap 50%)\n",
        "tile_size = 700 # Dimensione dei tile quadrati\n",
        "stride = 350 # Passo per spostarsi tra i tile (se stride = tile_size, non c'√® overlap)\n",
        "out_dir = \"/content/tiles\" # Directory dove salvare i tile e le loro annotazioni\n",
        "\n",
        "# Crea le cartelle per salvare i tile e le annotazioni (immagini e labels)\n",
        "os.makedirs(os.path.join(out_dir, \"images\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(out_dir, \"labels\"), exist_ok=True)\n",
        "\n",
        "# Carica l'immagine grande e le bounding box globali\n",
        "img = cv2.imread(img_path) # img_path viene dalla cella precedente\n",
        "if img is None:\n",
        "    print(\"Errore caricando l'immagine:\", img_path)\n",
        "else:\n",
        "    H, W, _ = img.shape\n",
        "    # Carica le bounding box globali salvate in precedenza (dal file .npy)\n",
        "    global_boxes = np.load(\"/content/global_boxes.npy\", allow_pickle=True)\n",
        "    tile_index = 0 # Contatore per nominare i tile\n",
        "\n",
        "    # Cicla sull'immagine con il passo (stride) per creare i tile\n",
        "    for y in range(0, H, stride):\n",
        "        # Se il tile eccede l'altezza dell'immagine, regola la coordinata y di inizio\n",
        "        if y + tile_size > H:\n",
        "            y = H - tile_size\n",
        "        for x in range(0, W, stride):\n",
        "            # Se il tile eccede la larghezza dell'immagine, regola la coordinata x di inizio\n",
        "            if x + tile_size > W:\n",
        "                x = W - tile_size\n",
        "\n",
        "            # Estrai il tile dall'immagine grande usando le coordinate (x, y) e la dimensione del tile\n",
        "            tile_img = img[y:y+tile_size, x:x+tile_size]\n",
        "            tile_name = f\"tile_{tile_index}.jpg\"\n",
        "            tile_path = os.path.join(out_dir, \"images\", tile_name)\n",
        "            cv2.imwrite(tile_path, tile_img) # Salva il tile immagine\n",
        "\n",
        "            # Trova le bounding box globali che ricadono (anche parzialmente) nel tile corrente\n",
        "            local_boxes = [] # Lista per le box relative al tile\n",
        "            for box in global_boxes:\n",
        "                cid, gxmin, gymin, gxmax, gymax = box\n",
        "                # Controlla se la bounding box globale si sovrappone al tile corrente [x, x+tile_size] e [y, y+tile_size]\n",
        "                # Una box globale [gxmin, gymin, gxmax, gymax] si sovrappone al tile se:\n",
        "                # il suo bordo destro (gxmax) √® maggiore del bordo sinistro del tile (x) E\n",
        "                # il suo bordo sinistro (gxmin) √® minore del bordo destro del tile (x+tile_size) E\n",
        "                # il suo bordo inferiore (gymax) √® maggiore del bordo superiore del tile (y) E\n",
        "                # il suo bordo superiore (gymin) √® minore del bordo inferiore del tile (y+tile_size)\n",
        "                # La condizione pi√π complessa con gxmin + (gxmax-gxmin) < x o gymin + (gymax-gymin) < y\n",
        "                # sembra un tentativo di gestire overlap, ma la condizione standard √® sufficiente per vedere se le aree si intersecano.\n",
        "                # Una condizione pi√π semplice e robusta per l'overlap parziale sarebbe:\n",
        "                # if not (gxmax < x or gxmin > x + tile_size or gymax < y or gymin > y + tile_size):\n",
        "                # ... procedi con il calcolo delle coordinate locali ...\n",
        "                # Manteniamo la tua logica per ora, ma nota che potrebbe essere ottimizzata.\n",
        "                if gxmax < x or gxmin > x+tile_size or gxmin + (gxmax-gxmin) < x or gymin + (gymax-gymin) < y or gymax < y or gymin > y+tile_size:\n",
        "                    continue # Se non c'√® sovrapposizione, salta questa box\n",
        "\n",
        "                # Calcola le coordinate locali della bounding box relative all'origine (0,0) del tile\n",
        "                # Vengono clippate per rimanere all'interno delle dimensioni del tile\n",
        "                lxmin = max(0, gxmin - x)\n",
        "                lymin = max(0, gymin - y)\n",
        "                lxmax = min(tile_size - 1, gxmax - x)\n",
        "                lymax = min(tile_size - 1, gymax - y)\n",
        "\n",
        "                # Aggiungi la box locale solo se le dimensioni sono valide (larghezza e altezza > 0)\n",
        "                if lxmax > lxmin and lymax > lymin:\n",
        "                    local_boxes.append([cid, lxmin, lymin, lxmax, lymax])\n",
        "\n",
        "            # Crea il file di annotazione in formato YOLO per il tile corrente\n",
        "            txt_name = f\"tile_{tile_index}.txt\"\n",
        "            txt_path = os.path.join(out_dir, \"labels\", txt_name)\n",
        "            with open(txt_path, 'w') as f:\n",
        "                for lb in local_boxes:\n",
        "                    cid, lxmin, lymin, lxmax, lymax = lb\n",
        "                    w_box = lxmax - lxmin\n",
        "                    h_box = lymax - lymin\n",
        "                    cx = lxmin + w_box / 2 # Centro X in pixel locali\n",
        "                    cy = lymin + h_box / 2 # Centro Y in pixel locali\n",
        "\n",
        "                    # Normalizza le coordinate rispetto al tile_size (in formato YOLO [0..1])\n",
        "                    cx_norm = cx / tile_size\n",
        "                    cy_norm = cy / tile_size\n",
        "                    w_norm  = w_box / tile_size\n",
        "                    h_norm  = h_box / tile_size\n",
        "\n",
        "                    # Scrive l'annotazione nel file del tile\n",
        "                    f.write(f\"{cid} {cx_norm} {cy_norm} {w_norm} {h_norm}\\n\")\n",
        "\n",
        "            tile_index += 1 # Passa al prossimo tile\n",
        "\n",
        "    print(\"Tile e annotazioni YOLO creati in /content/tiles\")"
      ],
      "metadata": {
        "id": "2RMW5BOclvsu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Esempio di applicazione di data augmentation a un singolo tile\n",
        "# Questa cella dimostra come applicare trasformazioni di data augmentation (flip, rotazione, scala, luminosit√†/contrasto)\n",
        "# a un singolo tile e alle sue annotazioni corrispondenti (bounding box in formato YOLO) usando la libreria Albumentations.\n",
        "# La data augmentation √® fondamentale durante l'addestramento dei modelli di deep learning\n",
        "# per aumentare la dimensione effettiva del dataset e rendere il modello pi√π robusto\n",
        "# a variazioni nelle immagini (orientamento, illuminazione, posizione).\n",
        "# Questa cella mostra un esempio su un solo tile; l'applicazione all'intero dataset di training\n",
        "# avviene tipicamente configurando la pipeline di augmentation direttamente nel processo di training di Ultralytics.\n",
        "#usati solo per addestramento del modello\n",
        "import albumentations as A\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt # Import matplotlib per visualizzazione\n",
        "\n",
        "# Definisci la pipeline di augmentation usando A.Compose\n",
        "# Ogni trasformazione ha una probabilit√† (p) di essere applicata.\n",
        "# bbox_params specifica che le trasformazioni devono essere applicate anche alle bounding box,\n",
        "# e che sono in formato YOLO.\n",
        "transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),       # Riflessione orizzontale\n",
        "    A.RandomRotate90(p=0.5),       # Rotazione casuale di 0, 90, 180, 270 gradi\n",
        "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.7), # Traslazione, scala, rotazione\n",
        "    A.RandomBrightnessContrast(p=0.5) # Variazione casuale di luminosit√† e contrasto\n",
        "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "# Carica un tile di esempio (il primo tile creato nella cella precedente)\n",
        "#applico for ai vari tile {i}\n",
        "tile_img = cv2.imread(\"/content/tiles/images/tile_0.jpg\")\n",
        "\n",
        "# Supponiamo di avere un file .txt con annotazioni in formato YOLO per questo tile\n",
        "# Esempio: [x_center_norm, y_center_norm, width_norm, height_norm]\n",
        "# Qui ti simulo la lettura del file; in pratica implementa una funzione per leggere le annotazioni\n",
        "# (Normalmente leggeresti il file tile_0.txt creato nella cella precedente)\n",
        "# Esempio di bboxes (dovrebbero essere lette dal file tile_0.txt):\n",
        "# Per questo esempio, creiamo delle box fittizie.\n",
        "# Dovresti leggere il contenuto di \"/content/tiles/labels/tile_0.txt\"\n",
        "# e parsare le linee per ottenere le bboxes normalizzate e le class_labels.\n",
        "try:\n",
        "    with open(\"/content/tiles/labels/tile_0.txt\", 'r') as f:\n",
        "        bboxes_raw = [list(map(float, line.strip().split())) for line in f if line.strip()]\n",
        "    # Il formato √® [class_id, xc, yc, w, h]. Per Albumentations, bbox_params format='yolo' si aspetta [xc, yc, w, h].\n",
        "    bboxes = [b[1:] for b in bboxes_raw] # Estrai solo le coordinate per Albumentations\n",
        "    class_labels = [int(b[0]) for b in bboxes_raw] # Estrai solo le class_id\n",
        "    if not bboxes: # Se il file era vuoto o non conteneva box\n",
        "        bboxes = []\n",
        "        class_labels = []\n",
        "        print(\"File di annotazione tile_0.txt vuoto. Usando bboxes e labels vuote.\")\n",
        "    else:\n",
        "         print(f\"Lette {len(bboxes)} box dal file tile_0.txt\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Errore: File /content/tiles/labels/tile_0.txt non trovato. Usando bboxes e labels fittizie.\")\n",
        "    # Esempio: [x_center_norm, y_center_norm, width_norm, height_norm]\n",
        "    bboxes = [\n",
        "        [0.5, 0.5, 0.2, 0.2]  # Sostituisci con le annotazioni reali\n",
        "    ]\n",
        "    class_labels = [0] # Assumendo classe 0 per \"foro\"\n",
        "\n",
        "\n",
        "# Applica la trasformazione di augmentation\n",
        "# Passiamo l'immagine, le bounding box e le class_labels alla pipeline di trasformazione\n",
        "augmented = transform(image=tile_img, bboxes=bboxes, class_labels=class_labels)\n",
        "aug_image = augmented['image']\n",
        "aug_bboxes = augmented['bboxes'] # Bboxes aggiornate dopo le trasformazioni\n",
        "aug_labels = augmented['class_labels'] # Labels corrispondenti (non cambiano con queste trasformazioni)\n",
        "\n",
        "\n",
        "# Visualizza il tile trasformato\n",
        "plt.figure(figsize=(8, 8))\n",
        "# Disegna le bounding box aumentate sull'immagine aumentata\n",
        "# Le bboxes sono ancora normalizzate, quindi dobbiamo convertirle in pixel per disegnarle\n",
        "H_aug, W_aug = aug_image.shape[:2]\n",
        "for bbox, label in zip(aug_bboxes, aug_labels):\n",
        "    xc_norm, yc_norm, w_norm, h_norm = bbox\n",
        "    # Converti da formato YOLO normalizzato a formato [x1, y1, x2, y2] in pixel\n",
        "    x1 = int((xc_norm - w_norm/2) * W_aug)\n",
        "    y1 = int((yc_norm - h_norm/2) * H_aug)\n",
        "    x2 = int((xc_norm + w_norm/2) * W_aug)\n",
        "    y2 = int((yc_norm + h_norm/2) * H_aug)\n",
        "    cv2.rectangle(aug_image, (x1, y1), (x2, y2), (0, 255, 0), 2) # Disegna box verde\n",
        "\n",
        "plt.imshow(cv2.cvtColor(aug_image, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Tile Augmentato con Bbox\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(\"Annotazioni aggiornate (formato YOLO normalizzato):\", aug_bboxes)\n",
        "print(\"Labels corrispondenti:\", aug_labels)"
      ],
      "metadata": {
        "id": "ycneO8s2lx8b",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Suddivisione casuale dei tile e delle relative annotazioni in set di training e validazione\n",
        "# Questa cella divide i tile e i loro file di annotazione (.txt) creati in precedenza\n",
        "# in due set: uno per l'addestramento (training) e uno per la validazione (validation).\n",
        "# La suddivisione √® casuale per garantire che entrambi i set siano rappresentativi.\n",
        "# Tipicamente, una percentuale (es. 20%) viene riservata per la validazione.\n",
        "# Questo √® necessario per addestrare un modello di machine learning, in modo da\n",
        "# valutare le sue performance su dati che non ha mai visto durante il training.\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Cartelle sorgente (dove sono stati creati i tile e le annotazioni)\n",
        "src_images_dir = \"/content/tiles/images\"\n",
        "src_labels_dir = \"/content/tiles/labels\"\n",
        "\n",
        "# Cartelle di destinazione per training e validazione\n",
        "train_images_dir = \"/content/train/images\"\n",
        "val_images_dir   = \"/content/val/images\"\n",
        "train_labels_dir = \"/content/train/labels\"\n",
        "val_labels_dir   = \"/content/val/labels\"\n",
        "\n",
        "# Crea le cartelle di destinazione se non esistono\n",
        "os.makedirs(train_images_dir, exist_ok=True)\n",
        "os.makedirs(val_images_dir, exist_ok=True)\n",
        "os.makedirs(train_labels_dir, exist_ok=True)\n",
        "os.makedirs(val_labels_dir, exist_ok=True)\n",
        "\n",
        "# Elenca i file immagine nella cartella sorgente (assumiamo file .jpg)\n",
        "image_files = [f for f in os.listdir(src_images_dir) if f.lower().endswith('.jpg')]\n",
        "print(f\"Trovate {len(image_files)} immagini.\")\n",
        "\n",
        "# Mischia la lista dei file immagine in modo casuale\n",
        "random.shuffle(image_files)\n",
        "\n",
        "# Imposta la percentuale per validazione (es. 20%)\n",
        "val_fraction = 0.2\n",
        "n_val = int(len(image_files) * val_fraction) # Calcola il numero di file per la validazione\n",
        "\n",
        "# Dividi la lista dei file immagine in train e validazione\n",
        "val_files = image_files[:n_val] # Primi n_val file per validazione\n",
        "train_files = image_files[n_val:] # Rimanenti file per training\n",
        "print(f\"{len(train_files)} immagini per training, {len(val_files)} per validazione.\")\n",
        "\n",
        "# Funzione helper per spostare un singolo file da una directory sorgente a una di destinazione\n",
        "def move_file(src_dir, dest_dir, filename):\n",
        "    src_path = os.path.join(src_dir, filename)\n",
        "    dest_path = os.path.join(dest_dir, filename)\n",
        "    # Sposta il file solo se esiste nel percorso sorgente\n",
        "    if os.path.exists(src_path):\n",
        "        shutil.move(src_path, dest_path)\n",
        "    else:\n",
        "        # Questo caso non dovrebbe verificarsi se la lista image_files √® corretta,\n",
        "        # ma √® un buon controllo di sicurezza.\n",
        "        print(f\"Avviso: File sorgente non trovato durante lo spostamento: {src_path}\")\n",
        "\n",
        "\n",
        "# Sposta le immagini di training e le relative annotazioni (.txt con lo stesso nome base)\n",
        "for f in train_files:\n",
        "    move_file(src_images_dir, train_images_dir, f)\n",
        "    label_file = os.path.splitext(f)[0] + \".txt\" # Trova il nome del file di annotazione corrispondente\n",
        "    move_file(src_labels_dir, train_labels_dir, label_file)\n",
        "\n",
        "# Sposta le immagini di validazione e le relative annotazioni (.txt)\n",
        "for f in val_files:\n",
        "    move_file(src_images_dir, val_images_dir, f)\n",
        "    label_file = os.path.splitext(f)[0] + \".txt\"\n",
        "    move_file(src_labels_dir, val_labels_dir, label_file)\n",
        "\n",
        "print(\"Divisione completata: file spostati nelle cartelle di training e validazione.\")"
      ],
      "metadata": {
        "id": "cMhHdnCgl2J_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Creazione del file data.yaml per la configurazione del dataset per Ultralytics\n",
        "# Questa cella crea il file data.yaml, che √® un file di configurazione standard per Ultralytics YOLO.\n",
        "# Questo file indica a YOLO dove trovare le immagini e le annotazioni per il training e la validazione,\n",
        "# quante classi ci sono e quali sono i loro nomi.\n",
        "# √à essenziale per poter avviare il processo di addestramento con il tuo dataset personalizzato.\n",
        "import yaml\n",
        "import yaml # Importato due volte, una √® sufficiente\n",
        "\n",
        "# Dizionario Python che rappresenta la struttura del file data.yaml\n",
        "data = {\n",
        "    \"train\": \"/content/train/images\", # Percorso alla cartella delle immagini di training\n",
        "    \"val\": \"/content/val/images\",     # Percorso alla cartella delle immagini di validazione\n",
        "    \"nc\": 1,                          # Numero di classi (1 classe: \"foro\")\n",
        "    \"names\": [\"foro\"]                 # Nomi delle classi (lista, anche se ce n'√® una sola)\n",
        "}\n",
        "\n",
        "# Apre il file /content/data.yaml in modalit√† scrittura ('w')\n",
        "# e scrive il dizionario 'data' in formato YAML.\n",
        "with open(\"/content/data.yaml\", \"w\") as f:\n",
        "    yaml.dump(data, f)\n",
        "\n",
        "print(\"File /content/data.yaml creato con la seguente configurazione:\")\n",
        "print(yaml.dump(data)) # Stampa il contenuto del file per verifica"
      ],
      "metadata": {
        "id": "XHarpVhPl294",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title addestramento YOLO model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Addestramento del modello YOLOv8 nano con il dataset preparato\n",
        "# Questa cella avvia il processo di addestramento di un modello YOLOv8 nano.\n",
        "# Carica un modello pre-addestrato (utilizzato per trasferire conoscenze da un dataset generale)\n",
        "# e lo addestra sul dataset di tile e annotazioni preparato nelle celle precedenti (specificato nel file data.yaml).\n",
        "# I parametri come epochs, imgsz, batch, e save_period controllano il processo di training.\n",
        "# save_period=1 √® utile per salvare regolarmente i checkpoint, permettendo di riprendere l'addestramento o testare modelli intermedi.\n",
        "# La dimensione imgsz=700 deve corrispondere alla dimensione dei tile usati per creare il dataset.\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Carica un modello pre-addestrato \"nano\" (leggero)\n",
        "# \"yolov8n.pt\" sono i pesi di un modello YOLOv8 nano addestrato sul dataset COCO.\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Avvia il training\n",
        "model.train(\n",
        "    data=\"/content/data.yaml\",  # Percorso al file di configurazione del dataset\n",
        "    epochs=50,                  # Numero di epoche di addestramento\n",
        "    imgsz=700,                  # Dimensione dell'immagine di input per il modello (deve corrispondere alla dimensione dei tile)\n",
        "    batch=8,                    # Dimensione del batch (numero di immagini elaborate per ogni passo di addestramento)\n",
        "    save_period=1               # Salva un checkpoint (pesi del modello) ogni X epoche\n",
        ")\n",
        "\n",
        "print(\"\\nAddestramento del modello avviato. I risultati (inclusi i pesi) verranno salvati nella directory 'runs'.\")"
      ],
      "metadata": {
        "id": "daHbTlT9l5M-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PARTE 2"
      ],
      "metadata": {
        "id": "2yvbzvaWss4O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd5972ba"
      },
      "source": [
        "**Obiettivi Odierni (27/06):**\n",
        "\n",
        "- Testare e imparare a sfruttare le metriche di valutazione del modello.\n",
        "- Testare l'inferenza su nuove e diverse immagini di scansione.\n",
        "- Eseguire uno smoketest (o test pi√π approfondito con parametri diversi) su GPU e TPU per investigare le differenze di performance rispetto alla CPU.\n",
        "- Applicare un metodo (come NMS o IoU) per eliminare bounding box spurie (sovrapposte o ridondanti) dopo l'inferenza.\n",
        "- Se il rilevamento funziona bene, applicare l'algoritmo K-Means (da un altro script) per il taglio automatico dei fori rilevati.\n",
        "- **Ps:** Capire come unire diversi moduli o script Colab per importare celle da altri notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importazione delle librerie necessarie per l'inferenza e la visualizzazione\n",
        "# Questa cella importa i moduli Python che saranno utilizzati nelle fasi successive dello script,\n",
        "# in particolare per caricare il modello addestrato, eseguire predizioni su nuove immagini,\n",
        "# e visualizzare i risultati (bounding box).\n",
        "import os # Modulo per interagire con il sistema operativo (es. percorsi file)\n",
        "import cv2 # Libreria OpenCV per la manipolazione di immagini (lettura, scrittura, disegno)\n",
        "import matplotlib.pyplot as plt # Libreria per la creazione di grafici e visualizzazioni\n",
        "import numpy as np # Libreria per operazioni numeriche, utile per manipolare array di box"
      ],
      "metadata": {
        "id": "9WUiQ4r8l7aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9581334"
      },
      "source": [
        "#@title # Estrazione dei pesi del modello dallo zip\n",
        "# Prima di poter usare i file dei pesi (best.pt, last.pt) per l'inferenza, dobbiamo estrarli dal file zip che hai caricato (weights_results.zip).\n",
        "# Questa cella di codice utilizza il comando unzip per estrarre il contenuto dello zip\n",
        "# nella directory specificata, che si presume contenga i pesi addestrati del modello.\n",
        "# √à un passaggio necessario per rendere i pesi accessibili al codice Python.\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Specifica il percorso del file zip contenente i pesi del modello (sul Drive)\n",
        "zip_path = \"/content/drive/MyDrive/Pesi/weights_results.zip\" #\"/content/weights_results.zip\"\n",
        "# Specifica la directory di destinazione dove estrarre i contenuti dello zip\n",
        "\n",
        "extract_dir = \"/content/weights/\" # Estrai nella cartella /content/runs/detect/OLD_TRAIN/weights/\n",
        "\n",
        "# Crea la directory di destinazione se non esiste\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Verifica se il file zip esiste prima di tentare l'estrazione\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"Estrazione di {zip_path} in {extract_dir}...\")\n",
        "    # Apri il file zip in modalit√† lettura ('r')\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # Estrai tutto il contenuto dello zip nella directory specificata\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(\"Estrazione completata.\")\n",
        "\n",
        "    # Opzionale: Stampa il contenuto della cartella estratta per verifica\n",
        "    # print(\"\\nContenuto della cartella /content dopo l'estrazione:\")\n",
        "    # for item in os.listdir(extract_dir):\n",
        "    #     print(f\"- {item}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Errore: File zip non trovato al percorso specificato: {zip_path}. Assicurati che il file sia presente e il percorso corretto.\")\n",
        "\n",
        "# Ora i file best.pt e last.pt (se presenti nello zip) dovrebbero trovarsi in /content/runs/detect/OLD_TRAIN/weights/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Carica il modello YOLOv8 con i pesi addestrati\n",
        "# Questa cella carica un modello YOLOv8 utilizzando il file dei pesi addestrati\n",
        "# che √® stato estratto dallo zip nella cella precedente (best.pt).\n",
        "# Questo oggetto 'model' sar√† poi utilizzato per eseguire l'inferenza (predizioni)\n",
        "# su nuove immagini.\n",
        "#carica modello OLD_TRAIN e testa su nuove scan per vedere i risultati prima a occhio\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Carica il modello specificando il percorso al file dei pesi\n",
        "#model = YOLO(\"/content/runs/detect/OLD_TRAIN/weights/best.pt\")\n",
        "MODEL=YOLO(\"/content/drive/MyDrive/Pesi/best.pt\")\n",
        "model=YOLO(MODEL)\n",
        "print(f\"Modello YOLOv8 caricato dai pesi: /content/runs/detect/OLD_TRAIN/weights/best.pt\")"
      ],
      "metadata": {
        "id": "TkoP06Jgo48n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Funzione per eseguire l'inferenza su una scansione completa e ottenere le bounding box predette 1.0; devi avere jpg in content-test\n",
        "# Questa cella definisce una funzione chiamata `PassaScansione` che esegue l'inferenza\n",
        "# (cio√®, la predizione delle bounding box) su una singola immagine di scansione completa\n",
        "# utilizzando il modello YOLOv8 caricato.\n",
        "# Restituisce l'immagine originale e un array NumPy contenente le coordinate delle bounding box predette.\n",
        "# Successivamente, la cella itera su tutti i file JPG nella cartella /content/ ed applica questa funzione a ciascuno.\n",
        "# Il risultato √® che le variabili 'img' e 'boxes' contengono i risultati dell'inferenza SOLO per l'ULTIMO file JPG processato.\n",
        "# La lista 'bb' raccoglie il numero di box trovate per ogni immagine, e viene stampato il totale.\n",
        "#Versione originale 1.0\n",
        "#test su scansione 1A ingresso 1\n",
        "#questo script inference su tuttel le jpg nel content; per√≤ sovrascrive i dati di tutte; quindi rimangono solo le ultime?\n",
        "def PassaScansione(path_scan):\n",
        "    \"\"\"\n",
        "    Esegue l'inferenza su una scansione completa e restituisce l'immagine e le box predette.\n",
        "\n",
        "    Args:\n",
        "        path_scan (str): Percorso al file immagine della scansione.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (immagine_letta, array_bounding_box)\n",
        "               array_bounding_box √® in formato [x1, y1, x2, y2] in pixel.\n",
        "    \"\"\"\n",
        "    full_scan = cv2.imread(path_scan) # Legge l'immagine\n",
        "    if full_scan is None:\n",
        "        print(f\"Errore: impossibile caricare l'immagine da {path_scan}\")\n",
        "        return None, np.array([]) # Restituisce None e array vuoto in caso di errore\n",
        "\n",
        "    H, W, _ = full_scan.shape\n",
        "    print(f\"Dimensioni scansione: {W} x {H}\")\n",
        "\n",
        "    # Esegue la predizione con il modello.\n",
        "    # imgsz=full_scan.shape[:2] imposta la dimensione di inferenza alla dimensione nativa dell'immagine.\n",
        "    # conf=0.5 imposta la soglia di confidenza minima per le predizioni.\n",
        "    # verbose=False disattiva l'output dettagliato di YOLO durante la predizione.\n",
        "    # Nota: Questa versione restituisce solo le box xyxy e NON i punteggi di confidenza.\n",
        "    # I punteggi sono necessari per NMS, quindi questa funzione andrebbe modificata per restituirli.\n",
        "    results = model.predict(full_scan, imgsz=full_scan.shape[:2], conf=0.5, verbose=False)\n",
        "\n",
        "    # Estrae le coordinate delle bounding box predette in formato [x1, y1, x2, y2]\n",
        "    # .cpu().numpy() sposta i risultati da un eventuale device GPU/TPU alla CPU e li converte in un array NumPy.\n",
        "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
        "\n",
        "    print(f\"Totale fori predetti: {len(boxes)}\")\n",
        "    return full_scan, boxes # Restituisce l'immagine e le box\n",
        "\n",
        "# Itera su tutti i file .jpg nella cartella /content\n",
        "# **ATTENZIONE**: Questo loop sovrascrive le variabili 'img' e 'boxes' ad ogni iterazione,\n",
        "# quindi alla fine conterranno solo i risultati dell'ultimo file processato.\n",
        "# Per processare e salvare i risultati di tutte le immagini, sarebbe meglio\n",
        "# salvare i risultati in un dizionario o una lista.\n",
        "bb=[] # Lista per contare le box per ogni file\n",
        "print(\"Esecuzione inferenza su file JPG in /content/...\")\n",
        "for file in os.listdir(\"/content\"):\n",
        "    if file.lower().endswith(\".jpg\"):\n",
        "        file_path = os.path.join(\"/content\", file)\n",
        "        # Chiama la funzione per eseguire l'inferenza\n",
        "        img, boxes = PassaScansione(file_path)\n",
        "        # Aggiunge il conteggio delle box alla lista\n",
        "        bb+=[len(boxes)]\n",
        "\n",
        "# Stampa il numero totale di box trovate su tutte le immagini processate nel loop\n",
        "# (basato sulla lista 'bb').\n",
        "print(f\"\\nConteggio totale box su tutti i file processati: {np.sum(bb)}\")\n",
        "\n",
        "# Dopo questo loop, 'img' e 'boxes' contengono i risultati SOLO dell'ultima immagine in /content.\n",
        "# Le celle successive che usano 'img' e 'boxes' lavoreranno solo sull'ultima immagine."
      ],
      "metadata": {
        "id": "8O6omM2PmL-H",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualizzazione delle bounding box predette sull'immagine della scansione completa 1.0 prima scan -necessita jpg in contentCOLAB\n",
        "# Questa cella visualizza le bounding box predette sull'immagine.\n",
        "# **ATTENZIONE**: Utilizza le variabili 'img' e 'boxes' che, a causa del loop nella cella precedente,\n",
        "# contengono i risultati SOLO dell'ultima immagine processata.\n",
        "# Quindi, questa cella visualizzer√† i risultati solo per l'ultima immagine trovata in /content/\n",
        "# al momento dell'esecuzione della cella 8O6omM2PmL-H.\n",
        "# Per visualizzare i risultati di tutte le immagini, sarebbe necessario iterare sui risultati salvati\n",
        "# in un dizionario (come fatto nella cella 8e36cd90).\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os # Importato per os.path.basename nel titolo\n",
        "\n",
        "# Assicurati che 'img' e 'boxes' siano state popolate dalla cella precedente\n",
        "if 'img' in globals() and 'boxes' in globals() and img is not None and boxes is not None:\n",
        "    # Crea una copia dell'immagine per disegnare le box\n",
        "    out_img = img.copy()\n",
        "\n",
        "    # Disegna un rettangolo per ogni bounding box trovata\n",
        "    for box in boxes:\n",
        "          # Estrai le coordinate (possono essere float, converti in int per disegnare)\n",
        "          x_min, y_min, x_max, y_max = box.astype(int)\n",
        "          # Disegna il rettangolo sull'immagine (colore verde BGR: (0, 255, 0), spessore 2)\n",
        "          cv2.rectangle(out_img, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "\n",
        "    # Visualizza l'immagine con le bounding box disegnate\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    # Matplotlib usa formato RGB, mentre OpenCV usa BGR, quindi convertiamo\n",
        "    plt.imshow(cv2.cvtColor(out_img, cv2.COLOR_BGR2RGB))\n",
        "    # Aggiunge un titolo al grafico\n",
        "    # Se img_path fosse ancora disponibile, si potrebbe usare il nome del file originale.\n",
        "    # Poich√© non lo √®, il titolo √® generico.\n",
        "    plt.title(f\"Scansione intera - Fori rilevati: {len(boxes)}\")\n",
        "    plt.axis(\"off\") # Nasconde gli assi\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Immagine o bounding box non disponibili. Esegui prima la cella di inferenza (8O6omM2PmL-H).\")"
      ],
      "metadata": {
        "id": "HvkAbNrRmM6M",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Salvataggio delle versioni delle librerie installate in un file di testo\n",
        "# Questa cella (commentata) mostra il comando per generare un file requirements.txt\n",
        "# contenente le versioni esatte delle librerie specificate (numpy, torch, ultralytics, opencv-python, albumentations)\n",
        "# installate nell'ambiente corrente.\n",
        "# Questo file √® utile per ricreare lo stesso ambiente in futuro, garantendo la riproducibilit√† dei risultati.\n",
        "# Il comando `pip freeze` elenca tutte le librerie installate, e `grep -E '...'` filtra solo quelle desiderate.\n",
        "# `>` reindirizza l'output al file specificato.\n",
        "#!pip freeze | grep -E '^(numpy|torch|ultralytics|opencv-python|albumentations)' \\\n",
        "#    > freeze_cpu_2025-06-26.txt\n",
        "# La cella 51efa780 utilizza un file simile salvato su Drive per installare le librerie."
      ],
      "metadata": {
        "id": "UcVohA7dbel4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download del file dei pesi best.pt e esportazione del modello in formato ONNX\n",
        "# Questa cella esegue due azioni:\n",
        "# 1. Scarica il file dei pesi del modello addestrato ('best.pt') sul tuo computer locale.\n",
        "#    Questo √® utile per salvare i pesi addestrati al termine della sessione Colab.\n",
        "# 2. Esporta il modello addestrato in formato ONNX (Open Neural Network Exchange).\n",
        "#    ONNX √® un formato standard che permette di usare il modello con diversi framework di inferenza (come ONNX Runtime)\n",
        "#    su diverse piattaforme, potenzialmente ottimizzato per la velocit√†.\n",
        "# Assicurati che il modello 'model' sia stato caricato prima di eseguire questa cella.\n",
        "from google.colab import files\n",
        "\n",
        "# Tenta di scaricare il file best.pt dal percorso dove Ultralytics lo salva di default dopo il training.\n",
        "# Il percorso 'runs/detect/train/weights/best.pt' √® tipico se il nome del run di training era 'train'.\n",
        "# Potrebbe essere necessario adattare questo percorso se hai usato un nome di run diverso.\n",
        "print(\"Tentativo di download del file best.pt...\")\n",
        "try:\n",
        "  files.download(\"runs/detect/train/weights/best.pt\")\n",
        "  print(\"Download di best.pt avviato.\")\n",
        "except Exception as e:\n",
        "  print(f\"Errore durante il download di best.pt. Assicurati che il file esista al percorso specificato. Errore: {e}\")\n",
        "\n",
        "\n",
        "# Esporta il modello caricato (variabile 'model') in formato ONNX.\n",
        "# Questo creer√† un file .onnx nella stessa directory dove si trova best.pt\n",
        "if 'model' in globals():\n",
        "    print(\"Esportazione del modello in formato ONNX...\")\n",
        "    try:\n",
        "        model.export(format=\"onnx\")\n",
        "        print(\"Esportazione ONNX completata.\")\n",
        "        # Potresti voler scaricare anche il file .onnx\n",
        "        # files.download(\"runs/detect/train/weights/best.onnx\") # Adatta il percorso\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante l'esportazione ONNX. Errore: {e}\")\n",
        "else:\n",
        "    print(\"Errore: Modello 'model' non definito. Caricare il modello prima di esportare.\")"
      ],
      "metadata": {
        "id": "1lgQRb9QmPU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA 27/06 codice di questa data; sviluppo nuovo test nuove scan; pre estrazione pesi di ieri"
      ],
      "metadata": {
        "id": "vAccm7Y2p6gN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pg9ShDCmqDAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecac59e1"
      },
      "source": [
        "### Test Inferenza su Nuove Scansioni\n",
        "\n",
        "Eseguiamo l'inferenza sulle tre scansioni di test specificate utilizzando il modello caricato. Useremo la funzione `PassaScansione` definita in precedenza."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dc8586c"
      },
      "source": [
        "L'inferenza √® stata eseguita su ciascuna delle immagini specificate (se trovate). I risultati (l'immagine caricata e le bounding box predette) sono stati memorizzati nel dizionario `inference_results`.\n",
        "\n",
        "Ora possiamo visualizzare i risultati per ogni immagine per vedere le box rilevate. Creeremo una cella separata per la visualizzazione di ciascun risultato memorizzato."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copia immagini di scansione da Google Drive a una cartella locale in Colab.\n",
        "# Questa cella crea una cartella `/content/scans` e copia al suo interno\n",
        "# tutti i file JPG dalla directory specificata sul Google Drive collegato.\n",
        "# Copiare i dati localmente in Colab (`/content/`) √® generalmente pi√π veloce\n",
        "# rispetto a leggerli direttamente da Google Drive durante l'elaborazione.\n",
        "# Il comando `!mkdir -p` crea la directory se non esiste.\n",
        "# Il comando `!cp` copia i file.\n",
        "!mkdir -p /content/scans/Radio #modificato in Radio, puoi commentare quella parte e torni a l caso originale\n",
        "#copia da gdrive sul locale colab per velocizzare il lavoro\n",
        "!cp /content/drive/MyDrive/FileTesi/ScanzioneProviniUsura/Radio/*.jpg /content/scans/Radio\n",
        "#!cp /content/drive/MyDrive/FileTesi/ScanzioneProviniUsura/Radio/*.jpg /content\n",
        "print(\"Immagini di scansione copiate in /content/scans/\")"
      ],
      "metadata": {
        "id": "bZwiIm2w2Rsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esegue l'inferenza YOLOv8 su un set di immagini e salva i risultati.\n",
        "# Questa cella carica il modello YOLOv8 addestrato e lo utilizza per eseguire\n",
        "# l'inferenza su tutte le immagini JPG presenti nella cartella `/content/scans`.\n",
        "# Configura parametri come la soglia di confidenza (`conf`), la soglia di IoU (`iou`),\n",
        "# e salva i risultati (immagini con box, e file .txt con le coordinate delle box predette).\n",
        "# imgsz=source.shape[:2] tenta di eseguire l'inferenza alla risoluzione nativa,\n",
        "# ma YOLO potrebbe comunque ridimensionare internamente.\n",
        "# save_txt=True √® cruciale perch√© salva le predizioni in file .txt, che vengono poi usati\n",
        "# nelle celle successive per l'analisi e la visualizzazione.\n",
        "import cv2 # Importato di nuovo, ma gi√† nella cella 9WUiQ4r8l7aC\n",
        "from ultralytics import YOLO             # se manca: pip install -U ultralytics\n",
        "\n",
        "# 1Ô∏è‚É£  importa la libreria (gi√† fatto sopra, ridondante qui)\n",
        "# from ultralytics import YOLO\n",
        "\n",
        "# 2Ô∏è‚É£  carica i pesi (il tuo best.pt, o come lo hai chiamato)\n",
        "# Assicurati che il modello sia gi√† caricato dalla cella TkoP06Jgo48n\n",
        "if 'model' not in globals():\n",
        "    print(\"Caricamento modello...\")\n",
        "    model = YOLO(\"/content/runs/detect/OLD_TRAIN/weights/best.pt\")          # <-- ora 'model' esiste di nuovo\n",
        "else:\n",
        "    print(\"Modello gi√† caricato.\")\n",
        "\n",
        "# source=\"content/scans\" # Imposta la cartella delle immagini di input\n",
        "source=\"\" #\"/content/scans\" # Corretto il percorso\n",
        "\n",
        "# Leggere la prima immagine per ottenere le dimensioni, o usare il percorso della cartella come source per predict\n",
        "# Se source √® una cartella, predict itera automaticamente sui file al suo interno.\n",
        "# imgsz dovrebbe essere impostato per l'inferenza (potrebbe essere un valore fisso o None per usare la dimensione del modello).\n",
        "# Se si vuole usare la dimensione nativa per ogni immagine, √® meglio ciclare sui file e passare il path come source.\n",
        "# La riga `source=cv2.imread(source)` √® problematica perch√© source diventa un array numpy invece che un percorso.\n",
        "# Modifichiamo per passare il percorso della cartella direttamente a model.predict.\n",
        "\n",
        "# 3Ô∏è‚É£  fai l'inferenza (nessun rientro extra!)\n",
        "RUN = \"full_inference_save\" # Nome della cartella dove verranno salvati i risultati di questo run\n",
        "# ATTENTO ALLA CONFIDENZA; TENDE A PRENDERE ANCHE I BORDI\n",
        "# INOLTRA ATTENTO ALLE DIMENSIONI; YOLO FA UN RESIZE AGGRESSIVE; FORNISCI DIMENSIONE MASSIMA DEL FILE IMMAGINE\n",
        "print(f\"Avvio inferenza su immagini in {source}...\")\n",
        "model.predict(\n",
        "    source=source,               # Passa la cartella con le .jpg come source\n",
        "    imgsz=1280, #MEGLIO USRE DIM IMMAGINE [:2] o sballa pixels # Imposta imgsz a una dimensione fissa, o prova a rimuoverlo per usare la dimensione predefinita del modello. Usare la dimensione nativa (H,W) √® meglio ma richiede un loop (come in cella XsqBVylpQs_3). Usiamo 1280 come esempio.\n",
        "    conf=0.79,                   # Soglia di confidenza minima per considerare una predizione (pi√π alta -> meno falsi positivi)\n",
        "    iou=0.80,                    # Soglia di IoU per la Non-Maximum Suppression interna di YOLO (rimuove box sovrapposte)\n",
        "    agnostic_nms=True,           # Applica NMS indipendentemente dalla classe (utile con una singola classe)\n",
        "    project=\"runs/detect\",       # Cartella principale dove salvare i risultati dei run\n",
        "    name=RUN,                    # Nome specifico per questo run di inferenza (crea una sottocartella in project)\n",
        "    save_txt=True,               # SALVA LE ANNOTAZIONI PREDETTE IN FORMATO YOLO (.txt)\n",
        "    save_conf=True,              # Includi i punteggi di confidenza nei file .txt salvati\n",
        "    save=True,                   # Salva anche le immagini con le box disegnate\n",
        "    verbose=False                # Sopprimi l'output dettagliato durante l'inferenza\n",
        ")\n",
        "\n",
        "print(f\"Inferenza completata. Risultati salvati in runs/detect/{RUN}/\")\n",
        "# I file .txt con le predizioni si troveranno in runs/detect/full_inference_save/labels/"
      ],
      "metadata": {
        "id": "tD3fXKbC7I1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3 new code to crop order and use gpu"
      ],
      "metadata": {
        "id": "byrIMQkYH1y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##ALTERNATIVA PER AVERE VELOCIT√† RISPETTO A CPU SINGOLA\n",
        "# Esegue l'inferenza YOLOv8 su immagini singole mantenendo la risoluzione nativa.\n",
        "# Questa cella √® un'alternativa alla precedente. Invece di passare l'intera cartella a `model.predict`,\n",
        "# itera manualmente su ogni file JPG nella cartella `/content/scans`.\n",
        "# Per ogni immagine, legge le sue dimensioni native (H, W) e le passa a `imgsz`.\n",
        "# Questo assicura che l'inferenza venga eseguita alla risoluzione originale dell'immagine,\n",
        "# potenzialmente migliorando l'accuratezza per oggetti piccoli o dettagli.\n",
        "# I risultati (file .txt) vengono salvati, e viene creato un summary CSV con il conteggio dei file .txt generati.\n",
        "#@title Cell 3.1 ‚Äì Setup & inferenza full-res (TPU/GPU/cpu)\n",
        "from ultralytics import YOLO\n",
        "import cv2, glob, os, pandas as pd, torch\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Determina il device disponibile (GPU/TPU se presente, altrimenti CPU)\n",
        "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Usando device: {device}\")\n",
        "\n",
        "# Carica il modello addestrato\n",
        "if 'model' not in globals():\n",
        "     model = YOLO(\"/content/weights/best.pt\") #\"/content/runs/detect/OLD_TRAIN/weights/best.pt\") #older\n",
        "     print(\"Modello caricato.\")\n",
        "else:\n",
        "     print(\"Modello gi√† caricato.\")\n",
        "\n",
        "\n",
        "RUN = \"full_native_loop\" # Nome del run per salvare i risultati\n",
        "rows = [] # Lista per raccogliere i dati per il summary CSV\n",
        "\n",
        "# Itera su ogni file JPG nella cartella delle scansioni (ordinato alfabeticamente)\n",
        "print(f\"Avvio inferenza loop su immagini in /content/scans/...\")\n",
        "for path in sorted(glob.glob(\"/content/scans/Radio/Carbon*.jpg\")):#\"/content/scans/*.jpg\")): #before-older:\n",
        "    print(f\"  Processando: {os.path.basename(path)}\")\n",
        "    img = cv2.imread(path)           # Legge l'immagine solo per ottenere H,W\n",
        "    if img is None:\n",
        "        print(f\"  ‚ö†Ô∏è  Errore: impossibile caricare immagine {path}. Salto.\")\n",
        "        continue\n",
        "    H,W  = img.shape[:2] # Ottiene altezza e larghezza native\n",
        "\n",
        "    # Esegue la predizione sull'immagine singola passando il PERCORSO\n",
        "    model.predict(\n",
        "        source=path,                 # ‚Üê passiamo il **percorso** dell'immagine singola\n",
        "        imgsz=(H,W),                 # ‚Üê risoluzione nativa ‚áí nessun resize aggressivo di YOLO\n",
        "        conf=0.79, iou=0.80, agnostic_nms=True, # Parametri di confidenza e NMS\n",
        "        save_txt=True, save_conf=True, save=True, # Salva file .txt, confidenza e immagini con box\n",
        "        project=\"runs/detect\", name=RUN, # Definisce dove salvare i risultati\n",
        "        device=device, verbose=False # Specifica il device e sopprime output dettagliato\n",
        "    )\n",
        "\n",
        "    # Adesso Ultralytics ha scritto direttamente il file .txt di annotazione\n",
        "    # nella directory corretta (es. runs/detect/full_native_loop/labels/NomeImmagine.txt)\n",
        "    # Controlla se il file .txt √® stato creato e registra il risultato\n",
        "    txt_path_check = f\"runs/detect/{RUN}/labels/{os.path.basename(path)[:-4]}.txt\"\n",
        "    txt_exists = os.path.exists(txt_path_check)\n",
        "    # Conta il numero di box nel file TXT se esiste, altrimenti 0\n",
        "    n_boxes = 0\n",
        "    if txt_exists:\n",
        "        try:\n",
        "            # np.loadtxt con ndmin=2 gestisce anche file vuoti correttamente\n",
        "            arr = np.loadtxt(txt_path_check, ndmin=2)\n",
        "            # shape[0] d√† il numero di righe (box) se arr non √® vuoto, altrimenti 0\n",
        "            n_boxes = arr.shape[0] if arr.size > 0 else 0\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Errore leggendo {txt_path_check}: {e}. Conteggio box impostato a 0.\")\n",
        "            n_boxes = 0\n",
        "\n",
        "    rows.append([os.path.basename(path), n_boxes]) # Aggiunge nome file e conteggio box al summary\n",
        "\n",
        "# Crea un DataFrame pandas dal summary e lo salva come CSV(added _radio in csv)\n",
        "pd.DataFrame(rows, columns=[\"image\",\"n_boxes\"])\\\n",
        "  .to_csv(\"/content/inference_summary_radio.csv\", index=False)\n",
        "print(\"\\n‚úîÔ∏è  Run terminata. Conteggio box per immagine salvato in /content/inference_summary_radio.csv\")"
      ],
      "metadata": {
        "id": "XsqBVylpQs_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copia i file di log e summary CSV da Colab a Google Drive.\n",
        "# Questa cella serve a salvare i risultati dell'inferenza (in questo caso, il summary CSV)\n",
        "# dalla sessione Colab temporanea al Google Drive, per evitare di perderli.\n",
        "# Il comando `!cp -r` copia ricorsivamente la directory di output dei run\n",
        "# (commentato, potenzialmente molto grande) e il file CSV di summary.\n",
        "#!cp -r runs/detect/full_native_loop*  \\\n",
        "#      /content/drive/MyDrive/yolo_logs/\n",
        "# Copia il file inference_summary.csv generato dalla cella precedente\n",
        "!cp /content/inference_summary.csv  \\\n",
        "      /content/drive/MyDrive/yolo_logs/csv/\n",
        "##salva i file su gdive per il futuro;\n",
        "print(\"File inference_summary.csv copiato su Google Drive.\")"
      ],
      "metadata": {
        "id": "JO7PQnTBRiJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copia i file CSV contenenti le annotazioni YOLO (predette) da Google Drive a Colab.\n",
        "# Questa cella esegue l'operazione opposta rispetto alla cella precedente:\n",
        "# copia i file CSV che hai precedentemente salvato su Google Drive (nella cartella csvs_per_scan)\n",
        "# nella directory /content/csvs_per_scan/ della sessione Colab corrente.\n",
        "# Questo √® necessario per poter accedere e utilizzare questi file CSV nelle celle successive,\n",
        "# ad esempio per l'elaborazione, il filtraggio o la visualizzazione delle bounding box predette.\n",
        "# Questo era uno dei passaggi che ti servivano per ricreare i box sulle immagini per il confronto visivo.\n",
        "# Assicurati che il percorso Drive sia corretto e che la cartella esista.\n",
        "!mkdir -p /content/csvs_per_scan/ # Crea la cartella di destinazione se non esiste\n",
        "!cp /content/drive/MyDrive/csvs_per_scan/*.csv /content/csvs_per_scan/ # Copia tutti i file .csv\n",
        "\n",
        "print(\"File CSV copiati da Google Drive a /content/csvs_per_scan/\")"
      ],
      "metadata": {
        "id": "q6oX8FB4KZaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crea un file zip dei risultati di un run di inferenza per il download.\n",
        "# Questa cella impacchetta i contenuti di una specifica directory di run di inferenza YOLO\n",
        "# (identificata dal nome del run, es. `full_native_loop`) in un file zip.\n",
        "# Questo file zip pu√≤ poi essere scaricato sul tuo computer locale o spostato facilmente.\n",
        "# √à un modo pratico per salvare tutti i risultati di un run (immagini con box, file txt, ecc.)\n",
        "# in un unico file compresso.\n",
        "#@title 3.2 creazione zip per passare i file di yololog\n",
        "# Crea un file zip chiamato full_native_loops.zip contenente la directory runs/detect/full_native_loop*\n",
        "# L'asterisco '*' qui potrebbe causare problemi se ci sono pi√π cartelle che iniziano con full_native_loop.\n",
        "# Sarebbe meglio specificare il nome esatto del run se √® fisso (es. full_native_loop).\n",
        "!zip -r /content/full_native_loops.zip  runs/detect/full_native_loop*\n",
        "#scarica i loop inference sul pc in zip\n",
        "#aletrnativa 2 al tutto\n",
        "'''import shutil, os\n",
        "src_dir = \"runs/detect/full_native_loop3*\"\n",
        "shutil.make_archive(base_name=\"/content/full_native_loops\",\n",
        "                    format=\"zip\",\n",
        "                    root_dir=os.path.dirname(src_dir),\n",
        "                    base_dir=os.path.basename(src_dir))\n",
        "'''\n",
        "print(\"File zip dei risultati di inferenza creato in /content/full_native_loops.zip\")"
      ],
      "metadata": {
        "id": "RlUPTIahSwOc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK adesso testare veloce nuovo csv se funziona si applica su altre scan e si ordina con kmeans e si taglia gli stronzoni!!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Andhz_wRH8b6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/drive/MyDrive/progetto_fori_yolo/freeze_cpu_YOLOSCRIPTfori_2025-06-26.txt --quiet"
      ],
      "metadata": {
        "id": "vmNW1PrfJL9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title INFERENZA IMMAGINI sia DNT/Optic; settaggio completo; usare per inferenze future sulle SCAN\n",
        "#\n",
        "#pass all scans!!\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "IMG_DIR  = \"/content/scans/Radio\"#vecchie per scan ottiche#\"/content/scan_ruotate_da_inferire_ingressi\"#\"/content/drive/MyDrive/Scan_Orientate\"#\"/content/scans\"\n",
        "CSV_DIR  = \"NUOVE_CSV_RADIO\"#\"/content/scansCSV_oriented\"\n",
        "os.makedirs(CSV_DIR, exist_ok=True)\n",
        "MODEL = YOLO(\"/content/drive/MyDrive/Pesi/best.pt\") #pezzo precedente, adesso richiamo dal cloud diretto #/content/PESI/best.pt\")#\"/content/weights/best.pt\")   # <- path you used\n",
        "\n",
        "for img_path in sorted(glob.glob(f\"{IMG_DIR}/Carbon*.jpg\")):\n",
        "    H,W = cv2.imread(img_path).shape[:2]\n",
        "    res = MODEL.predict(img_path, imgsz=(H,W), conf=0.79,\n",
        "                        iou=0.80,\n",
        "                        )[0]#verbose=False)[0]\n",
        "    if not len(res.boxes):\n",
        "        print(\"‚ö†Ô∏è  no boxes in\", img_path);  continue\n",
        "\n",
        "    df = pd.DataFrame(res.boxes.xyxy.cpu().numpy().astype(int),\n",
        "                      columns=[\"x1\",\"y1\",\"x2\",\"y2\"])\n",
        "    df.insert(0, \"conf\", res.boxes.conf.cpu().numpy())\n",
        "    stem = Path(img_path).stem\n",
        "    df.to_csv(f\"{CSV_DIR}/{stem}.csv\", index=False)\n",
        "    print(\"‚Ä¢\", stem, \"‚Üí CSV ok\")\n",
        "print(\"üéâ  all scans processed ‚Üí\", CSV_DIR)\n"
      ],
      "metadata": {
        "id": "_WB-faePS2Om",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#parte ingresso/iniziale"
      ],
      "metadata": {
        "id": "_3g_gyi-JjaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title serve a creare un pandas dataframe in coordinate assolute e non relative di yolo\n",
        "# res.boxes.xyxy is already **absolute pixel coords**  (x1,y1,x2,y2)\n",
        "boxes_xyxy = res.boxes.xyxy.cpu().numpy().astype(int)        # (N,4)\n",
        "scores      = res.boxes.conf.cpu().numpy()                   # (N,)\n",
        "print(f\"model returned {len(boxes_xyxy)} boxes   conf‚àà[{scores.min():.2f},{scores.max():.2f}]\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Kvu57nPbjnTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title questa cella segue ordine bustrofedico a colonna(ingresso); boustrophedon path ‚Üë‚Üí‚Üì‚Üí (uscite sono inverse)\n",
        "##possible fix order cluster kmenas\n",
        "# Cell 5: Batch‚Äêorder each scan‚Äôs CSV via serpentine K-Means\n",
        "##\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def order_holes(df, n_cols=None):\n",
        "    \"\"\"Adds 'hole_idx' to df by left‚Üíright snake ordering on x1,x2,y1,y2 (absolute pixels).\"\"\"\n",
        "    df = df.reset_index(drop=True)\n",
        "    # 1) compute each bbox‚Äôs center‚Äêx\n",
        "    xc = ((df.x1 + df.x2) / 2).to_numpy().reshape(-1, 1)\n",
        "    # 2) auto‚Äêestimate how many vertical columns if not given\n",
        "    if n_cols is None:\n",
        "        step = (xc.max() - xc.min()) / 20\n",
        "        n_cols = len(np.unique((xc // step).astype(int)))\n",
        "    # 3) cluster center-x's into columns\n",
        "    km = KMeans(n_clusters=n_cols, n_init=10, random_state=42).fit(xc)\n",
        "    cols = np.argsort(km.cluster_centers_.ravel())  # left‚Üíright ordering of clusters\n",
        "    # 4) assign snake indices\n",
        "    hole_idx = {}\n",
        "    counter = 1\n",
        "    for j, col in enumerate(cols):\n",
        "        # get all indices in this column\n",
        "        idxs = np.where(km.labels_ == col)[0]\n",
        "        # sort within‚Äêcolumn: bottom‚Üítop on even columns, top‚Üíbottom on odd\n",
        "        idxs_sorted = sorted(\n",
        "            idxs,\n",
        "            key=lambda i: -((df.y1.iat[i] + df.y2.iat[i]) / 2)\n",
        "                          if (j % 2 == 0)\n",
        "                          else  ((df.y1.iat[i] + df.y2.iat[i]) / 2)\n",
        "        )\n",
        "        for i in idxs_sorted:\n",
        "            hole_idx[i] = counter\n",
        "            counter += 1\n",
        "    df['hole_idx'] = df.index.map(hole_idx)\n",
        "    return df\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 2) Loop over all your raw scan CSVs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "IN_DIR  = \"/content/NUOVE_CSV_RADIO\" ###RADIO ## usato per test e ottici\"/content/CSV_5\"#\"/content/SCAN_5\"#NUOVE_CSV\"#\"/content/scansCSV_oriented\"         # ‚Üê your YOLO‚Äêexported CSVs\n",
        "OUT_DIR = \"/content/NUOVE_CSV_RADIO/Ordered_radio\" ###RADIO ## usato per test e ottici\"/content/CSV_5/ORDINATE_CSV\"#\"/content/ordered_csv_NEW\"          # ‚Üê will receive *_ordered.csv\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "for csv_path in sorted(glob.glob(f\"{IN_DIR}/*.csv\")):\n",
        "    df = pd.read_csv(csv_path)            # expects columns: conf, x1, y1, x2, y2\n",
        "    if df.empty:\n",
        "        print(f\"‚ö†Ô∏è Skipping empty file {os.path.basename(csv_path)}\")\n",
        "        continue\n",
        "\n",
        "    # snake‚Äêorder it (auto‚Äêestimate columns)\n",
        "    df_ord = order_holes(df, n_cols=None)\n",
        "\n",
        "    base = os.path.splitext(os.path.basename(csv_path))[0]\n",
        "    out  = os.path.join(OUT_DIR, f\"{base}_ordered_NEW.csv\")\n",
        "    df_ord.to_csv(out, index=False)\n",
        "    print(f\"‚úÖ {base}: wrote {len(df_ord)} holes ‚Üí {out}\")\n",
        "\n",
        "print(\"üéâ All done! Check ordered_csv/*.csv for your serpentine‚Äênumbered files.\")\n"
      ],
      "metadata": {
        "id": "8-GEtj7po2ZF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê  VISUAL CHECK BATCH (ingresso fixed)  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "import cv2, glob, os, pandas as pd\n",
        "from pathlib import Path\n",
        "from google.colab.patches import cv2_imshow   # solo in Colab\n",
        "\n",
        "CSV_DIR  = \"/content/NUOVE_CSV_RADIO/Ordered_radio\" #adattamenteo da uscite#\"/content/ordered_csv_fixed2\"     # tutti i *_fixed.csv\n",
        "IMG_DIR  = \"/content/scans/Radio\" #adattamenteo da uscite#\"/content/drive/MyDrive/Scan_Orientate\"  # scans .jpg\n",
        "OUT_DIR  = \"/content/Radio_DNT\"#adattamenteo da uscite#\"/content/overlay_batch\"         # salva gli overlay\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "for csv_path in sorted(Path(CSV_DIR).glob(\"*.csv\")):\n",
        "    stem = csv_path.stem.replace(\"_fixed\", \"\").replace(\"_ordered_NEW\",\"\")     # es. T0_90_3B_uscita\n",
        "    img  = cv2.imread(f\"{IMG_DIR}/{stem}.jpg\")\n",
        "    if img is None:\n",
        "        print(\"‚ö†Ô∏è  immagine mancante:\", stem);  continue\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    for _, r in df.iterrows():\n",
        "        x1,y1,x2,y2 = map(int, (r.x1,r.y1,r.x2,r.y2))\n",
        "        idx = int(r.hole_idx)\n",
        "        cv2.rectangle(img, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "        cv2.putText(img, str(idx), (x1, y1-6),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                    (255,255,255), 1, cv2.LINE_AA)\n",
        "\n",
        "    out = f\"{OUT_DIR}/{stem}_vis.jpg\"\n",
        "    cv2.imwrite(out, img)\n",
        "    #cv2_imshow(img)     # mostra subito in Colab (facoltativo)\n",
        "    print(\"‚úî overlay:\", out)\n",
        "\n",
        "print(\"üéâ  Overlay creati per tutte le uscite fixed\")\n",
        "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n"
      ],
      "metadata": {
        "id": "wLCZS5PbJodW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one click download overlays Radio\n",
        "#@title serve a  creare un zip completo CSV - overlay controllo - evita di avviare in tempo reale gli overlay\n",
        "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê  Prepara ZIP di CSV & overlay ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "import os, glob, zipfile\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "CSV_DIR   = \"/content/NUOVE_CSV_RADIO/Ordered_radio\"##\"/content/ordered_csv_fixed2\"     # i tuoi *_fixed.csv\n",
        "VIS_DIR   = \"/content/Radio_DNT\"##\"/content/overlay_batch\"         # gli overlay che hai appena generato\n",
        "ZIP_PATH  = \"/content/Zip_radio.zip\"##\"/content/uscite_checksA.zip\"     # dove scrivere lo ZIP\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Rimuovi eventuale vecchio ZIP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "if os.path.exists(ZIP_PATH):\n",
        "    os.remove(ZIP_PATH)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Crea lo ZIP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "with zipfile.ZipFile(ZIP_PATH, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "    # aggiungi CSV\n",
        "    for f in glob.glob(f\"{CSV_DIR}/*.csv\"):\n",
        "        zf.write(f, arcname=os.path.basename(f))\n",
        "    # aggiungi overlay\n",
        "    for f in glob.glob(f\"{VIS_DIR}/*_vis.jpg\"):\n",
        "        zf.write(f, arcname=os.path.join(\"overlays\", os.path.basename(f)))\n",
        "\n",
        "print(f\"üéâ ZIP creato: {ZIP_PATH}\")\n",
        "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n"
      ],
      "metadata": {
        "id": "jItaC8KyNH9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, os, glob, zipfile, pandas as pd\n",
        "from pathlib import Path\n",
        "import re                                           # new\n",
        "\n",
        "ORDERED_DIR = \"/content/NUOVE_CSV_RADIO/Ordered_radio\"\n",
        "SCAN_DIR    = \"/content/scans/Radio\"\n",
        "PATCH_DIR   = \"/content/patches_Radio_carbon\"\n",
        "ZIP_PATH    = \"/content/patches_Radio_carbon.zip\"\n",
        "HS = 126                                         # half-side ‚Üí 700√ó700\n",
        "\n",
        "os.makedirs(PATCH_DIR, exist_ok=True)\n",
        "if os.path.exists(ZIP_PATH):\n",
        "    os.remove(ZIP_PATH)\n",
        "\n",
        "global_idx = 1\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "    # grab *all* CSVs and let the code figure out the stem\n",
        "    for csv_path in sorted(Path(ORDERED_DIR).glob(\"*.csv\")):\n",
        "        raw_stem = Path(csv_path).stem                 # e.g. Carbon Textile 1A_ordered_NEW\n",
        "        # strip known trailers (_fixed, _ordered_NEW, _uscita, etc.)\n",
        "        stem     = re.sub(r\"(_fixed|_ordered_NEW|_uscita.*)$\", \"\", raw_stem).rstrip(\"_\")\n",
        "        img_path = f\"{SCAN_DIR}/{stem}.jpg\"\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            print(f\"‚ö†Ô∏è  Immagine mancante: {img_path} ‚Äî salto\")\n",
        "            continue\n",
        "        H_img, W_img = img.shape[:2]\n",
        "\n",
        "        df = (pd.read_csv(csv_path)\n",
        "                .sort_values(\"hole_idx\")               # ensure order\n",
        "                .reset_index(drop=True))\n",
        "\n",
        "        scan_outdir = f\"{PATCH_DIR}/{stem}\"\n",
        "        os.makedirs(scan_outdir, exist_ok=True)\n",
        "\n",
        "        for _, r in df.iterrows():\n",
        "            cx, cy = int((r.x1 + r.x2)//2), int((r.y1 + r.y2)//2)\n",
        "            x1, y1 = max(0, cx-HS), max(0, cy-HS)\n",
        "            x2, y2 = min(W_img, cx+HS), min(H_img, cy+HS)\n",
        "            patch  = img[y1:y2, x1:x2]\n",
        "\n",
        "            # pad out to exactly 700√ó700 if near an edge\n",
        "            h, w = patch.shape[:2]\n",
        "            patch = cv2.copyMakeBorder(patch, 0, 256-h, 0, 256-w,\n",
        "                                       cv2.BORDER_CONSTANT, 0)\n",
        "\n",
        "            fname = f\"H{global_idx:03d}_h{int(r.hole_idx):03d}_{stem}.jpg\"\n",
        "            fpath = f\"{scan_outdir}/{fname}\"\n",
        "            cv2.imwrite(fpath, patch)\n",
        "            zf.write(fpath, arcname=f\"{stem}/{fname}\")\n",
        "\n",
        "            global_idx += 1\n",
        "\n",
        "        print(f\"‚úî {stem}: {len(df)} patch(es))\")\n",
        "\n",
        "print(f\"üéâ  Ho creato {global_idx-1} patch e le ho zippate in {ZIP_PATH}\")\n"
      ],
      "metadata": {
        "id": "hXETearhRPfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['x1','y1','x2','y2']].describe()\n"
      ],
      "metadata": {
        "id": "eho2kp6xY2Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#parte uscite"
      ],
      "metadata": {
        "id": "H8POSXxNJeLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#USCITE ORDINE crop\n",
        "gruppo A ‚Üì‚Üë‚Üì‚Üë\n",
        "Le altre\n",
        "1A, 1B, 2A, 2B, 3B\n",
        "\n",
        "gruppo B da destra ‚Üë‚Üì‚Üë\n",
        "6 uscita\n",
        ",3A uscita"
      ],
      "metadata": {
        "id": "CuiPXyoT7teW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title gruppo B da destra ‚Üë‚Üì‚Üë\n",
        "import os, glob, cv2, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Config\n",
        "IN_DIR   = \"/content/\"\n",
        "OUT_DIR  = \"/content/NEWordered_csv_fixed\"\n",
        "GROUP_B  = {\n",
        "    \"T0_90_3A_uscita\",\n",
        "    \"T0_90_6_uscita\",\n",
        "}\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Ordering function\n",
        "def order_holes(df, n_cols=None):\n",
        "    df = df.reset_index(drop=True)\n",
        "    xc = ((df.x1 + df.x2)/2).to_numpy().reshape(-1,1)\n",
        "    if n_cols is None:\n",
        "        step = (xc.max() - xc.min())/20\n",
        "        n_cols = len(np.unique((xc//step).astype(int)))\n",
        "    km   = KMeans(n_clusters=n_cols, n_init=10, random_state=42).fit(xc)\n",
        "    cols = np.argsort(km.cluster_centers_.ravel())\n",
        "    hole_idx, counter = {}, 1\n",
        "    for j, c in enumerate(cols):\n",
        "        idxs = np.where(km.labels_==c)[0]\n",
        "        idxs_sorted = sorted(\n",
        "            idxs,\n",
        "            key=lambda i: -((df.y1.iat[i]+df.y2.iat[i])/2)\n",
        "                          if j%2==0\n",
        "                          else  ((df.y1.iat[i]+df.y2.iat[i])/2)\n",
        "        )\n",
        "        for i in idxs_sorted:\n",
        "            hole_idx[i] = counter\n",
        "            counter += 1\n",
        "    df['hole_idx'] = df.index.map(hole_idx)\n",
        "    return df\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Batch: trasforma e ordina\n",
        "for csv_path in sorted(glob.glob(f\"{IN_DIR}/*.csv\")):\n",
        "    base = Path(csv_path).stem.replace(\"_ordered\",\"\")\n",
        "    # salta quelli non uscita\n",
        "    if not base.endswith(\"_uscita\"):\n",
        "        continue\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # applichi qui eventuale flip_x/rot180 se serve‚Ä¶\n",
        "    # df_tf = transform_boxes(df, action, W, H)\n",
        "    df_tf = df  # se gi√† OK\n",
        "\n",
        "    df_fixed = order_holes(df_tf)\n",
        "\n",
        "    # **Solo per Gruppo A**, inverti il count\n",
        "    if base in GROUP_A:\n",
        "        total = len(df_fixed)\n",
        "        df_fixed['hole_idx'] = (total + 1) - df_fixed['hole_idx']\n",
        "\n",
        "    out_csv = os.path.join(OUT_DIR, f\"{base}_fixed.csv\")\n",
        "    df_fixed.to_csv(out_csv, index=False)\n",
        "    print(f\"‚úÖ {base}: fixed saved ‚Üí {out_csv}\")\n",
        "\n",
        "print(\"üéâ Tutti i CSV di uscita sono stati ri‚Äêindicizzati correttamente.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kDMF64X3Bhft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell: correzione batch uscite con serpentine top-down per Gruppo A\n",
        "import os, glob, cv2, pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Parametri ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "IN_DIR    = \"/content\"              # CSV ordered ‚Äúsbagliati‚Äù\n",
        "OUT_DIR   = \"/content/ordered_csv_fixed2\"       # CSV fixed\n",
        "VIS_DIR   = \"/content/OVERLAY_USCITE_FIXED2\"    # overlay visivi\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(VIS_DIR, exist_ok=True)\n",
        "\n",
        "# i soli stem di scan uscite che vogliamo partire dall'alto\n",
        "GROUP_A = {\n",
        "    \"T0_90_1A_uscita\",\n",
        "    \"T0_90_1B_uscita\",\n",
        "    \"T0_90_2A_uscita\",\n",
        "    \"T0_90_2B_uscita\",\n",
        "    \"T0_90_3B_uscita\",\n",
        "}\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Funzione di ordinamento serpentine ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def order_holes(df, n_cols=None, start_from_top: bool = False):\n",
        "    df = df.reset_index(drop=True)\n",
        "    xc = ((df.x1 + df.x2)/2).to_numpy().reshape(-1,1)\n",
        "    if n_cols is None:\n",
        "        step = (xc.max()-xc.min())/20\n",
        "        n_cols = len(np.unique((xc//step).astype(int)))\n",
        "    km = KMeans(n_clusters=n_cols, n_init=10, random_state=42).fit(xc)\n",
        "    cols = np.argsort(km.cluster_centers_.ravel())\n",
        "    hole_idx = {}\n",
        "    counter = 1\n",
        "    for j, c in enumerate(cols):\n",
        "        idxs = np.where(km.labels_==c)[0]\n",
        "        if start_from_top:\n",
        "            # colonna pari: top‚Üíbottom ; dispari: bottom‚Üítop\n",
        "            idxs_sorted = sorted(\n",
        "                idxs,\n",
        "                key=lambda i: ((df.y1.iat[i]+df.y2.iat[i])/2)\n",
        "                              if (j%2==0)\n",
        "                              else -((df.y1.iat[i]+df.y2.iat[i])/2)\n",
        "            )\n",
        "        else:\n",
        "            # serpentine classico: pari: bottom‚Üítop ; dispari: top‚Üíbottom\n",
        "            idxs_sorted = sorted(\n",
        "                idxs,\n",
        "                key=lambda i: -((df.y1.iat[i]+df.y2.iat[i])/2)\n",
        "                              if (j%2==0)\n",
        "                              else  ((df.y1.iat[i]+df.y2.iat[i])/2)\n",
        "            )\n",
        "        for i in idxs_sorted:\n",
        "            hole_idx[i] = counter\n",
        "            counter += 1\n",
        "    df['hole_idx'] = df.index.map(hole_idx).astype(int)\n",
        "    return df\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Loop batch ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "for csv_path in sorted(glob.glob(f\"{IN_DIR}/*_uscita*.csv\")):\n",
        "    base = Path(csv_path).stem.replace(\"_ordered\",\"\")\n",
        "    df   = pd.read_csv(csv_path)\n",
        "\n",
        "    # decidi top‚Äêdown o bottom‚Äêup\n",
        "    start_top = base in GROUP_A\n",
        "\n",
        "    # riordina\n",
        "    df_fixed = order_holes(df, n_cols=None, start_from_top=start_top)\n",
        "\n",
        "    # salva CSV fixed\n",
        "    out_csv = os.path.join(OUT_DIR, f\"{base}_fixed.csv\")\n",
        "    df_fixed.to_csv(out_csv, index=False)\n",
        "\n",
        "    # overlay per sanity‚Äêcheck\n",
        "    img_path = f\"/content/drive/MyDrive/Scan_Orientate/{base}.jpg\"\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is not None:\n",
        "        vis = img.copy()\n",
        "        for _, r in df_fixed.iterrows():\n",
        "            x1,y1,x2,y2 = map(int,(r.x1,r.y1,r.x2,r.y2))\n",
        "            cv2.rectangle(vis,(x1,y1),(x2,y2),(0,255,0),2)\n",
        "            cv2.putText(vis, str(r.hole_idx),(x1,y1-6),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n",
        "        cv2.imwrite(f\"{VIS_DIR}/{base}_vis.jpg\", vis)\n",
        "\n",
        "    print(f\"‚úÖ {base}: fixed.csv ‚Üí {out_csv}  \"\n",
        "          f\"({'top‚Üídown' if start_top else 'bottom‚Üíup'})\")\n",
        "\n",
        "print(\"üéâ Tutto fatto! Controlla:\", OUT_DIR, \"e\", VIS_DIR)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jNrVvPLEEXVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê  VISUAL CHECK BATCH -ottico-(uscite fixed)  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "import cv2, glob, os, pandas as pd\n",
        "from pathlib import Path\n",
        "from google.colab.patches import cv2_imshow   # solo in Colab\n",
        "\n",
        "CSV_DIR  = \"/content/ordered_csv_fixed2\"     # tutti i *_fixed.csv\n",
        "IMG_DIR  = \"/content/drive/MyDrive/Scan_Orientate\"  # scans .jpg\n",
        "OUT_DIR  = \"/content/overlay_batch\"         # salva gli overlay\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "for csv_path in sorted(Path(CSV_DIR).glob(\"*_fixed.csv\")):\n",
        "    stem = csv_path.stem.replace(\"_fixed\", \"\")     # es. T0_90_3B_uscita\n",
        "    img  = cv2.imread(f\"{IMG_DIR}/{stem}.jpg\")\n",
        "    if img is None:\n",
        "        print(\"‚ö†Ô∏è  immagine mancante:\", stem);  continue\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    for _, r in df.iterrows():\n",
        "        x1,y1,x2,y2 = map(int, (r.x1,r.y1,r.x2,r.y2))\n",
        "        idx = int(r.hole_idx)\n",
        "        cv2.rectangle(img, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "        cv2.putText(img, str(idx), (x1, y1-6),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                    (255,255,255), 1, cv2.LINE_AA)\n",
        "\n",
        "    out = f\"{OUT_DIR}/{stem}_vis.jpg\"\n",
        "    cv2.imwrite(out, img)\n",
        "    #cv2_imshow(img)     # mostra subito in Colab (facoltativo)\n",
        "    print(\"‚úî overlay:\", out)\n",
        "\n",
        "print(\"üéâ  Overlay creati per tutte le uscite fixed\")\n",
        "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n"
      ],
      "metadata": {
        "id": "_Z43-qC_4NRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title serve a  creare un zip completo CSV - overlay controllo(ottico-uscita) - evita di avviare in tempo reale gli overlay\n",
        "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê  Prepara ZIP di CSV & overlay ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "import os, glob, zipfile\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "CSV_DIR   = \"/content/ordered_csv_fixed2\"     # i tuoi *_fixed.csv\n",
        "VIS_DIR   = \"/content/overlay_batch\"         # gli overlay che hai appena generato\n",
        "ZIP_PATH  = \"/content/uscite_checksA.zip\"     # dove scrivere lo ZIP\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Rimuovi eventuale vecchio ZIP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "if os.path.exists(ZIP_PATH):\n",
        "    os.remove(ZIP_PATH)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Crea lo ZIP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "with zipfile.ZipFile(ZIP_PATH, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "    # aggiungi CSV\n",
        "    for f in glob.glob(f\"{CSV_DIR}/*.csv\"):\n",
        "        zf.write(f, arcname=os.path.basename(f))\n",
        "    # aggiungi overlay\n",
        "    for f in glob.glob(f\"{VIS_DIR}/*_vis.jpg\"):\n",
        "        zf.write(f, arcname=os.path.join(\"overlays\", os.path.basename(f)))\n",
        "\n",
        "print(f\"üéâ ZIP creato: {ZIP_PATH}\")\n",
        "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n"
      ],
      "metadata": {
        "id": "x3DMonhe_K_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, os, glob, zipfile, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Parametri ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "ORDERED_DIR = \"/content/ordered_csv_fixed2\"        # tutti *_uscita_fixed.csv che hai gi√† sistemato\n",
        "SCAN_DIR    = \"/content/drive/MyDrive/Scan_Orientate\"\n",
        "PATCH_DIR   = \"/content/patches_uscite_all\"         # output dei crop\n",
        "ZIP_PATH    = \"/content/patches_uscite_all.zip\"\n",
        "HS          = 350                                   # mezzo lato del crop (700√ó700)\n",
        "\n",
        "# preparo cartelle\n",
        "os.makedirs(PATCH_DIR, exist_ok=True)\n",
        "if os.path.exists(ZIP_PATH):\n",
        "    os.remove(ZIP_PATH)\n",
        "\n",
        "global_idx = 1  # unico contatore globale\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "    # prendo TUTTI i CSV di uscita (sia quelli ‚Äú_uscita_fixed.csv‚Äù sia eventuali varianti)\n",
        "    for csv_path in sorted(glob.glob(f\"{ORDERED_DIR}/*_uscita*_fixed.csv\")):\n",
        "        stem      = Path(csv_path).stem.replace(\"_fixed\",\"\")  # es. \"T0_90_1A_uscita\"\n",
        "        img_path  = f\"{SCAN_DIR}/{stem}.jpg\"\n",
        "        df        = pd.read_csv(csv_path)\n",
        "        img       = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            print(f\"‚ö†Ô∏è Immagine mancante: {img_path}, salto\"); continue\n",
        "        H_img, W_img = img.shape[:2]\n",
        "\n",
        "        # assicuro che h (= hole_idx) parta da 1 e segua l‚Äôordine corretto\n",
        "        df = df.sort_values(\"hole_idx\")\n",
        "\n",
        "        # cartella di output per questa scan\n",
        "        scan_outdir = f\"{PATCH_DIR}/{stem}\"\n",
        "        os.makedirs(scan_outdir, exist_ok=True)\n",
        "        # ciclo sui fori\n",
        "        for _, r in df.iterrows():\n",
        "            cx, cy = int((r.x1 + r.x2)//2), int((r.y1 + r.y2)//2)\n",
        "            x1, y1 = max(0, cx-HS), max(0, cy-HS)\n",
        "            x2, y2 = min(W_img, cx+HS), min(H_img, cy+HS)\n",
        "            patch  = img[y1:y2, x1:x2]\n",
        "            h, w   = patch.shape[:2]\n",
        "            # padding fino a esatto 700√ó700\n",
        "            patch  = cv2.copyMakeBorder(patch, 0, 700-h, 0, 700-w,\n",
        "                                        cv2.BORDER_CONSTANT, 0)\n",
        "\n",
        "            # nome: H<globale>_h<locale>_<scan>.jpg\n",
        "            fname = f\"H{global_idx:03d}_h{int(r.hole_idx):03d}_{stem}.jpg\"\n",
        "            fpath = f\"{scan_outdir}/{fname}\"\n",
        "            cv2.imwrite(fpath, patch)\n",
        "            # aggiungo al ZIP mantenendo la sottocartella per scan\n",
        "            zf.write(fpath, arcname=f\"{stem}/{fname}\")\n",
        "\n",
        "            global_idx += 1\n",
        "\n",
        "print(f\"üéâ Ho creato {global_idx-1} patch e le ho zippate in {ZIP_PATH}\")\n"
      ],
      "metadata": {
        "id": "TXV54lYh_MJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "89mU4w3OOnsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "amvXRvFVOnoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PARTE CHE FORSE POTREBBE ESSERE TOLTA ADESSO; teniamo per sicurezza futura, erano celle di test veloci"
      ],
      "metadata": {
        "id": "a8seQmTXLqUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title --- 0.  install / import once ----------------------------------------------\n",
        "#!pip install -qqU ultralytics                   # Colab: fetch latest build\n",
        "from ultralytics import YOLO\n",
        "import cv2, pandas as pd\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# --- 1.  load your weight ----------------------------------------------------\n",
        "MODEL = YOLO(\"/content/weights/best.pt\")   # <- path you used\n",
        "\n",
        "# --- 2.  choose ONE image ----------------------------------------------------\n",
        "IMG  = \"/content/scans/T0_90_1A_uscita.jpg\"                      # test file\n",
        "img = cv2.imread(IMG)\n",
        "H,W  = img.shape[:2] #\n",
        "# --- 3.  predict (no save_txt, no tricks: we only need the returned object) --\n",
        "res = MODEL.predict(IMG, imgsz=(H,W), conf=0.79, iou=0.80, verbose=False)[0]\n",
        "\n",
        "# res.boxes.xyxy is already **absolute pixel coords**  (x1,y1,x2,y2)\n",
        "boxes_xyxy = res.boxes.xyxy.cpu().numpy().astype(int)        # (N,4)\n",
        "scores      = res.boxes.conf.cpu().numpy()                   # (N,)\n",
        "print(f\"model returned {len(boxes_xyxy)} boxes   conf‚àà[{scores.min():.2f},{scores.max():.2f}]\")\n"
      ],
      "metadata": {
        "id": "ZQjqAdGxF0JQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title visual sanity that works on ordered amd shows orders of holes and indexes on one scan\n",
        "##VISUAL SANITY CHECK WORKS on one scan 4 here: works\n",
        "import cv2, pandas as pd\n",
        "from pathlib import Path\n",
        "from google.colab.patches import cv2_imshow   # solo in Colab\n",
        "\n",
        "# 1Ô∏è‚É£  scegli il CSV ordinato da visualizzare\n",
        "CSV_PATH = \"/content/ordered_csv_fixed/T0_90_3B_uscita_fixed.csv\"#\"/content/CSV_5/ORDINATE_CSV/T0_90_5_entrata_NEW_ordered_NEW.csv\"##\"/content/ORDINATE_CSV/T0_90_3B_ingresso_ordered_NEW.csv\"#\"/content/ordered_csv_NEW/T0_90_4_ingresso_ordered_NEW.csv\"#\"/content/T0_90_4_uscita_NEW_ordered.csv\" #\"/content/ordered_csv_NEW/T0_90_4_uscita_NEW_ordered.csv\"\n",
        "\n",
        "# 2Ô∏è‚É£  leggi il file ‚Üí DataFrame\n",
        "df = pd.read_csv(CSV_PATH)          # colonne: conf,x1,y1,x2,y2,hole_idx  (o simile)\n",
        "\n",
        "# 3Ô∏è‚É£  recupera il percorso dell‚Äôimmagine dalla stringa ‚Äúscan‚Äù contenuta nel nome file\n",
        "#     (p.es. T0_90_1A_uscita.jpg si trova in /content/scans/)\n",
        "img = cv2.imread(\"/content/drive/MyDrive/Scan_Orientate/T0_90_3B_uscita.jpg\")#\"/content/SCAN_5/T0_90_5_entrata_NEW.jpg\")#\"/content/scan_ruotate_da_inferire_ingressi/T0_90_3B_ingresso.jpg\")#\"/content/drive/MyDrive/Scan_Orientate/T0_90_4_ingresso.jpg\")#\"/content/drive/MyDrive/FileTesi/ScanzioneProviniUsura/T0_90_4_uscita_NEW.jpg\")\n",
        "\n",
        "#img = cv2.imread(IMG_PATH)\n",
        "assert img is not None, f\"Immagine non trovata: {IMG_PATH}\"\n",
        "\n",
        "# 4Ô∏è‚É£  disegna per ogni riga del DataFrame\n",
        "for _, r in df.iterrows():\n",
        "    x1, y1, x2, y2 = int(r.x1), int(r.y1), int(r.x2), int(r.y2)\n",
        "    idx            = int(r.hole_idx)\n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "    cv2.putText(img, str(idx),\n",
        "                (x1, y1 - 8),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                (255, 255, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "cv2_imshow(img)     # mostra in Colab\n"
      ],
      "metadata": {
        "id": "n7SDD_AQzIaI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title visual sanity that works on ordered amd shows orders of holes and indexes on one scan\n",
        "##VISUAL SANITY CHECK WORKS on one scan 4 here: works\n",
        "import cv2, pandas as pd\n",
        "from pathlib import Path\n",
        "from google.colab.patches import cv2_imshow   # solo in Colab\n",
        "\n",
        "# 1Ô∏è‚É£  scegli il CSV ordinato da visualizzare\n",
        "CSV_PATH = \"/content/ordered_csv_fixed/T0_90_3A_uscita_fixed.csv\"#\"/content/CSV_5/ORDINATE_CSV/T0_90_5_entrata_NEW_ordered_NEW.csv\"##\"/content/ORDINATE_CSV/T0_90_3B_ingresso_ordered_NEW.csv\"#\"/content/ordered_csv_NEW/T0_90_4_ingresso_ordered_NEW.csv\"#\"/content/T0_90_4_uscita_NEW_ordered.csv\" #\"/content/ordered_csv_NEW/T0_90_4_uscita_NEW_ordered.csv\"\n",
        "\n",
        "# 2Ô∏è‚É£  leggi il file ‚Üí DataFrame\n",
        "df = pd.read_csv(CSV_PATH)          # colonne: conf,x1,y1,x2,y2,hole_idx  (o simile)\n",
        "\n",
        "# 3Ô∏è‚É£  recupera il percorso dell‚Äôimmagine dalla stringa ‚Äúscan‚Äù contenuta nel nome file\n",
        "#     (p.es. T0_90_1A_uscita.jpg si trova in /content/scans/)\n",
        "img = cv2.imread(\"/content/drive/MyDrive/Scan_Orientate/T0_90_3A_uscita.jpg\")#\"/content/SCAN_5/T0_90_5_entrata_NEW.jpg\")#\"/content/scan_ruotate_da_inferire_ingressi/T0_90_3B_ingresso.jpg\")#\"/content/drive/MyDrive/Scan_Orientate/T0_90_4_ingresso.jpg\")#\"/content/drive/MyDrive/FileTesi/ScanzioneProviniUsura/T0_90_4_uscita_NEW.jpg\")\n",
        "\n",
        "#img = cv2.imread(IMG_PATH)\n",
        "assert img is not None, f\"Immagine non trovata: {IMG_PATH}\"\n",
        "\n",
        "# 4Ô∏è‚É£  disegna per ogni riga del DataFrame\n",
        "for _, r in df.iterrows():\n",
        "    x1, y1, x2, y2 = int(r.x1), int(r.y1), int(r.x2), int(r.y2)\n",
        "    idx            = int(r.hole_idx)\n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "    cv2.putText(img, str(idx),\n",
        "                (x1, y1 - 8),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                (255, 255, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "cv2_imshow(img)     # mostra in Colab"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2bgX3V5S4zZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7CHZJm54bhuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## creazione del check visivo"
      ],
      "metadata": {
        "id": "4D4suTSMbn_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell: evaluate serpentine ordering with quantitative adjacency metrics\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def check_serpentine(df_ordered):\n",
        "    # 1) compute centers\n",
        "    df = df_ordered.copy()\n",
        "    df['cx'] = (df.x1 + df.x2) / 2\n",
        "    df['cy'] = (df.y1 + df.y2) / 2\n",
        "\n",
        "    # 2) re-cluster into columns exactly as before\n",
        "    X = df['cx'].to_numpy().reshape(-1,1)\n",
        "    # auto n_cols\n",
        "    step = (X.max() - X.min()) / 20\n",
        "    n_cols = len(np.unique((X // step).astype(int)))\n",
        "    km = KMeans(n_clusters=n_cols, n_init=10, random_state=42).fit(X)\n",
        "    df['col'] = km.labels_\n",
        "\n",
        "    # 3) sort by hole_idx to walk the snake\n",
        "    df = df.sort_values('hole_idx').reset_index(drop=True)\n",
        "\n",
        "    # 4) for each i ‚Üí i+1, check adjacency rule\n",
        "    correct = []\n",
        "    for i in range(len(df)-1):\n",
        "        c0, c1 = df.loc[i,'col'], df.loc[i+1,'col']\n",
        "        y0, y1 = df.loc[i,'cy'],  df.loc[i+1,'cy']\n",
        "        x0, x1 = df.loc[i,'cx'],  df.loc[i+1,'cx']\n",
        "\n",
        "        if c0 == c1:\n",
        "            # within same column: even‚Äêindexed columns (0‚Äêbased) go bottom‚Üítop (y increases),\n",
        "            # odd‚Äêindexed go top‚Üíbottom (y decreases)\n",
        "            if (c0 % 2 == 0 and (y1 - y0) > 0) or (c0 % 2 == 1 and (y1 - y0) < 0):\n",
        "                correct.append(True)\n",
        "            else:\n",
        "                correct.append(False)\n",
        "        else:\n",
        "            # between columns must always move rightwards (x increases)\n",
        "            correct.append((x1 - x0) > 0)\n",
        "\n",
        "    # 5) return fraction correct + total steps\n",
        "    return np.mean(correct), len(correct)\n",
        "\n",
        "# 6) batch over all ordered CSVs\n",
        "RESULTS = []\n",
        "for path in sorted(glob.glob(\"/content/ordered_csv_NEW/*.csv\")):\n",
        "    df_ord = pd.read_csv(path)\n",
        "    frac, steps = check_serpentine(df_ord)\n",
        "    name = path.split(\"/\")[-1].replace(\"_ordered_NEW.csv\",\"\")\n",
        "    RESULTS.append((name, frac, steps))\n",
        "\n",
        "# assemble a quick table\n",
        "res_df = pd.DataFrame(RESULTS, columns=[\"scan\",\"frac_correct\",\"n_steps\"])\n",
        "print(res_df.to_markdown(index=False))\n",
        "\n",
        "# highlight any scans <100%:\n",
        "bad = res_df[res_df.frac_correct < 1.0]\n",
        "if not bad.empty:\n",
        "    print(\"\\n‚ö†Ô∏è These scans have mis-ordered adjacencies:\")\n",
        "    print(bad.to_markdown(index=False))\n",
        "else:\n",
        "    print(\"\\n‚úÖ All scans follow a perfect serpentine path!\")\n"
      ],
      "metadata": {
        "id": "5bgAt2YDi2cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##end here"
      ],
      "metadata": {
        "id": "uL4WU9iIbik3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_DIR  = \"/content/SCAN_5\"#\"/content/drive/MyDrive/Scan_Orientate\"#\"/content/scans\"\n",
        "CSV_DIR  = \"/content/CSV_5\"#\"/content/scansCSV_oriented\"\n",
        "os.makedirs(CSV_DIR, exist_ok=True)\n",
        "MODEL = YOLO(\"/content/PESI/best.pt\")#\"/content/weights/best.pt\")   # <- path you used\n",
        "\n",
        "for img_path in sorted(glob.glob(f\"{IMG_DIR}/*.jpg\")):\n",
        "    H,W = cv2.imread(img_path).shape[:2]\n",
        "    res = MODEL.predict(img_path, imgsz=(H,W), conf=0.79,\n",
        "                        iou=0.80,\n",
        "                        )[0]#verbose=False)[0]\n",
        "    if not len(res.boxes):\n",
        "        print(\"‚ö†Ô∏è  no boxes in\", img_path);  continue\n",
        "\n",
        "    df = pd.DataFrame(res.boxes.xyxy.cpu().numpy().astype(int),\n",
        "                      columns=[\"x1\",\"y1\",\"x2\",\"y2\"])\n",
        "    df.insert(0, \"conf\", res.boxes.conf.cpu().numpy())\n",
        "    stem = Path(img_path).stem\n",
        "    df.to_csv(f\"{CSV_DIR}/{stem}.csv\", index=False)\n",
        "    print(\"‚Ä¢\", stem, \"‚Üí CSV ok\")\n",
        "print(\"üéâ  all scans processed ‚Üí\", CSV_DIR)\n"
      ],
      "metadata": {
        "id": "5_F92-rWJd6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title -ONE TO USE- crops patchees NEW, avoid uscita 6 e odina sottocartelle scan\n",
        "##Skips every CSV whose name contains _6_uscita (until you fix its order). 1.\n",
        "\"\"\"patches_700/\n",
        "  ‚îú‚îÄ‚îÄ T0_90_4_ingresso/\n",
        "  ‚îÇ     ‚îú‚îÄ‚îÄ H257_h032_T0_90_4_ingresso.jpg\n",
        "  ‚îÇ     ‚îî‚îÄ‚îÄ ‚Ä¶\n",
        "  ‚îî‚îÄ‚îÄ T0_90_4_uscita/\n",
        "        ‚îî‚îÄ‚îÄ ‚Ä¶\n",
        "\"\"\"\n",
        "# Adds a dual index in the file-name: -- H<GLOBAL:03d>_h<SCANIDX:03d>_<scan_name>.jpg\n",
        "'''where GLOBAL is a running counter over all patches (so H000, H001 ‚Ä¶ H599) and SCANIDX is the per-scan hole_idx.'''\n",
        "# Cell ‚ñ∏ crop patches into per-scan sub-folders with dual index ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# crop patches into per-scan sub-folders ‚Äì with ordered rows\n",
        "import cv2, os, glob, zipfile, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "ORDERED_DIR = \"/content/CSV_5/ORDINATE_CSV\"#\"/content/ORDINATE_CSV\"#\"/content\"                                   # *_ordered*.csv\n",
        "SCAN_DIR    = \"/content/SCAN_5\"#\"/content/scan_ruotate_da_inferire_ingressi\"#\"/content/drive/MyDrive/Scan_Orientate\"      # rotated scans\n",
        "PATCH_DIR   = \"/content/SCAN_5/PATCHES_5\"#\"/content/Patches_nuove\"#\"/content/patches_700_NEW\"\n",
        "ZIP_PATH    = \"Pat5ches_5NEW\"#\"/content/Patches_nuove.zip\"#\"/content/patches_700_NEW.zip\"\n",
        "HS = 350                                                   # half-crop size\n",
        "\n",
        "os.makedirs(PATCH_DIR, exist_ok=True)\n",
        "if os.path.exists(ZIP_PATH):\n",
        "    os.remove(ZIP_PATH)\n",
        "\n",
        "global_idx = 1\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
        "    for csv_path in sorted(glob.glob(f\"{ORDERED_DIR}/*_ordered*.csv\")):\n",
        "        if \"_6_uscita\" in csv_path:        # still on hold\n",
        "            print(\"‚è©  skipping\", Path(csv_path).name)\n",
        "            continue\n",
        "\n",
        "        stem = Path(csv_path).stem\n",
        "        for suf in (\"_ordered_NEW\", \"_ordered\"):\n",
        "            if stem.endswith(suf):\n",
        "                stem = stem[:-len(suf)]\n",
        "                break\n",
        "        scan_name = stem\n",
        "\n",
        "        img_path = f\"{SCAN_DIR}/{scan_name}.jpg\"\n",
        "        img      = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            print(f\"‚ö†Ô∏è  missing image {img_path}, skipping\"); continue\n",
        "        H, W = img.shape[:2]\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if \"manual_flag\" in df.columns:\n",
        "            df = df[df.manual_flag != 1]\n",
        "\n",
        "        df = df.sort_values(\"hole_idx\")    #  ‚Üê ensure 1,2,3‚Ä¶\n",
        "\n",
        "        scan_outdir = f\"{PATCH_DIR}/{scan_name}\"\n",
        "        os.makedirs(scan_outdir, exist_ok=True)\n",
        "\n",
        "        for _, r in df.iterrows():\n",
        "            cx = int((r.x1 + r.x2) // 2)\n",
        "            cy = int((r.y1 + r.y2) // 2)\n",
        "            x1, x2 = cx - HS, cx + HS\n",
        "            y1, y2 = cy - HS, cy + HS\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(W, x2), min(H, y2)\n",
        "\n",
        "            patch = img[y1:y2, x1:x2]\n",
        "            h, w  = patch.shape[:2]\n",
        "            patch = cv2.copyMakeBorder(patch, 0, 700-h, 0, 700-w,\n",
        "                                       cv2.BORDER_CONSTANT, value=0)\n",
        "\n",
        "            fname = (f\"H{global_idx:03d}_\"\n",
        "                     f\"h{int(r.hole_idx):03d}_{scan_name}.jpg\")\n",
        "            fpath = f\"{scan_outdir}/{fname}\"\n",
        "            cv2.imwrite(fpath, patch)\n",
        "            zf.write(fpath, arcname=f\"{scan_name}/{fname}\")\n",
        "            global_idx += 1\n",
        "\n",
        "print(f\"üéâ  Cropped {global_idx-1} patches to {PATCH_DIR}  (ZIP: {ZIP_PATH})\")\n"
      ],
      "metadata": {
        "id": "WM3diIEni2oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ciKDol5u2pPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TPM-cowpi2YD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}