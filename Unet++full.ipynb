{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1hDVGUwXPFh0ebtLIBM9al425idpzjOBT",
      "authorship_tag": "ABX9TyOD652wIIgfe2cDzKEywzkX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riccardo-Venturi/Tesi_Script_Colab/blob/main/Unet%2B%2Bfull.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unet plus 3 è promettente ma da evitare per ora\n",
        "\n",
        "Direi di applicare ++ ma con una strategia da cecchino; si congelano encoder(addestrato) e si passa solo il decoder che deve creare le maschere... sotto"
      ],
      "metadata": {
        "id": "SAWZrK0-_f47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Mossa                | Codice                                                       | Riduzione stimata                                            | Perdita di accuracy                                         |\n",
        "| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ----------------------------------------------------------- |\n",
        "| **AMP** (già attivo) | `with torch.cuda.amp.autocast()`                             | ‑35 % activations ([PyTorch Forums][1], [PyTorch Forums][2]) | nessuna                                                     |\n",
        "| **Freeze encoder**   | `for p in model.encoder.parameters(): p.requires_grad=False` | ‑25‑30 % (niente gradienti) ([PyTorch Forums][3])            | trascurabile su dataset piccoli                             |\n",
        "| **Encoder leggero**  | `encoder_name=\"efficientnet-b0\"`                             | ‑35‑40 % VRAM ([developer.furiosa.ai][4])                    | 0 – 1 pt Dice grazie ai pesi ImageNet ([docs.vultr.com][5]) |\n",
        "\n",
        "[1]: https://discuss.pytorch.org/t/explanation-of-exact-effect-of-amp-on-memory-usage/131358?utm_source=chatgpt.com \"Explanation of exact effect of AMP on memory usage - mixed-precision\"\n",
        "[2]: https://discuss.pytorch.org/t/increased-memory-usage-with-amp/125486?utm_source=chatgpt.com \"Increased memory usage with AMP - PyTorch Forums\"\n",
        "[3]: https://discuss.pytorch.org/t/does-freezzing-part-of-the-network-saves-gpu-memory/107781?utm_source=chatgpt.com \"Does freezzing part of the network saves GPU memory?\"\n",
        "[4]: https://developer.furiosa.ai/furiosa-models/latest/models/efficientnet_b0/?utm_source=chatgpt.com \"EfficientNetB0 - Furiosa Models\"\n",
        "[5]: https://docs.vultr.com/how-to-use-gradient-accumulation-to-overcome-gpu-memory-limitations?utm_source=chatgpt.com \"How to use Gradient Accumulation to Overcome GPU Memory ...\"\n"
      ],
      "metadata": {
        "id": "cmbDf9wdMF-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELLA 1: SETUP E CARICAMENTO DATI 512px\n",
        "# ===================================================================\n",
        "print(\"--- [1/4] Setup e Caricamento Dati 512px ---\")\n",
        "!pip install -q segmentation-models-pytorch==0.3.3 albumentations torchinfo\n",
        "import torch, cv2, json, pathlib, numpy as np\n",
        "import segmentation_models_pytorch as smp\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"✅ Librerie pronte.\")\n",
        "\n",
        "# Percorso del dataset GIÀ RIDIMENSIONATO a 512px su Drive\n",
        "DATA_512_DRIVE_PATH = \"/content/drive/MyDrive/Colab_Datasets/Augmented_Dataset_55\" # Assumo sia questo\n",
        "LOCAL_DATA_PATH = pathlib.Path(\"/content/dataset_512\")\n",
        "\n",
        "if not LOCAL_DATA_PATH.exists():\n",
        "    print(f\"Copiando dati da '{DATA_512_DRIVE_PATH}'...\")\n",
        "    !cp -r \"{DATA_512_DRIVE_PATH}\" \"{LOCAL_DATA_PATH}\"\n",
        "    print(\"✅ Copia completata.\")\n",
        "else:\n",
        "    print(f\"ℹ️ Dataset già presente in '{LOCAL_DATA_PATH}'.\")"
      ],
      "metadata": {
        "id": "3uVcRJltADTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Cella 2: Dataset e DataLoaders\n",
        "# ===================================================================\n",
        "# CELLA 2: DATASET E DATALOADERS\n",
        "# ===================================================================\n",
        "print(\"--- [2/4] Definizione Dataset e Creazione DataLoaders ---\")\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, ids, img_dir, msk_dir):\n",
        "        self.ids, self.img_dir, self.msk_dir = ids, img_dir, msk_dir\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, i):\n",
        "        stem = self.ids[i]\n",
        "        img = cv2.imread(str(self.img_dir / f\"{stem}.png\"))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        msk = cv2.imread(str(self.msk_dir / f\"{stem}.png\"), cv2.IMREAD_UNCHANGED)\n",
        "        img_tensor = torch.from_numpy(img/255.0).permute(2, 0, 1).float()\n",
        "        msk_tensor = torch.from_numpy(msk).long()\n",
        "        return img_tensor, msk_tensor\n",
        "\n",
        "split_path = LOCAL_DATA_PATH / \"split.json\"\n",
        "with open(split_path, 'r') as f: splits = json.load(f)\n",
        "train_ids, val_ids = splits['train'], splits['val']\n",
        "\n",
        "IMG_DIR = LOCAL_DATA_PATH / \"images\"\n",
        "MSK_DIR = LOCAL_DATA_PATH / \"masks\"\n",
        "\n",
        "# Batch size che DOVREBBERO funzionare ora\n",
        "# --- Crea i Dataset ---\n",
        "train_ds = SegmentationDataset(train_ids, IMG_DIR, MSK_DIR)\n",
        "val_ds = SegmentationDataset(val_ids, IMG_DIR, MSK_DIR)\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VAL_BATCH_SIZE = 4\n",
        "#potresti aumentare workers senza effettivo aumento di vram = 4?\n",
        "train_dl = DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_dl = DataLoader(val_ds, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"✅ DataLoaders pronti. Training set: {len(train_ds)}.\")"
      ],
      "metadata": {
        "id": "N5osaGdfAJwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#testiamo la B0 per vantaggi su vram con minima perdita dice su piccoli dataset; B1 forse sarebbe meglio ma per ora ci accontetntiamo"
      ],
      "metadata": {
        "id": "4SYE7EanaRVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Cella 3: Modello con Encoder Congelato e Loss\n",
        "# ===================================================================\n",
        "# CELLA 3: MODELLO CON ENCODER CONGELATO E LOSS\n",
        "# ===================================================================\n",
        "print(\"--- [3/4] Definizione Modello e Componenti di Training ---\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# --- La nostra Focal Loss custom ---\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha, self.gamma, self.reduction = alpha, gamma, reduction\n",
        "        self.crit = nn.CrossEntropyLoss(weight=None, reduction='none')\n",
        "    def forward(self, output, target):\n",
        "        ce_loss = self.crit(output, target)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        loss = ( (1 - pt)**self.gamma ) * ce_loss\n",
        "        if self.alpha is not None: loss = self.alpha[target] * loss\n",
        "        if self.reduction == 'mean': return loss.mean()\n",
        "        return loss\n",
        "\n",
        "# --- PARAMETRI E MODELLO ---\n",
        "LEARNING_RATE, NUM_EPOCHS, N_CLASSES = 1e-4, 50, 3\n",
        "#cambia encoder backbone per ottimizzare ram\n",
        "model = smp.UnetPlusPlus(encoder_name=\"efficientnet-b0\", encoder_weights=\"imagenet\", in_channels=3, classes=N_CLASSES).to(DEVICE)\n",
        "#model = smp.UnetPlusPlus(encoder_name=\"resnet34\", encoder_weights=\"imagenet\", in_channels=3, classes=N_CLASSES).to(DEVICE)\n",
        "\n",
        "# --- LA MOSSA VINCENTE: CONGELARE L'ENCODER ---\n",
        "for param in model.encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "print(\"✅ Encoder CONGELATO. Verrà addestrato solo il decoder.\")\n",
        "\n",
        "# Filtra i parametri per l'optimizer: solo quelli che richiedono gradienti\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "# --- LOSS, OPTIMIZER, SCHEDULER ---\n",
        "class_weights = torch.tensor([0.25, 0.5, 1.0]).to(DEVICE)\n",
        "loss_fn = FocalLoss(alpha=class_weights, gamma=2.0)\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=LEARNING_RATE) # Passa solo i parametri addestrabili\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
        "\n",
        "print(f\"✅ Modello: UnetPlusPlus-B0 (frozen), Loss: FocalLoss (Custom). Pronti.\")"
      ],
      "metadata": {
        "id": "Kmi9_OZKARMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELLA 4 (ex 4.9): PANNELLO DI CONTROLLO PRE-TRAINING CON CHECK MEMORIA\n",
        "# ===================================================================\n",
        "print(\"--- [4/5] CONTROLLO FINALE PRIMA DEL TRAINING ---\")\n",
        "\n",
        "def mem_report(msg=\"\"):\n",
        "    if DEVICE == 'cuda':\n",
        "        res = torch.cuda.memory_reserved()/1e9\n",
        "        alloc = torch.cuda.memory_allocated()/1e9\n",
        "        print(f\"{msg:>25}: reserved={res:.2f} GB, allocated={alloc:.2f} GB\")\n",
        "\n",
        "# --- 1. Controlla il DataLoader ---\n",
        "try:\n",
        "    images, masks = next(iter(train_dl))\n",
        "    print(\"✅ DataLoader: OK\")\n",
        "    print(f\"    -> Dimensione batch immagini: {images.shape}\")\n",
        "    print(f\"    -> Dimensione batch maschere: {masks.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERRORE NEL DATALOADER: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 2. Controlla il Modello e la Memoria ---\n",
        "model.eval()\n",
        "if DEVICE == 'cuda': torch.cuda.empty_cache() # Pulisci la cache prima del test\n",
        "mem_report(\"Inizio Test Modello\")\n",
        "\n",
        "with torch.no_grad(), torch.cuda.amp.autocast(enabled=(DEVICE=='cuda')):\n",
        "    try:\n",
        "        test_images = images.to(DEVICE)\n",
        "        outputs = model(test_images)\n",
        "        mem_report(\"Dopo Forward Pass\") # VEDIAMO QUANTO CONSUMA LA FORWARD\n",
        "        print(\"✅ Modello: OK\")\n",
        "        print(f\"    -> Dimensione output modello: {outputs.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERRORE NEL MODELLO: {e}\")\n",
        "        raise\n",
        "model.train()\n",
        "\n",
        "# --- 3. Controlla la Loss Function ---\n",
        "try:\n",
        "    test_masks = masks.to(DEVICE)\n",
        "    loss = loss_fn(outputs, test_masks)\n",
        "    print(\"✅ Loss Function: OK\")\n",
        "    print(f\"    -> Tipo di Loss: {loss_fn.__class__.__name__}\")\n",
        "    print(f\"    -> Valore di loss di test: {loss.item()}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERRORE NELLA LOSS FUNCTION: {e}\")\n",
        "    print(\"\\n--- DEBUG DIMENSIONI ---\")\n",
        "    print(f\"Shape Output (input): {outputs.shape}\")\n",
        "    print(f\"Shape Maschere (target): {masks.shape}\")\n",
        "    raise\n",
        "\n",
        "# Pulisci la memoria occupata dai tensori di test\n",
        "del images, masks, test_images, test_masks, outputs, loss\n",
        "if DEVICE == 'cuda': torch.cuda.empty_cache()\n",
        "mem_report(\"Dopo Pulizia Test\")\n",
        "\n",
        "print(\"\\n🚀 CONTROLLI SUPERATI. PRONTI PER IL TRAINING. 🚀\")"
      ],
      "metadata": {
        "id": "dz4B9uJEEOSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Cella 5: Training Loop con AMP (come richiesto)\n",
        "\n",
        "#Questa è la cella finale. Pulita, con la mixed precision (AMP) per massimizzare il risparmio di memoria.\n",
        "#\n",
        "#```python\n",
        "# ===================================================================\n",
        "# CELLA 5: TRAINING LOOP\n",
        "# ===================================================================\n",
        "print(\"--- [5/5] Inizio Training ---\")\n",
        "\n",
        "# Inizializza lo scaler per la mixed precision (solo se si usa la GPU)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == 'cuda'))\n",
        "best_val_loss = float('inf')\n",
        "checkpoint_path_B0 = \"/content/drive/MyDrive/Pesi/PesiUNET/best_model_FrozenEncoder_Epochs50_GPU_B0_v2.pth\"\n",
        "\n",
        "# Pulisci la cache della GPU un'ultima volta prima di iniziare\n",
        "if DEVICE == 'cuda': torch.cuda.empty_cache()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # --- Training Loop ---\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    loop_train = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", leave=False)\n",
        "    for images, masks in loop_train:\n",
        "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Usa il contesto autocast per la mixed precision\n",
        "        with torch.cuda.amp.autocast(enabled=(DEVICE == 'cuda')):\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, masks)\n",
        "\n",
        "        # Scala la loss e fa la backward pass\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # --- Validation Loop ---\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_dl:\n",
        "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "            # Usa autocast anche in validazione per coerenza e velocità\n",
        "            with torch.cuda.amp.autocast(enabled=(DEVICE == 'cuda')):\n",
        "                outputs = model(images)\n",
        "                loss = loss_fn(outputs, masks)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dl)\n",
        "    avg_val_loss = val_loss / len(val_dl)\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.6f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), checkpoint_path_B0)\n",
        "        print(f\"--> New best model saved to {checkpoint_path_B0} with val loss {best_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Completato ---\")"
      ],
      "metadata": {
        "id": "di7LUpHuAlFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELLA 6: INFERENZA TOTALE E SALVATAGGIO SU DISCO\n",
        "# ===================================================================\n",
        "print(\"--- [6/7] INFERENZA TOTALE SU TUTTO IL DATASET E SALVATAGGIO ---\")\n",
        "\n",
        "# --- 1. PARAMETRI ---\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/Pesi/PesiUNET/best_model_FrozenEncoder_Epochs50_GPU_B0_v2.pth\"\n",
        "# Il percorso dei dati 512px che hai usato per il training\n",
        "DATA_PATH = pathlib.Path(\"/content/dataset_512\")\n",
        "OUTPUT_DIR = pathlib.Path(\"/content/inference_results_FULL\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "ENCODER_NAME = \"efficientnet-b0\" # Assicurati che sia quello giusto!\n",
        "\n",
        "# --- 2. CARICA MODELLO ---\n",
        "INFERENCE_MODEL = smp.UnetPlusPlus(\n",
        "    encoder_name=ENCODER_NAME,\n",
        "    encoder_weights=None,\n",
        "    in_channels=3,\n",
        "    classes=3\n",
        ").to(DEVICE)\n",
        "try:\n",
        "    INFERENCE_MODEL.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n",
        "    print(f\"✅ Pesi caricati con successo da: {CHECKPOINT_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERRORE NEL CARICAMENTO DEI PESI: {e}\")\n",
        "    raise e\n",
        "INFERENCE_MODEL.eval()\n",
        "\n",
        "# --- 3. CARICA TUTTI GLI ID DELLE IMMAGINI ---\n",
        "# Invece di usare solo il test set, prendiamo TUTTE le immagini per avere una visione completa\n",
        "all_image_paths = sorted(list((DATA_PATH / \"images\").glob(\"*.png\")))\n",
        "all_stems = [p.stem for p in all_image_paths]\n",
        "print(f\"Trovate {len(all_stems)} immagini totali da processare.\")\n",
        "\n",
        "# --- 4. PRE-PROCESSING PER L'INFERENZA ---\n",
        "inference_transform = A.Compose([\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# --- 5. LOOP DI INFERENZA SU TUTTO ---\n",
        "print(\"Avvio del processo di inferenza totale...\")\n",
        "with torch.no_grad():\n",
        "    for stem in tqdm(all_stems, desc=\"Processing All Images\"):\n",
        "        img_path = DATA_PATH / \"images\" / f\"{stem}.png\"\n",
        "        image = cv2.imread(str(img_path))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        processed = inference_transform(image=image)\n",
        "        image_normalized = processed['image']\n",
        "\n",
        "        input_tensor = torch.from_numpy(image_normalized).permute(2,0,1).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        with torch.amp.autocast(device_type=DEVICE, enabled=(DEVICE=='cuda')):\n",
        "            output = INFERENCE_MODEL(input_tensor)\n",
        "\n",
        "        predicted_mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy().astype(np.uint8)\n",
        "\n",
        "        save_path = OUTPUT_DIR / f\"{stem}_pred.png\"\n",
        "        cv2.imwrite(str(save_path), predicted_mask)\n",
        "\n",
        "print(f\"\\n✅ Inferenza completata. {len(all_stems)} maschere predette sono state salvate in '{OUTPUT_DIR}'.\")"
      ],
      "metadata": {
        "id": "rNCfrgtWilV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Cella 7: Il Visualizzatore di Maschere\n",
        "\n",
        "#Questa cella contiene la tua logica di visualizzazione, ma trasformata in una funzione potente e riutilizzabile. Esegue la visualizzazione a 3 pannelli (Originale, Reale, Prevista) che è il modo migliore per giudicare.\n",
        "\n",
        "# ===================================================================\n",
        "# CELLA 7: VISUALIZZATORE DI RISULTATI\n",
        "# ===================================================================\n",
        "print(\"--- [7/7] Visualizzatore di Risultati di Inferenza ---\")\n",
        "\n",
        "def visualize_prediction(stem_name):\n",
        "    \"\"\"\n",
        "    Carica e visualizza l'immagine originale, la maschera reale e la maschera predetta\n",
        "    per un dato 'stem' (nome del file).\n",
        "    \"\"\"\n",
        "    original_img_path = DATA_PATH / \"images\" / f\"{stem_name}.png\"\n",
        "    true_mask_path = DATA_PATH / \"masks\" / f\"{stem_name}.png\"\n",
        "    pred_mask_path = OUTPUT_DIR / f\"{stem_name}_pred.png\"\n",
        "\n",
        "    if not original_img_path.exists() or not pred_mask_path.exists():\n",
        "        print(f\"❌ Immagine o maschera predetta non trovata per lo stem: {stem_name}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- Risultati per: {stem_name} ---\")\n",
        "\n",
        "    original_img = cv2.imread(str(original_img_path))\n",
        "    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    predicted_mask = cv2.imread(str(pred_mask_path), cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
        "\n",
        "    axes[0].imshow(original_img)\n",
        "    axes[0].set_title(\"Immagine Originale\")\n",
        "\n",
        "    if true_mask_path.exists():\n",
        "        true_mask = cv2.imread(str(true_mask_path), cv2.IMREAD_UNCHANGED)\n",
        "        axes[1].imshow(true_mask, cmap='jet', vmin=0, vmax=2)\n",
        "        axes[1].set_title(\"Maschera Reale\")\n",
        "    else:\n",
        "        axes[1].set_title(\"Maschera Reale (Non trovata)\")\n",
        "\n",
        "    axes[2].imshow(predicted_mask, cmap='jet', vmin=0, vmax=2)\n",
        "    axes[2].set_title(\"Maschera Prevista\")\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# --- ESEMPI DI UTILIZZO ---\n",
        "# Ora puoi visualizzare qualsiasi immagine che vuoi, semplicemente chiamando la funzione.\n",
        "\n",
        "# Visualizza un'immagine specifica\n",
        "visualize_prediction(\"hole_0021_orig\")\n",
        "\n",
        "# Visualizza una delle immagini di test\n",
        "if 'test_ids' in locals() and test_ids:\n",
        "    visualize_prediction(test_ids[0])\n",
        "\n",
        "# Visualizza un'altra immagine a caso\n",
        "visualize_prediction(\"hole_0035_aug_5\")"
      ],
      "metadata": {
        "id": "TSaBYNh0oyUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
        "\n",
        "print(sorted(list(smp.encoders.encoders.keys()))[:30], \"...\")  # native\n",
        "# timm bridge\n",
        "print([k for k in smp.encoders.encoders.keys() if k.startswith(\"tu-\")][:30])\n"
      ],
      "metadata": {
        "id": "_58XuWzz5_Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cella DEGENERE test consumo picco test di ogni backbone\n",
        "from torch.amp import autocast, GradScaler\n",
        "encoders_to_test = [\"efficientnet-b0\",\n",
        "                    \"efficientnet-b1\",\n",
        "                    \"tu-mobilenetv3_large_100\",\n",
        "                    \"resnet18\",\"efficientnet-b7\"]\n",
        "\n",
        "for enc in encoders_to_test:\n",
        "    model = smp.UnetPlusPlus(enc, encoder_weights=\"imagenet\", in_channels=3, classes=3).cuda()\n",
        "    with torch.no_grad(), autocast(\"cuda\"):\n",
        "        x = torch.randn(1,3,512,512, device=\"cuda\")\n",
        "        _ = model(x)\n",
        "    print(enc, torch.cuda.max_memory_reserved()/1e9, \"GB\")\n",
        "    torch.cuda.reset_peak_memory_stats(); torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "jfYVn28eHqjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#scelta backbone della rete\n",
        "| Famiglia                                                | Parametri ↑                  | Peak VRAM\\* (512², batch 2, fp32) | Accuracy tipica su set piccoli                     | Quando usarla                             |\n",
        "| ------------------------------------------------------- | ---------------------------- | --------------------------------- | -------------------------------------------------- | ----------------------------------------- |\n",
        "| **ResNet‑34** (baseline)                                | 21 M ([docs.pytorch.org][1]) | ≈ 11‑12 GiB                       | solida, ma pesante                                 | valore di partenza                        |\n",
        "| **ResNet‑18**                                           | 11 M                         | ≈ 9 GiB                           | ‑1 pt Dice rispetto a R‑34                         | se vuoi cambiare il minimo indispensabile |\n",
        "| **EfficientNet‑B0**                                     | **5.3 M** ([viso.ai][2])     | **≈ 7 GiB**                       | = / +0‑1 pt Dice (grazie allo scaling compresso)   | scelta più equilibrata                    |\n",
        "| **EfficientNet‑B1**                                     | 7.9 M                        | ≈ 8 GiB                           | +0‑1 pt vs B0, ancora leggera                      | se hai 1 GiB in più di margine            |\n",
        "| **MobileNet V3‑Large**                                  | 5.4 M ([keras.io][3])        | ≈ 6.5 GiB                         | ‑1‑3 pt Dice in texture complesse                  | massima velocità / min‑VRAM               |\n",
        "| **HarDNet‑68**                                          | 17 M                         | ≈ 8‑9 GiB                         | +1 pt Dice nei polipi, buona velocità ([arXiv][4]) | se vuoi FPS senza perdere troppo          |\n",
        "| \\*stima UNet++ completo (decoder da 256→64), senza AMP. |                              |                                   |                                                    |                                           |\n",
        "\n",
        "[1]: https://docs.pytorch.org/vision/0.21/models/generated/torchvision.models.resnet34.html?utm_source=chatgpt.com \"resnet34 — Torchvision 0.21 documentation\"\n",
        "[2]: https://viso.ai/deep-learning/efficientnet/?utm_source=chatgpt.com \"EfficientNet: Optimizing Deep Learning Efficiency - Viso Suite\"\n",
        "[3]: https://keras.io/api/applications/mobilenet/?utm_source=chatgpt.com \"MobileNet, MobileNetV2, and MobileNetV3 - Keras\"\n",
        "[4]: https://arxiv.org/abs/2101.07172?utm_source=chatgpt.com \"HarDNet-MSEG: A Simple Encoder-Decoder Polyp Segmentation ...\"\n"
      ],
      "metadata": {
        "id": "ZFyY7if_CRqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3 · Come cambiano VRAM e tempo\n",
        "\n",
        "#Il peso dei parametri incide poco; la VRAM è dominata dalle feature‑map prodotte dall’encoder:\n",
        "#VRAM≈B  (∑lClHlWl)×bytes×F\n",
        "#VRAM≈B(l∑​Cl​Hl​Wl​)×bytes×F\n",
        "#\n",
        "#dove C_l cala molto quando passi da ResNet a EfficientNet/MobileNet. Con batch 2, 512 px, fp16 (AMP) e UNet++ decoder standard:\n",
        "#\n",
        "#    ResNet‑34 ➜ ≈ 7‑8 GiB VRAM (con AMP).\n",
        "#\n",
        "#    EfficientNet‑B0 ➜ ≈ 4‑5 GiB (‑35 %).\n",
        "#\n",
        "#    MobileNet V3 ➜ ≈ 4 GiB (‑45 %).\n",
        "#\n",
        "#AMP dimezza i byte per activation (F≈2) e conviene sempre su T4\n",
        "\n",
        "###ResNet‑34 ➜ ≈ 7‑8 GiB VRAM (con AMP).\n",
        "###\n",
        "###EfficientNet‑B0 ➜ ≈ 4‑5 GiB (‑35 %).\n",
        "###\n",
        "###MobileNet V3 ➜ ≈ 4 GiB (‑45 %)."
      ],
      "metadata": {
        "id": "BAURpIVdCWul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}