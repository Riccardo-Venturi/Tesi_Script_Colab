{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Pg3ScU8S7yiu"
      ],
      "gpuType": "T4",
      "mount_file_id": "1NcCbRvdwLzVXDf05JKpg4hi1p_t6D14e",
      "authorship_tag": "ABX9TyPh5he7zsYpN7fDqvS/xCPx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riccardo-Venturi/Tesi_Script_Colab/blob/main/Unet%2B%2BV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Obiettivo del Pre-addestramento (le 25 epoche): L'obiettivo NON √® la precisione. √à insegnare al modello le caratteristiche di basso livello:\n",
        "\n",
        "        Che aspetto ha la texture della fibra di carbonio?\n",
        "\n",
        "        Qual √® la forma generale, circolare, di un foro?\n",
        "\n",
        "        Dove, approssimativamente, si trova il danno rispetto al foro?\n",
        "\n",
        "        Come distinguere i bordi netti da quelli frastagliati?\n",
        "\n",
        "  Poi si passa al fine-tune incrementale pre-trained_model.pth; modello conoscenza base dominio del danno :: **ENCODER CONGELATO**\n",
        "\n",
        "Il Ciclo di Fine-Tuning (per ogni \"ondata\" di 10 maschere nuove):\n",
        "\n",
        "  model.load_state_dict(torch.load('pre-trained_model.pth'))\n",
        "   for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "<details>Learning Rate BASSISSIMO: Questa √® la regola d'oro del fine-tuning.[2][3] I pesi sono gi√† buoni, non vogliamo stravolgerli. Usa un learning rate di uno o due ordini di grandezza inferiore a quello del pre-training. Se prima avevi 1e-3, ora usa 1e-5.[4]\n",
        "\n",
        "Pochissime Epoche per Sessione: NON devi fare 25 epoche su 10 immagini. Le imparerebbe a memoria in un attimo. Per ogni sessione di fine-tuning (dopo aver aggiunto 10 nuove maschere), addestra per 3-5 epoche al massimo.[5] L'obiettivo √® dare un \"nudge\", una piccola spinta nella direzione giusta, non ri-addestrare da capo.[6]\n",
        "\n",
        "Monitoraggio Ossessivo della Validation Loss: Ad ogni epoca, calcola la loss sul tuo \"Validation Set d'Elite\".\n",
        "\n",
        "    Se la val_loss scende: Fantastico! Salva il modello. Stai andando nella direzione giusta.\n",
        "\n",
        "    Se la val_loss smette di scendere o, peggio, inizia a salire mentre la train_loss continua a scendere: STOP IMMEDIATO. Quello √® il segnale inequivocabile di overfitting.[7][8] Significa che il modello ha smesso di imparare e ha iniziato a memorizzare.\n",
        "\n",
        "Data Augmentation Forte: Siccome il tuo Golden Dataset √® piccolo, devi sfruttarlo al massimo. Usa albumentations per applicare rotazioni, flip, variazioni di luminosit√†/contrasto. Questo crea artificialmente nuovi esempi e costringe il modello a generalizzare."
      ],
      "metadata": {
        "id": "M0bEac01GzD9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIMjRL1WDqGP"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# #@title CELLA 1: SETUP INIZIALE\n",
        "# ===================================================================\n",
        "print(\"--- [1/4] Installazione librerie ---\")\n",
        "!pip install -q segmentation-models-pytorch==0.3.3 albumentations==1.3.1 torchinfo==1.8.0\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import shutil\n",
        "import random\n",
        "import re\n",
        "\n",
        "print(\"‚úÖ Librerie pronte.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELLA 2: PREPARAZIONE E SPLIT DEL DATASET RADIOGRAFICO\n",
        "# =============================================================================\n",
        "print(\"--- [2/4] Preparazione e Split del Dataset ---\")\n",
        "\n",
        "# --- 1. CONFIGURA I TUOI PERCORSI ---\n",
        "# Attenzione: .glob('**/*.jpg') legge ricorsivamente in tutte le sottocartelle\n",
        "IMG_DIR = Path(\"/content/drive/MyDrive/Patches_test/Radio_Patches_Normalized\")\n",
        "MASK_DIR = Path(\"/content/drive/MyDrive/Patches_test/Patches_masks\") # Le nuove maschere\n",
        "\n",
        "# Directory di output per il dataset splittato\n",
        "DATASET_DIR = Path(\"/content/dataset_radiografico_split\")\n",
        "\n",
        "# --- 2. TROVA E ACCOPPIA I FILE ---\n",
        "print(\"Scansione file in corso...\")\n",
        "all_images = list(IMG_DIR.glob(\"**/*.jpg\"))\n",
        "all_masks = list(MASK_DIR.glob(\"*.png\"))\n",
        "print(f\"Trovate {len(all_images)} immagini e {len(all_masks)} maschere.\")\n",
        "\n",
        "# Per accoppiarli, dobbiamo usare un ID comune. Usiamo il numero del foro.\n",
        "# Funzione per estrarre H<numero> dal nome del file\n",
        "def extract_hole_id(path):\n",
        "    match = re.search(r'H(\\d+)', path.stem)\n",
        "    return f\"H{match.group(1)}\" if match else None\n",
        "\n",
        "# Creiamo dei dizionari per un lookup veloce\n",
        "img_map = {extract_hole_id(p): p for p in all_images if extract_hole_id(p) is not None}\n",
        "mask_map = {extract_hole_id(p): p for p in all_masks if extract_hole_id(p) is not None}\n",
        "\n",
        "# Trova gli ID in comune\n",
        "common_ids = sorted(list(set(img_map.keys()) & set(mask_map.keys())))\n",
        "print(f\"\\nTrovate {len(common_ids)} coppie (immagine-maschera) corrispondenti.\")\n",
        "\n",
        "# --- 3. CREAZIONE LISTA DI COPPIE E SPLIT ---\n",
        "# Lista di tuple (percorso_immagine, percorso_maschera)\n",
        "paired_files = [(img_map[id], mask_map[id]) for id in common_ids]\n",
        "\n",
        "if DATASET_DIR.exists(): shutil.rmtree(DATASET_DIR)\n",
        "for subset in [\"train\", \"val\", \"test\"]:\n",
        "    (DATASET_DIR / subset / \"images\").mkdir(parents=True)\n",
        "    (DATASET_DIR / subset / \"masks\").mkdir(parents=True)\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(paired_files)\n",
        "train_end = int(len(paired_files) * 0.8)\n",
        "val_end = train_end + int(len(paired_files) * 0.1)\n",
        "\n",
        "train_files = paired_files[:train_end]\n",
        "val_files = paired_files[train_end:val_end]\n",
        "test_files = paired_files[val_end:]\n",
        "\n",
        "def copy_files(files, subset_name):\n",
        "    for img_path, mask_path in tqdm(files, desc=f\"Copia in {subset_name}\"):\n",
        "        # Usiamo lo stesso nome file per entrambi per coerenza\n",
        "        shutil.copy(img_path, DATASET_DIR / subset_name / \"images\" / mask_path.name)\n",
        "        shutil.copy(mask_path, DATASET_DIR / subset_name / \"masks\" / mask_path.name)\n",
        "\n",
        "copy_files(train_files, \"train\")\n",
        "copy_files(val_files, \"val\")\n",
        "copy_files(test_files, \"test\")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset creato con successo in '{DATASET_DIR}':\")\n",
        "print(f\"   - Train: {len(train_files)} campioni\")\n",
        "print(f\"   - Val: {len(val_files)} campioni\")\n",
        "print(f\"   - Test: {len(test_files)} campioni\")"
      ],
      "metadata": {
        "id": "JpQA4iG-D2XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELLA 3: DATASET PYTORCH E DATALOADERS\n",
        "# ===================================================================\n",
        "print(\"--- [3/4] Creazione Dataset e DataLoaders ---\")\n",
        "\n",
        "# --- 1. PARAMETRI ---\n",
        "IMG_SIZE = 512\n",
        "BATCH_SIZE = 8 # Iniziamo con 8, si pu√≤ aumentare se la VRAM lo permette\n",
        "\n",
        "# --- 2. TRASFORMAZIONI CON ALBUMENTATIONS ---\n",
        "# Queste sono immagini in scala di grigi (1 canale)\n",
        "transform = A.Compose([\n",
        "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
        "    A.Normalize(mean=(0.5,), std=(0.5,)), # Normalizzazione per 1 canale\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# --- 3. CLASSE DATASET PERSONALIZZATA ---\n",
        "class RadioDataset(Dataset):\n",
        "    def __init__(self, subset):\n",
        "        self.img_dir = DATASET_DIR / subset / \"images\"\n",
        "        self.mask_dir = DATASET_DIR / subset / \"masks\"\n",
        "        # I nomi ora corrispondono perfettamente grazie alla cella precedente\n",
        "        self.file_names = sorted([p.name for p in self.mask_dir.glob(\"*.png\")])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_name = self.file_names[idx]\n",
        "        img_path = self.img_dir / file_name\n",
        "        mask_path = self.mask_dir / file_name\n",
        "\n",
        "        # Carica immagine in SCALA DI GRIGI\n",
        "        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
        "        # Carica maschera\n",
        "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Applica le trasformazioni\n",
        "        augmented = transform(image=img, mask=mask)\n",
        "        # Ritorna il tensore immagine e il tensore maschera (.long() √® cruciale per la loss)\n",
        "        return augmented['image'], augmented['mask'].long()\n",
        "\n",
        "# --- 4. CREA I DATALOADER ---\n",
        "train_loader = DataLoader(RadioDataset(\"train\"), batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(RadioDataset(\"val\"), batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"‚úÖ DataLoaders pronti. Batch size: {BATCH_SIZE}.\")\n",
        "\n",
        "# Test veloce per verificare che tutto funzioni\n",
        "try:\n",
        "    img_batch, msk_batch = next(iter(train_loader))\n",
        "    print(f\"Dimensioni batch immagini: {img_batch.shape}\") # Atteso: [BATCH_SIZE, 1, 512, 512]\n",
        "    print(f\"Dimensioni batch maschere: {msk_batch.shape}\")   # Atteso: [BATCH_SIZE, 512, 512]\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERRORE NEL DATALOADER: {e}\")"
      ],
      "metadata": {
        "id": "Dz04ZIpBD2UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELLA 4: TRAINING LOOP (PRE-ADDESTRAMENTO CON ENCODER CONGELATO)\n",
        "# =============================================================================\n",
        "print(\"--- [4/4] Avvio Pre-addestramento ---\")\n",
        "\n",
        "# --- 1. PARAMETRI DI TRAINING ---\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ENCODER_NAME = \"efficientnet-b0\" # Leggero ed efficace\n",
        "NUM_EPOCHS = 25 # Come richiesto\n",
        "LEARNING_RATE = 1e-3 # Partiamo un po' pi√π aggressivi per il pre-training\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/PesiUNETPP/best_model_pre-training.pth\"\n",
        "\n",
        "# --- 2. DEFINIZIONE MODELLO ---\n",
        "model = smp.UnetPlusPlus(\n",
        "    encoder_name=ENCODER_NAME,\n",
        "    encoder_weights=\"imagenet\", # Usiamo i pesi pre-addestrati!\n",
        "    in_channels=1,              # <-- CRUCIALE: le nostre immagini hanno 1 canale\n",
        "    classes=3                   # 0: bg, 1: hole, 2: damage\n",
        ").to(DEVICE)\n",
        "\n",
        "# --- 3. STRATEGIA CECCHINO: CONGELA L'ENCODER ---\n",
        "for param in model.encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "print(f\"‚úÖ Modello UNet++ con encoder '{ENCODER_NAME}' (CONGELATO) pronto su {DEVICE}.\")\n",
        "\n",
        "# --- 4. LOSS, OPTIMIZER, SCHEDULER ---\n",
        "# La DiceLoss √® un'ottima scelta per la segmentazione sbilanciata\n",
        "loss_fn = smp.losses.DiceLoss(mode='multiclass', from_logits=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "scaler = torch.cuda.amp.GradScaler() # Per mixed precision\n",
        "\n",
        "# --- 5. LOOP DI TRAINING ---\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    for images, masks in loop:\n",
        "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, masks)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, masks)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        print(f\"üèÜ Nuovo modello migliore! Salvo i pesi in '{CHECKPOINT_PATH}'\")\n",
        "        best_val_loss = avg_val_loss\n",
        "        # Assicurati che la cartella esista\n",
        "        Path(CHECKPOINT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
        "        torch.save(model.state_dict(), CHECKPOINT_PATH)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "print(\"\\nüéâ Pre-addestramento Finito.\")"
      ],
      "metadata": {
        "id": "2-yJEcObECw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELLA 4.5: VALUTAZIONE DEL MODELLO PRE-ADDESTRATO (ROBUSTA SMP)\n",
        "# =============================================================================\n",
        "import torch\n",
        "import segmentation_models_pytorch as smp\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"--- [FASE di Valutazione] Valutazione del modello pre-addestrato sul Test Set ---\")\n",
        "\n",
        "MODEL_TO_EVAL_PATH = \"/content/drive/MyDrive/PesiUNETPP/best_model_pre-training.pth\"\n",
        "EVAL_DATASET_DIR = Path(\"/content/dataset_radiografico_split\")\n",
        "PREDICTIONS_DIR = Path(\"/content/pre-training_predictions\"); PREDICTIONS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# --- Modello ---\n",
        "model = smp.UnetPlusPlus(encoder_name=ENCODER_NAME, in_channels=1, classes=3).to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_TO_EVAL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# --- Dataloader test ---\n",
        "test_dataset = RadioDataset(\"test\")\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "test_file_names = test_dataset.file_names\n",
        "print(f\"Valutazione su {len(test_dataset)} campioni del test set.\")\n",
        "\n",
        "# --- Accumulatori per stats (liste di tensori [B,C]) ---\n",
        "tps, fps, fns, tns = [], [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (images, masks) in enumerate(tqdm(test_loader, desc=\"Generando predizioni sul test set\")):\n",
        "        images = images.to(DEVICE, non_blocking=True)\n",
        "        masks_gt = masks.to(DEVICE, non_blocking=True).long()   # dtype intero, shape [B,H,W]\n",
        "\n",
        "        logits = model(images)           # [B,3,H,W]\n",
        "        preds  = torch.argmax(logits, dim=1)  # [B,H,W] in {0,1,2}\n",
        "\n",
        "        # 1) Stats per-batch (posizionali, NO keywords per compatibilit√† SMP)\n",
        "        tp, fp, fn, tn = smp.metrics.get_stats(preds, masks_gt, mode='multiclass', num_classes=3)\n",
        "\n",
        "        # 2) Accumula per concatenazione successiva\n",
        "        tps.append(tp); fps.append(fp); fns.append(fn); tns.append(tn)\n",
        "\n",
        "        # 3) Salva maschere predette per ispezione\n",
        "        preds_np = preds.detach().cpu().numpy()\n",
        "        for j, pred_mask in enumerate(preds_np):\n",
        "            file_idx = i * BATCH_SIZE + j\n",
        "            if file_idx < len(test_file_names):\n",
        "                cv2.imwrite(str(PREDICTIONS_DIR / test_file_names[file_idx]),\n",
        "                            pred_mask.astype(np.uint8))\n",
        "\n",
        "# --- Concatena su dimensione \"immagini\" ---\n",
        "tp = torch.cat(tps, dim=0)\n",
        "fp = torch.cat(fps, dim=0)\n",
        "fn = torch.cat(fns, dim=0)\n",
        "tn = torch.cat(tns, dim=0)\n",
        "\n",
        "# Sanity checks\n",
        "assert tp.ndim == 2 and tp.shape[1] == 3, f\"tp shape inattesa: {tp.shape}\"\n",
        "M, C = tp.shape\n",
        "\n",
        "# --- Metriche ---\n",
        "iou_micro_img  = smp.metrics.iou_score(tp, fp, fn, tn, reduction='micro-imagewise')\n",
        "dice_micro_img = smp.metrics.f1_score (tp, fp, fn, tn, reduction='micro-imagewise')\n",
        "\n",
        "# Per-classe (macro su immagini)\n",
        "iou_per_image_per_class = smp.metrics.iou_score(tp, fp, fn, tn, reduction='none')  # [M,C]\n",
        "dice_per_image_per_class = smp.metrics.f1_score(tp, fp, fn, tn, reduction='none')  # [M,C]\n",
        "iou_per_class  = iou_per_image_per_class.mean(dim=0)   # [C]\n",
        "dice_per_class = dice_per_image_per_class.mean(dim=0)  # [C]\n",
        "\n",
        "# Micro globale (sommone)\n",
        "iou_micro  = smp.metrics.iou_score(tp, fp, fn, tn, reduction='micro')\n",
        "dice_micro = smp.metrics.f1_score (tp, fp, fn, tn, reduction='micro')\n",
        "\n",
        "print(\"\\n--- METRICHE SUL TEST SET (Modello Pre-Addestrato) ---\")\n",
        "print(f\"-> IoU micro-imagewise: {iou_micro_img.item():.4f}\")\n",
        "print(f\"-> Dice micro-imagewise:{dice_micro_img.item():.4f}\")\n",
        "print(f\"-> IoU micro (globale): {iou_micro.item():.4f}\")\n",
        "print(f\"-> Dice micro (globale):{dice_micro.item():.4f}\")\n",
        "for c, (ii, dd) in enumerate(zip(iou_per_class, dice_per_class)):\n",
        "    print(f\"   Classe {c}: IoU={ii.item():.4f}  Dice={dd.item():.4f}\")\n",
        "print(\"-----------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "7I6SPcqSSCle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title visualizza Maschere e scarica zip\n",
        "# QA + PACKAGER per CVAT \"Segmentation mask 1.1\"\n",
        "# ------------------------------------------------\n",
        "import os, glob, zipfile, cv2, numpy as np, shutil, matplotlib\n",
        "from pathlib import Path\n",
        "\n",
        "# CONFIG\n",
        "MASKS_DIR = Path(\"/content/pre-training_predictions\")   # original 0/1/2 masks\n",
        "VIS_DIR   = Path(\"/content/mask_previews\")                   # where colored previews will go\n",
        "ZIP_PATH  = Path(\"/content/mask_previews.zip\")\n",
        "\n",
        "# Clean previous run\n",
        "if VIS_DIR.exists():\n",
        "    shutil.rmtree(VIS_DIR)\n",
        "VIS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Generate colored previews\n",
        "for p in sorted(MASKS_DIR.glob(\"*.png\")):\n",
        "    m = cv2.imread(str(p), cv2.IMREAD_UNCHANGED)\n",
        "    if m is None:\n",
        "        print(f\"Skip unreadable {p.name}\")\n",
        "        continue\n",
        "    # ensure uint8 0/1/2\n",
        "    m = np.clip(np.rint(m), 0, 2).astype(np.uint8)\n",
        "    vis = cv2.applyColorMap((m*120).astype(np.uint8), cv2.COLORMAP_PARULA)\n",
        "    out_path = VIS_DIR / p.name\n",
        "    cv2.imwrite(str(out_path), vis)\n",
        "\n",
        "# Zip the previews\n",
        "if ZIP_PATH.exists():\n",
        "    ZIP_PATH.unlink()\n",
        "with zipfile.ZipFile(ZIP_PATH, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "    for img_path in VIS_DIR.glob(\"*.png\"):\n",
        "        zf.write(img_path, img_path.name)\n",
        "\n",
        "ZIP_PATH\n",
        "\n"
      ],
      "metadata": {
        "id": "3h_7v4yuQ9Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hxl9JzmUQ88r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELLA 5.25: CONVERTITORE UNA TANTUM (COLORE -> 0,1,2)\n",
        "# =============================================================================\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import shutil\n",
        "\n",
        "print(\"--- [TASK UNA TANTUM] Conversione delle maschere 'd'oro' da Colore a Numerico ---\")\n",
        "\n",
        "# --- 1. CONFIGURAZIONE ---\n",
        "# La cartella con le maschere a colori esportate da CVAT\n",
        "SOURCE_GOLDEN_MASKS_DIR = Path(\"/content/drive/MyDrive/GoldenDataset/masks\")\n",
        "\n",
        "# La nuova cartella dove salveremo le maschere corrette (formato 0,1,2)\n",
        "# NON SOVRASCRIVIAMO NIENTE PER SICUREZZA\n",
        "TARGET_CLEAN_MASKS_DIR = Path(\"/content/drive/MyDrive/GoldenDataset_Clean/masks\")\n",
        "TARGET_CLEAN_MASKS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Copia anche le immagini corrispondenti nella nuova struttura\n",
        "SOURCE_GOLDEN_IMG_DIR = Path(\"/content/drive/MyDrive/GoldenDataset/images\")\n",
        "TARGET_CLEAN_IMG_DIR = Path(\"/content/drive/MyDrive/GoldenDataset_Clean/images\")\n",
        "TARGET_CLEAN_IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Copia delle immagini sorgente...\")\n",
        "for img_file in SOURCE_GOLDEN_IMG_DIR.glob(\"*.png\"):\n",
        "    shutil.copy(img_file, TARGET_CLEAN_IMG_DIR / img_file.name)\n",
        "\n",
        "# La mappa di conversione (BGR di OpenCV -> Indice di Classe)\n",
        "COLOR_MAP = {\n",
        "    (0, 0, 0): 0,          # background\n",
        "    (100, 100, 255): 1,    # hole (Rosso/Rosa)\n",
        "    (220, 220, 0): 2       # damage (Ciano)\n",
        "}\n",
        "\n",
        "# --- 2. CICLO DI CONVERSIONE ---\n",
        "print(f\"\\nInizio conversione di {len(list(SOURCE_GOLDEN_MASKS_DIR.glob('*.png')))} maschere...\")\n",
        "for mask_color_path in tqdm(SOURCE_GOLDEN_MASKS_DIR.glob(\"*.png\")):\n",
        "    mask_color = cv2.imread(str(mask_color_path), cv2.IMREAD_COLOR)\n",
        "\n",
        "    h, w, _ = mask_color.shape\n",
        "    mask_indexed = np.zeros((h, w), dtype=np.uint8)\n",
        "\n",
        "    for color_bgr, class_index in COLOR_MAP.items():\n",
        "        mask_indexed[np.all(mask_color == color_bgr, axis=-1)] = class_index\n",
        "\n",
        "    save_path = TARGET_CLEAN_MASKS_DIR / mask_color_path.name\n",
        "    cv2.imwrite(str(save_path), mask_indexed)\n",
        "\n",
        "print(f\"\\n‚úÖ CONVERSIONE COMPLETATA. Le tue maschere numeriche sono pronte in:\")\n",
        "print(f\"   -> {TARGET_CLEAN_MASKS_DIR}\")\n",
        "print(\"Ora puoi usare questa cartella per il fine-tuning.\")"
      ],
      "metadata": {
        "id": "5bcmT4GGU9p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELLA 5.30: PREPARAZIONE DATI PER IL FINE-TUNING (robusta jpg/png)\n",
        "# =============================================================================\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"--- [FASE A] Preparazione dei Dati 'd'Oro' per il Fine-Tuning ---\")\n",
        "\n",
        "# --- 1. CONFIGURA I PERCORSI ---\n",
        "GOLDEN_IMG_DIR = Path(\"/content/drive/MyDrive/GoldenDataset/images\")\n",
        "GOLDEN_MASK_DIR = Path(\"/content/drive/MyDrive/GoldenDataset_Clean/masks\")\n",
        "FINETUNE_DATASET_DIR = Path(\"/content/finetune_dataset\")\n",
        "\n",
        "GOLDEN_IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "GOLDEN_MASK_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"ATTENZIONE: Assicurati di aver caricato le tue immagini e maschere corrette in:\\n  - {GOLDEN_IMG_DIR}\\n  - {GOLDEN_MASK_DIR}\")\n",
        "\n",
        "# --- 2. TROVA I CAMPIONI DALL'ELENCO MASCHERE (.png) ---\n",
        "all_golden_masks = sorted(list(GOLDEN_MASK_DIR.glob(\"*.png\")))\n",
        "golden_stems = [p.stem for p in all_golden_masks]\n",
        "\n",
        "if len(golden_stems) < 10:\n",
        "    print(f\"\\nAVVISO: Trovati solo {len(golden_stems)} campioni. User√≤ tutti per il training.\")\n",
        "    train_stems = golden_stems\n",
        "    val_stems = []\n",
        "else:\n",
        "    train_stems, val_stems = train_test_split(golden_stems, test_size=0.20, random_state=42)\n",
        "\n",
        "print(f\"\\nSplit eseguito: {len(train_stems)} train, {len(val_stems)} val.\")\n",
        "\n",
        "# --- 3. PREPARA STRUTTURA CARTELLE PULITA ---\n",
        "if FINETUNE_DATASET_DIR.exists():\n",
        "    shutil.rmtree(FINETUNE_DATASET_DIR)\n",
        "(FINETUNE_DATASET_DIR / \"train\" / \"images\").mkdir(parents=True)\n",
        "(FINETUNE_DATASET_DIR / \"train\" / \"masks\").mkdir(parents=True)\n",
        "(FINETUNE_DATASET_DIR / \"val\" / \"images\").mkdir(parents=True)\n",
        "(FINETUNE_DATASET_DIR / \"val\" / \"masks\").mkdir(parents=True)\n",
        "\n",
        "# --- 4. UTILS: copia rispettando estensioni (img .jpg o .png; mask .png) ---\n",
        "def _first_existing(base: Path, exts):\n",
        "    for ext in exts:\n",
        "        p = base.with_suffix(ext)\n",
        "        if p.exists():\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def copy_golden_files(stems, subset):\n",
        "    if not stems:\n",
        "        return 0, 0, 0\n",
        "    missing_img, missing_mask, copied = 0, 0, 0\n",
        "    for stem in stems:\n",
        "        # immagine: prova .jpg poi .png\n",
        "        img_path = _first_existing(GOLDEN_IMG_DIR / stem, [\".jpg\", \".png\", \".jpeg\", \".JPG\", \".PNG\", \".JPEG\"])\n",
        "        if img_path is None:\n",
        "            print(f\"[WARN] Immagine mancante per '{stem}' (attese .jpg/.png). Salto.\")\n",
        "            missing_img += 1\n",
        "            continue\n",
        "\n",
        "        # maschera: deve essere .png\n",
        "        mask_path = GOLDEN_MASK_DIR / f\"{stem}.png\"\n",
        "        if not mask_path.exists():\n",
        "            print(f\"[WARN] Maschera .png mancante per '{stem}'. Salto.\")\n",
        "            missing_mask += 1\n",
        "            continue\n",
        "\n",
        "        # destinazioni (mantieni estensione originale immagine)\n",
        "        dst_img = FINETUNE_DATASET_DIR / subset / \"images\" / img_path.name\n",
        "        dst_mask = FINETUNE_DATASET_DIR / subset / \"masks\" / f\"{stem}.png\"\n",
        "\n",
        "        shutil.copy(img_path, dst_img)\n",
        "        shutil.copy(mask_path, dst_mask)\n",
        "        copied += 1\n",
        "    return copied, missing_img, missing_mask\n",
        "\n",
        "copied_tr, miss_img_tr, miss_m_tr = copy_golden_files(train_stems, \"train\")\n",
        "copied_va, miss_img_va, miss_m_va = copy_golden_files(val_stems, \"val\")\n",
        "\n",
        "print(f\"\\n‚úÖ Copiati: train={copied_tr}, val={copied_va}\")\n",
        "if miss_img_tr + miss_img_va > 0 or miss_m_tr + miss_m_va > 0:\n",
        "    print(f\"‚ö†Ô∏è Mancanti: immagini={miss_img_tr+miss_img_va}, maschere={miss_m_tr+miss_m_va}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Dati per il fine-tuning pronti in '{FINETUNE_DATASET_DIR}'\")\n"
      ],
      "metadata": {
        "id": "E1GUedJjKkyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "#@title CELLA 5.5: CONTROLLO SANIT√Ä MASCHERE A COLORI\n",
        "# =============================================================================\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"--- Controllo Integrit√† Maschere a Colori nel Finetune Dataset ---\")\n",
        "\n",
        "FINETUNE_DATASET_DIR = Path(\"/content/content/drive/MyDrive/GoldenDataset_Clean/masks\")\n",
        "mask_dirs_to_check = [\n",
        "    FINETUNE_DATASET_DIR / \"train\" / \"masks\",\n",
        "    FINETUNE_DATASET_DIR / \"val\" / \"masks\"\n",
        "]\n",
        "\n",
        "# Definiamo i colori RGB attesi da CVAT\n",
        "# NOTA: OpenCV legge in BGR, quindi li invertiamo per il confronto\n",
        "EXPECTED_COLORS_BGR = {\n",
        "    (0, 0, 0): 'background',\n",
        "    (220, 220, 0): 'damage',      # Ciano in BGR\n",
        "    (100, 100, 255): 'hole'       # Rosso/Rosa in BGR\n",
        "}\n",
        "EXPECTED_COLORS_BGR_TUPLES = list(EXPECTED_COLORS_BGR.keys())\n",
        "print(f\"Mi aspetto di trovare solo questi colori (in formato BGR): {EXPECTED_COLORS_BGR_TUPLES}\")\n",
        "\n",
        "errors_found = 0\n",
        "for mask_dir in mask_dirs_to_check:\n",
        "    if not mask_dir.exists(): continue\n",
        "    print(f\"\\nAnalizzo la cartella: {mask_dir}...\")\n",
        "\n",
        "    for mask_path in mask_dir.glob(\"*.png\"):\n",
        "        # Carica come immagine a colori\n",
        "        mask_color = cv2.imread(str(mask_path))\n",
        "\n",
        "        # Trova i colori unici\n",
        "        unique_colors = np.unique(mask_color.reshape(-1, mask_color.shape[2]), axis=0)\n",
        "\n",
        "        # Controlla se ci sono colori inattesi\n",
        "        unexpected_colors = []\n",
        "        for color in unique_colors:\n",
        "            if tuple(color) not in EXPECTED_COLORS_BGR_TUPLES:\n",
        "                unexpected_colors.append(color)\n",
        "\n",
        "        if unexpected_colors:\n",
        "            print(f\"  -> ‚ùå PROBLEMA: {mask_path.name} contiene colori INATTESI: {unexpected_colors}\")\n",
        "            errors_found += 1\n",
        "\n",
        "if errors_found == 0:\n",
        "    print(\"\\n‚úÖ OK! Tutte le maschere contengono i colori RGB attesi. Il problema √® confermato.\")\n",
        "else:\n",
        "    print(f\"\\n‚ùóÔ∏è ATTENZIONE: {errors_found} file contengono colori strani oltre a quelli di CVAT.\")"
      ],
      "metadata": {
        "id": "LhAQ9nBFS_vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "#@title INVESTIGATORE DI FILE MANCANTI\n",
        "# =============================================================================\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"--- [DIAGNOSI] Controllo corrispondenza tra immagini e maschere ---\")\n",
        "\n",
        "CLEAN_DATA_DIR = Path(\"/content/drive/MyDrive/GoldenDataset_Clean\")\n",
        "images_dir = CLEAN_DATA_DIR / \"images\"\n",
        "masks_dir = CLEAN_DATA_DIR / \"masks\"\n",
        "\n",
        "mask_stems = [p.stem for p in masks_dir.glob(\"*.png\")]\n",
        "image_stems = [p.stem for p in images_dir.glob(\"*.*\")] # Prende sia .png che .jpg\n",
        "\n",
        "missing_images = []\n",
        "for m_stem in mask_stems:\n",
        "    if m_stem not in image_stems:\n",
        "        missing_images.append(m_stem)\n",
        "\n",
        "if not missing_images:\n",
        "    print(\"\\n‚úÖ OK! Tutte le maschere hanno un'immagine corrispondente.\")\n",
        "    print(\"   -> Il problema potrebbe essere un file corrotto.\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå ERRORE! Trovate {len(missing_images)} maschere senza un'immagine corrispondente:\")\n",
        "    for stem in missing_images:\n",
        "        print(f\"   -> Manca l'immagine per la maschera: {stem}.png\")\n",
        "\n",
        "print(\"\\n--- Azione richiesta ---\")\n",
        "print(\"Vai nella cartella '/content/drive/MyDrive/GoldenDataset_Clean/images' e assicurati che esistano le immagini per le maschere elencate sopra. Controlla il nome e l'estensione del file.\")"
      ],
      "metadata": {
        "id": "y4n7ZGWJXzqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avviamo un metodo di check zoom dei danni a ragnatela,ilmodello non riesce a prendere coraggio per segmentare il danno complesso, ritagliamo patch 128x128 che lo costringano a imparare mentre passano tutte dentro il train di fine tune insieme all'albumentation aggressivo"
      ],
      "metadata": {
        "id": "3EqvOVX-G98q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5r1wW3ghf5IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CREAZIONESELEZIONEPATCHES128"
      ],
      "metadata": {
        "id": "Pg3ScU8S7yiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELLA 6.1: GENERATORE DI PATCH v5.1 (\"THE BLACKOUT ALGORITHM - CORRETTO\")\n",
        "# =============================================================================\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"--- [FASE di Pre-processing] Generazione Patch con Metodo 'Blackout' ---\")\n",
        "\n",
        "# --- 1. CONFIGURAZIONE ---\n",
        "SOURCE_DATA_DIR = Path(\"/content/drive/MyDrive/GoldenDataset_Clean\")\n",
        "TEMP_SPLIT_DIR = Path(\"/content/temp_finetune_split\")\n",
        "PATCHES_OUTPUT_DIR = Path(\"/content/patches_dataset\")\n",
        "PATCH_SIZE = 128\n",
        "DESIRED_PATCHES_PER_IMAGE = 50\n",
        "BLACKOUT_MARGIN = 90\n",
        "\n",
        "if TEMP_SPLIT_DIR.exists(): shutil.rmtree(TEMP_SPLIT_DIR)\n",
        "if PATCHES_OUTPUT_DIR.exists(): shutil.rmtree(PATCHES_OUTPUT_DIR)\n",
        "\n",
        "# --- 2. SPLIT (Non cambia) ---\n",
        "all_stems = sorted([p.stem for p in (SOURCE_DATA_DIR / \"masks\").glob(\"*.png\")])\n",
        "train_stems, val_stems = train_test_split(all_stems, test_size=0.2, random_state=42)\n",
        "def copy_files_for_split(stems, subset):\n",
        "    (TEMP_SPLIT_DIR/subset/\"images\").mkdir(parents=True); (TEMP_SPLIT_DIR/subset/\"masks\").mkdir(parents=True)\n",
        "    for stem in stems:\n",
        "        img_path = next((SOURCE_DATA_DIR / \"images\").glob(f\"{stem}.*\")); shutil.copy(img_path, TEMP_SPLIT_DIR/subset/\"images\"/img_path.name)\n",
        "        shutil.copy(SOURCE_DATA_DIR/\"masks\"/f\"{stem}.png\", TEMP_SPLIT_DIR/subset/\"masks\"/f\"{stem}.png\")\n",
        "copy_files_for_split(train_stems, \"train\"); copy_files_for_split(val_stems, \"val\")\n",
        "print(f\"‚úÖ Split temporaneo creato.\")\n",
        "\n",
        "# --- 3. CICLO DI ESTRAZIONE \"BLACKOUT\" (LOGICA CORRETTA) ---\n",
        "for subset in [\"train\", \"val\"]:\n",
        "    print(f\"\\nProcesso la subset: {subset}...\")\n",
        "    source_img_dir = TEMP_SPLIT_DIR / subset / \"images\"; source_mask_dir = TEMP_SPLIT_DIR / subset / \"masks\"\n",
        "    target_img_dir = PATCHES_OUTPUT_DIR / subset / \"images\"; target_mask_dir = PATCHES_OUTPUT_DIR / subset / \"masks\"\n",
        "    target_img_dir.mkdir(parents=True); target_mask_dir.mkdir(parents=True)\n",
        "    stems = [p.stem for p in source_mask_dir.glob(\"*.png\")]\n",
        "\n",
        "    for stem in tqdm(stems, desc=f\"Applicando blackout su '{subset}'\"):\n",
        "        img_path = next(source_img_dir.glob(f\"{stem}.*\")); mask_path = source_mask_dir / f\"{stem}.png\"\n",
        "        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE); mask = cv2.imread(str(mask_path), cv2.IMREAD_UNCHANGED)\n",
        "        if img is None: continue\n",
        "\n",
        "        #inversione valori maschere perch√® Cvat ha sballato tutto\n",
        "\n",
        "        hole_mask = np.uint8(mask == 2)\n",
        "\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (BLACKOUT_MARGIN*2, BLACKOUT_MARGIN*2)) # Kernel circolare √® meglio\n",
        "        blackout_zone = cv2.dilate(hole_mask, kernel, iterations=1)\n",
        "\n",
        "        damage_mask = np.uint8(mask == 1)\n",
        "\n",
        "        # === LA CORREZIONE ===\n",
        "        # Cancella fisicamente i pixel di danno che cadono nella zona di blackout.\n",
        "        surviving_damage_mask = damage_mask.copy()\n",
        "        surviving_damage_mask[blackout_zone > 0] = 0\n",
        "\n",
        "        surviving_points_yx = np.argwhere(surviving_damage_mask > 0)\n",
        "\n",
        "        if len(surviving_points_yx) == 0: continue\n",
        "\n",
        "        num_to_sample = min(DESIRED_PATCHES_PER_IMAGE, len(surviving_points_yx))\n",
        "        sampled_indices = np.random.choice(len(surviving_points_yx), size=num_to_sample, replace=False)\n",
        "\n",
        "        for i, idx in enumerate(sampled_indices):\n",
        "            y, x = surviving_points_yx[idx]\n",
        "            # ... (il resto del codice da qui in poi √® GIUSTO e non cambia) ...\n",
        "            y_min = max(0, y - PATCH_SIZE // 2); x_min = max(0, x - PATCH_SIZE // 2)\n",
        "            y_max = y_min + PATCH_SIZE; x_max = x_min + PATCH_SIZE\n",
        "\n",
        "            if y_max > img.shape[0] or x_max > img.shape[1]: continue\n",
        "\n",
        "            img_patch = img[y_min:y_max, x_min:x_max]\n",
        "            mask_patch = mask[y_min:y_max, x_min:x_max]\n",
        "\n",
        "            if img_patch.shape == (PATCH_SIZE, PATCH_SIZE):\n",
        "                cv2.imwrite(str(target_img_dir / f\"{stem}_patch_{i}.png\"), img_patch)\n",
        "                cv2.imwrite(str(target_mask_dir / f\"{stem}_patch_{i}.png\"), mask_patch)\n",
        "\n",
        "print(f\"\\n‚úÖ GENERAZIONE PATCH COMPLETATA.\")"
      ],
      "metadata": {
        "id": "XnVLpBwYG81t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Post selezione eliminando celle con foro bianco; vogliamo solo patches con danno ragnatela, sappiamo che il foro ha un valore di intensit√† elelvato si user√† come scremaggio\n",
        "#Valori di pixel unici nel campione del foro: [255]\n",
        "#Valore di soglia per 'bianco accecante' identificato: 255\n",
        "\n",
        "# usiamo 250 per scremare di pi√π e essere sicuri\n",
        "# =============================================================================\n",
        "# CELLA 6.1.5: SCREMATURA AUTOMATICA PATCH TRAMITE ANALISI IMMAGINE\n",
        "# =============================================================================\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "print(\"--- [FASE di CURA AUTOMATICA] Scrematura delle patch contaminate dal foro ---\")\n",
        "\n",
        "# --- 1. PARAMETRI DI FILTRAGGIO ---\n",
        "SOURCE_PATCHES_DIR = Path(\"/content/patches_dataset\")\n",
        "\n",
        "# Soglia di luminosit√† per considerare un pixel come \"foro\".\n",
        "# Basato sull'analisi precedente, usiamo un valore leggermente inferiore a 255.\n",
        "BRIGHTNESS_THRESHOLD = 250\n",
        "\n",
        "# Percentuale massima di pixel \"foro\" consentita in una patch prima di essere scartata.\n",
        "# L'1% √® un buon punto di partenza: tollera piccoli artefatti ma scarta pezzi di foro.\n",
        "MAX_HOLE_PIXEL_PERCENTAGE = 0.09\n",
        "\n",
        "# --- 2. LOGICA DI SCANSIONE E FILTRAGGIO ---\n",
        "patches_to_delete = []\n",
        "\n",
        "for subset in [\"train\", \"val\"]:\n",
        "    img_dir = SOURCE_PATCHES_DIR / subset / \"images\"\n",
        "    if not img_dir.exists():\n",
        "        print(f\"La cartella {img_dir} non esiste. Salto.\")\n",
        "        continue\n",
        "\n",
        "    for img_path in tqdm(list(img_dir.glob(\"*.png\")), desc=f\"Scansione '{subset}' images\"):\n",
        "        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None: continue\n",
        "\n",
        "        # Conta i pixel che superano la soglia di luminosit√†\n",
        "        bright_pixel_count = np.count_nonzero(img > BRIGHTNESS_THRESHOLD)\n",
        "        total_pixels = img.size\n",
        "        hole_percentage = (bright_pixel_count / total_pixels) * 100\n",
        "\n",
        "        # Se la percentuale supera il limite, marca per l'eliminazione\n",
        "        if hole_percentage > MAX_HOLE_PIXEL_PERCENTAGE:\n",
        "            mask_path = img_path.parent.parent / \"masks\" / img_path.name\n",
        "            patches_to_delete.append(img_path)\n",
        "            patches_to_delete.append(mask_path)\n",
        "\n",
        "# --- 3. REPORT E AZIONE ---\n",
        "num_bad_patches = len(patches_to_delete) // 2\n",
        "print(f\"\\n--- REPORT ---\")\n",
        "print(f\"Identificate {num_bad_patches} patch da eliminare.\")\n",
        "\n",
        "if num_bad_patches > 0:\n",
        "    print(\"Primi 10 file marcati per l'eliminazione:\")\n",
        "    for f in patches_to_delete[:10]: print(f\"  - {f}\")\n",
        "\n",
        "    # === ESECUZIONE DELL'ELIMINAZIONE (ATTENZIONE: IRREVERSIBILE) ===\n",
        "    # Rimuovi il commento dalla riga sottostante per eliminare i file\n",
        "\n",
        "    for file_path in tqdm(patches_to_delete, desc=\"Eliminazione file\"):\n",
        "         if file_path.exists():\n",
        "            os.remove(file_path)\n",
        "            print(\"eliminazione patches con foro pixels bianchi\")\n",
        "\n",
        "    print(\"\\nPer eseguire l'eliminazione, decommenta le righe nella sezione 'ESECUZIONE DELL'ELIMINAZIONE'.\")\n",
        "else:\n",
        "    print(\"Nessuna patch ha superato la soglia di contaminazione. Il dataset √® pulito.\")\n"
      ],
      "metadata": {
        "id": "c_Qvf5kD2-4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELLA 6.1.7: IL CURATORE UMANO E COLLETTORE DI \"GEMME\"\n",
        "# =============================================================================\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "#Serve a selezionare a mano le patches e prendere patch con corrispondent maschera dalla seleione fatta\n",
        "#mettere il nome scelto nella lista per la selezione\n",
        "\n",
        "print(\"--- [FASE di CURA MANUALE] Seleziono e impacchetto le patch migliori ---\")\n",
        "\n",
        "# --- 1. CONFIGURAZIONE ---\n",
        "SOURCE_PATCHES_DIR = Path(\"/content/patches_dataset\")\n",
        "TEMP_CURATED_DIR = Path(\"/content/curated_patches_for_zip\") # Directory temporanea\n",
        "ZIP_OUTPUT_PATH = Path(\"/content/GoldenPatches_v1.zip\") # Il nome del tuo file ZIP finale\n",
        "\n",
        "if TEMP_CURATED_DIR.exists(): shutil.rmtree(TEMP_CURATED_DIR)\n",
        "\n",
        "# --- 2. LA TUA LISTA DELLA SPESA ---\n",
        "# Sfoglia le cartelle in /content/patches_dataset/train/images e /content/patches_dataset/val/images.\n",
        "# Quando trovi una patch che ti piace, copia il suo nome (SENZA .png) e incollalo qui.\n",
        "BEST_PATCH_STEMS = [\n",
        "    \"H546_h060_Scan5_patch_14\",\"H549_h063_Scan5_patch_26\",\"H551_h065_Scan5_patch_11\",\"H554_h068_Scan5_patch_0\",\"H554_h068_Scan5_patch_18\",\"H554_h068_Scan5_patch_4\",\n",
        "    \"H557_h071_Scan5_patch_0\",\"H557_h071_Scan5_patch_4\",\"H557_h071_Scan5_patch_5\",\"H560_h074_Scan5_patch_17\",\"H574_h002_Scan6_patch_11\",\"H574_h002_Scan6_patch_20\",\n",
        "    \"H574_h002_Scan6_patch_30\",\"H580_h008_Scan6_patch_8\",\"H583_h011_Scan6_patch_23\",\"H584_h012_Scan6_patch_1\",\"H585_h013_Scan6_patch_27\",\"H587_h015_Scan6_patch_10\",\n",
        "    \"H563_h077_Scan5_patch_45\",\"H565_h079_Scan5_patch_47\",\"H571_h085_Scan5_patch_0\",\"H571_h085_Scan5_patch_1\",\"H571_h085_Scan5_patch_10\",\n",
        "    \"H592_h020_Scan6_patch_46\"\n",
        "]\n",
        "print(f\"Selezionati {len(BEST_PATCH_STEMS)} campioni d'oro per l'archiviazione.\")\n",
        "\n",
        "# --- 3. PESCA E COPIA I FILE SELEZIONATI ---\n",
        "target_img_dir = TEMP_CURATED_DIR / \"images\"\n",
        "target_mask_dir = TEMP_CURATED_DIR / \"masks\"\n",
        "target_img_dir.mkdir(parents=True); target_mask_dir.mkdir(parents=True)\n",
        "\n",
        "found_count = 0\n",
        "for stem in tqdm(BEST_PATCH_STEMS, desc=\"Recuperando i file\"):\n",
        "    found = False\n",
        "    for subset in [\"train\", \"val\"]:\n",
        "        source_img_path = SOURCE_PATCHES_DIR / subset / \"images\" / f\"{stem}.png\"\n",
        "        source_mask_path = SOURCE_PATCHES_DIR / subset / \"masks\" / f\"{stem}.png\"\n",
        "\n",
        "        if source_img_path.exists():\n",
        "            shutil.copy(source_img_path, target_img_dir / source_img_path.name)\n",
        "            shutil.copy(source_mask_path, target_mask_dir / source_mask_path.name)\n",
        "            found = True\n",
        "            break # Trovato, passa al prossimo stelo\n",
        "    if found:\n",
        "        found_count += 1\n",
        "\n",
        "print(f\"\\nRecuperati con successo {found_count} su {len(BEST_PATCH_STEMS)} file richiesti.\")\n",
        "\n",
        "# --- 4. CREA L'ARCHIVIO ZIP ---\n",
        "if found_count > 0:\n",
        "    shutil.make_archive(ZIP_OUTPUT_PATH.stem, 'zip', TEMP_CURATED_DIR)\n",
        "    print(f\"\\n‚úÖ ARCHIVIO CREATO: '{ZIP_OUTPUT_PATH}'\")\n",
        "    print(\"   Puoi scaricarlo dalla barra laterale o copiarlo su Google Drive con:\")\n",
        "    print(f'   !cp \"{ZIP_OUTPUT_PATH}\" \"/content/drive/MyDrive/GoldenPatches/\"')\n",
        "else:\n",
        "    print(\"\\nNessun file recuperato. Controlla i nomi nella lista.\")"
      ],
      "metadata": {
        "id": "9jhzQNfMM_qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##fineselezionecCELLEPATCHES128"
      ],
      "metadata": {
        "id": "YJfu375A7q98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ADDESTRAMENTOIBRIDOCELLE+PATCHES FOVEALE"
      ],
      "metadata": {
        "id": "Z1SOM-Hk74Jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELLA 6.2: LA CELLA UNICA E DEFINITIVA DI TRAINING IBRIDO (v2 - ROBUSTA)\n",
        "# =============================================================================\n",
        "# --- [FASE 0] SETUP TOTALE ---\n",
        "print(\"--- [FASE 0] Setup e import completi ---\")\n",
        "import torch, cv2, shutil, random, numpy as np, zipfile\n",
        "import segmentation_models_pytorch as smp\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- [FASE 1] CONFIGURAZIONE E PREPARAZIONE DATI ---\n",
        "print(\"\\n--- [FASE 1] Configurazione e preparazione dati ---\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# <<<<<<< PARAMETRI DELLO SMOKETEST / RUN FINALE >>>>>>>>>\n",
        "FINETUNE_EPOCHS = 20 # Inizia con 2 per lo smoketest, poi aumenta a 20\n",
        "FINETUNE_LR = 1e-5; FINETUNE_BATCH_SIZE = 4\n",
        "ENCODER_NAME = \"efficientnet-b0\"\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/PesiUNETPP\")\n",
        "PRE_TRAINED_CHECKPOINT = CKPT_DIR / \"best_model_pre-training.pth\"\n",
        "FINETUNED_CHECKPOINT_GDRIVE = CKPT_DIR / \"best_model_finetuned.pth\"\n",
        "FINETUNED_CHECKPOINT_LOCAL  = Path(\"/content/best_model_finetuned_local.pth\")\n",
        "SOURCE_FULL_DATA_DIR = Path(\"/content/drive/MyDrive/GoldenDataset_Clean\")\n",
        "GOLDEN_PATCH_ZIP = Path(\"/content/drive/MyDrive/GoldenPatches/GoldenPatches_v1.zip\")\n",
        "FINETUNE_DATASET_DIR = Path(\"/content/finetune_full_images_split\")\n",
        "CURATED_PATCHES_DIR = Path(\"/content/curated_patches_unzipped\")\n",
        "\n",
        "if FINETUNE_DATASET_DIR.exists(): shutil.rmtree(FINETUNE_DATASET_DIR)\n",
        "if CURATED_PATCHES_DIR.exists(): shutil.rmtree(CURATED_PATCHES_DIR)\n",
        "with zipfile.ZipFile(GOLDEN_PATCH_ZIP, 'r') as zf: zf.extractall(CURATED_PATCHES_DIR)\n",
        "print(f\"‚úÖ Patch d'oro estratte in '{CURATED_PATCHES_DIR}'\")\n",
        "\n",
        "all_full_stems = sorted([p.stem for p in (SOURCE_FULL_DATA_DIR / \"masks\").glob(\"*.png\")])\n",
        "train_stems, val_stems = train_test_split(all_full_stems, test_size=0.2, random_state=42)\n",
        "def copy_files_for_split(stems, subset):\n",
        "    # <<< FIX JPG/PNG: Trova l'immagine con qualsiasi estensione usando glob e next\n",
        "    (FINETUNE_DATASET_DIR/subset/\"images\").mkdir(parents=True); (FINETUNE_DATASET_DIR/subset/\"masks\").mkdir(parents=True)\n",
        "    for stem in stems:\n",
        "        try:\n",
        "            img_path = next((SOURCE_FULL_DATA_DIR / \"images\").glob(f\"{stem}.*\"))\n",
        "            shutil.copy(img_path, FINETUNE_DATASET_DIR/subset/\"images\"/img_path.name)\n",
        "            shutil.copy(SOURCE_FULL_DATA_DIR/\"masks\"/f\"{stem}.png\", FINETUNE_DATASET_DIR/subset/\"masks\"/f\"{stem}.png\")\n",
        "        except StopIteration:\n",
        "            print(f\"ATTENZIONE: Immagine non trovata per la maschera '{stem}.png'. Viene saltata.\")\n",
        "copy_files_for_split(train_stems, \"train\"); copy_files_for_split(val_stems, \"val\")\n",
        "print(f\"‚úÖ Immagini intere splittate: {len(train_stems)} train, {len(val_stems)} val.\")\n",
        "\n",
        "# --- [FASE 2] DATASET IBRIDO, MODELLO E TRAINING ---\n",
        "print(\"\\n--- [FASE 2] Avvio ciclo di fine-tuning IBRIDO ---\")\n",
        "class HybridDataset(Dataset):\n",
        "    def __init__(self, full_img_dir, full_mask_dir, patch_img_dir, patch_mask_dir, transform):\n",
        "        self.transform = transform\n",
        "        self.full_mask_paths = sorted(list(full_mask_dir.glob(\"*.png\")))\n",
        "        self.patch_mask_paths = sorted(list(patch_mask_dir.glob(\"*.png\")))\n",
        "        # <<< FIX JPG/PNG: Usa la lista di maschere per trovare dinamicamente le immagini\n",
        "        self.full_img_paths = [next(full_img_dir.glob(f\"{p.stem}.*\")) for p in self.full_mask_paths]\n",
        "        self.patch_img_paths = [next(patch_img_dir.glob(f\"{p.stem}.*\")) for p in self.patch_mask_paths]\n",
        "        self.total_size = len(self.full_img_paths) + len(self.patch_img_paths)\n",
        "        print(f\"Dataset ibrido caricato: {len(self.full_img_paths)} immagini intere + {len(self.patch_img_paths)} patch.\")\n",
        "    def __len__(self): return self.total_size\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < len(self.full_img_paths): img_path, mask_path = self.full_img_paths[idx], self.full_mask_paths[idx]\n",
        "        else: patch_idx = idx - len(self.full_img_paths); img_path, mask_path = self.patch_img_paths[patch_idx], self.patch_mask_paths[patch_idx]\n",
        "        image = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE); mask = cv2.imread(str(mask_path), cv2.IMREAD_UNCHANGED)\n",
        "        augmented = self.transform(image=image, mask=mask); return augmented[\"image\"], augmented[\"mask\"].long()\n",
        "\n",
        "transform = A.Compose([A.Resize(512, 512), A.HorizontalFlip(), A.VerticalFlip(), A.Normalize(mean=(0.5,), std=(0.5,)), ToTensorV2()])\n",
        "train_ds = HybridDataset(FINETUNE_DATASET_DIR/\"train\"/\"images\", FINETUNE_DATASET_DIR/\"train\"/\"masks\", CURATED_PATCHES_DIR/\"images\", CURATED_PATCHES_DIR/\"masks\", transform)\n",
        "val_ds = HybridDataset(FINETUNE_DATASET_DIR/\"val\"/\"images\", FINETUNE_DATASET_DIR/\"val\"/\"masks\", CURATED_PATCHES_DIR/\"images\", CURATED_PATCHES_DIR/\"masks\", transform)\n",
        "train_loader = DataLoader(train_ds, batch_size=FINETUNE_BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=FINETUNE_BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "model = smp.UnetPlusPlus(encoder_name=ENCODER_NAME, in_channels=1, classes=3).to(DEVICE)\n",
        "model_to_load = FINETUNED_CHECKPOINT_GDRIVE if FINETUNED_CHECKPOINT_GDRIVE.exists() else PRE_TRAINED_CHECKPOINT\n",
        "print(f\"Caricamento pesi da: {model_to_load}\"); model.load_state_dict(torch.load(model_to_load, map_location=DEVICE))\n",
        "for p in model.parameters(): p.requires_grad = True\n",
        "print(\"‚úÖ Modello sbloccato.\"); loss_fn = smp.losses.DiceLoss(mode='multiclass', from_logits=True); optimizer = torch.optim.Adam(model.parameters(), lr=FINETUNE_LR)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\")); best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(FINETUNE_EPOCHS):\n",
        "    model.train()\n",
        "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1} Train\"):\n",
        "        images, masks = images.to(DEVICE), masks.to(DEVICE); optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")): outputs = model(images); loss = loss_fn(outputs, masks)\n",
        "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
        "    model.eval(); val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1} Val\"):\n",
        "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "            with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")): val_loss += loss_fn(model(images), masks).item()\n",
        "    avg_val_loss = val_loss / max(1, len(val_loader)); print(f\"Fine-Tuning Epoch {epoch+1}: Val Loss: {avg_val_loss:.4f}\")\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        print(f\"üèÜ Salvo (Val Loss: {avg_val_loss:.4f})...\"); best_val_loss = avg_val_loss; torch.save(model.state_dict(), FINETUNED_CHECKPOINT_GDRIVE); torch.save(model.state_dict(), FINETUNED_CHECKPOINT_LOCAL)\n",
        "\n",
        "print(\"\\nüéâ Fine-Tuning Ibrido Terminato.\")"
      ],
      "metadata": {
        "id": "q6XbAY-iHrb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3Okq3hAd_Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELLA 7: ISPEZIONE VISIVA COMPARATIVA (AUTOCONTENUTA E CORRETTA)\n",
        "# =============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import segmentation_models_pytorch as smp\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "\n",
        "print(\"--- [FASE 3] Ispezione Visiva Comparativa ---\")\n",
        "\n",
        "# --- 1. DEFINIZIONE DELLA CLASSE DATASET (NECESSARIA IN UN RUNTIME PULITO) ---\n",
        "# Questa √® la classe semplice per caricare le immagini INTERE per la valutazione\n",
        "class CleanDataset(Dataset):\n",
        "    def __init__(self, subset_dir, transform=None):\n",
        "        self.img_dir = subset_dir / \"images\"\n",
        "        self.mask_dir = subset_dir / \"masks\"\n",
        "        self.transform = transform\n",
        "        self.stems = sorted([p.stem for p in self.mask_dir.glob(\"*.png\")])\n",
        "    def __len__(self): return len(self.stems)\n",
        "    def __getitem__(self, idx):\n",
        "        stem = self.stems[idx]\n",
        "        try:\n",
        "            img_path = next((self.img_dir).glob(f\"{stem}.*\")) # Robusto per JPG/PNG\n",
        "        except StopIteration:\n",
        "            raise FileNotFoundError(f\"Immagine non trovata per la maschera: {stem}.png in {self.img_dir}\")\n",
        "        mask_path = self.mask_dir / f\"{stem}.png\"\n",
        "        image = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.imread(str(mask_path), cv2.IMREAD_UNCHANGED)\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
        "        return image, mask.long()\n",
        "\n",
        "# --- 2. CONFIGURAZIONE DEI MODELLI DA CONFRONTARE ---\n",
        "# Usiamo i percorsi esatti che hai fornito.\n",
        "MODEL_BEFORE_PATH = \"/content/drive/MyDrive/PesiUNETPP/best_model_pre-training.pth\"  # O il finetuned precedente, se preferisci\n",
        "MODEL_AFTER_PATH = \"/content/drive/MyDrive/PesiUNETPP/best_model_finetuned.pth\"    # Il modello appena addestrato\n",
        "\n",
        "# Ricrea le variabili necessarie se il runtime √® stato riavviato\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ENCODER_NAME = \"efficientnet-b0\"\n",
        "FINETUNE_DATASET_DIR = Path(\"/content/finetune_full_images_split\") # La cartella con le immagini intere splittate\n",
        "transform = A.Compose([A.Resize(512, 512), A.Normalize(mean=(0.5,), std=(0.5,)), ToTensorV2()])\n",
        "\n",
        "\n",
        "# --- 3. CARICAMENTO MODELLI ---\n",
        "model_before = smp.UnetPlusPlus(encoder_name=ENCODER_NAME, in_channels=1, classes=3).to(DEVICE)\n",
        "model_after = smp.UnetPlusPlus(encoder_name=ENCODER_NAME, in_channels=1, classes=3).to(DEVICE)\n",
        "try:\n",
        "    model_before.load_state_dict(torch.load(MODEL_BEFORE_PATH, map_location=DEVICE))\n",
        "    model_after.load_state_dict(torch.load(MODEL_AFTER_PATH, map_location=DEVICE))\n",
        "    model_before.eval(); model_after.eval()\n",
        "    print(\"‚úÖ Entrambi i modelli caricati con successo.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERRORE NEL CARICAMENTO DI UN MODELLO: {e}\")\n",
        "    raise e\n",
        "\n",
        "# --- 4. CICLO DI VISUALIZZAZIONE ---\n",
        "val_dataset_full = CleanDataset(FINETUNE_DATASET_DIR / \"val\", transform=transform)\n",
        "print(f\"Visualizzazione di {len(val_dataset_full)} campioni dal set di validazione...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(len(val_dataset_full)):\n",
        "        image_tensor, mask_gt_tensor = val_dataset_full[i]\n",
        "        input_tensor = image_tensor.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        # Inferenza con entrambi i modelli\n",
        "        pred_before = torch.argmax(model_before(input_tensor).squeeze(), dim=0).cpu().numpy()\n",
        "        pred_after = torch.argmax(model_after(input_tensor).squeeze(), dim=0).cpu().numpy()\n",
        "\n",
        "        image_vis = image_tensor.permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
        "        stem = val_dataset_full.stems[i]\n",
        "\n",
        "        fig, axes = plt.subplots(1, 4, figsize=(28, 7))\n",
        "        fig.suptitle(f'Confronto per: {stem}', fontsize=16)\n",
        "        axes[0].imshow(image_vis, cmap='gray'); axes[0].set_title(\"Immagine Originale\")\n",
        "        axes[1].imshow(mask_gt_tensor, cmap='jet', vmin=0, vmax=2); axes[1].set_title(\"Ground Truth\")\n",
        "        axes[2].imshow(pred_before, cmap='jet', vmin=0, vmax=2); axes[2].set_title(\"PRE-IBRIDO\")\n",
        "        axes[3].imshow(pred_after, cmap='jet', vmin=0, vmax=2); axes[3].set_title(\"POST-IBRIDO (2 epoche)\")\n",
        "        for ax in axes: ax.axis('off')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "Fy3G84HNWP_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELLA DI CHECK:10 INFERENZA + METRICHE + VISUALIZZAZIONE (10 sample random)\n",
        "# =============================================================================\n",
        "import random, torch, numpy as np, cv2, matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "from pathlib import Path\n",
        "\n",
        "# --- CONFIGURAZIONE ---\n",
        "SAMPLE_N      = 10\n",
        "IMG_DIR       = FINETUNE_DATASET_DIR / \"val\" / \"images\"   # Cambia se vuoi\n",
        "MASK_DIR      = FINETUNE_DATASET_DIR / \"val\" / \"masks\"    # idem; pu√≤ non esistere\n",
        "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "CLASSES       = 3                                         # == model output channels\n",
        "\n",
        "# --- TRANSFORM SOLO RESIZE+NORM (niente flip) ---\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "val_tf = A.Compose([\n",
        "    A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n",
        "    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# --- CAMPIONA FILE ---\n",
        "all_images = sorted(list(IMG_DIR.glob(\"*.*\")))\n",
        "sampled    = random.sample(all_images, min(SAMPLE_N, len(all_images)))\n",
        "\n",
        "# --- FUNZIONI METRICHE (smp.metrics) ---\n",
        "def _metrics(pred, gt):\n",
        "    # pred, gt: torch tensors [H,W] long\n",
        "    tp, fp, fn, tn = smp.metrics.get_stats(pred.unsqueeze(0), gt.unsqueeze(0), mode='multiclass', num_classes=CLASSES)\n",
        "    iou  = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
        "    dice = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
        "    return iou.item(), dice.item()\n",
        "\n",
        "# --- MODELL0 ---\n",
        "model = smp.UnetPlusPlus(encoder_name=\"efficientnet-b0\", in_channels=1, classes=CLASSES)\n",
        "model.load_state_dict(torch.load(FINETUNED_CHECKPOINT_GDRIVE, map_location=DEVICE))\n",
        "model.eval().to(DEVICE)\n",
        "\n",
        "ious, dices = [], []\n",
        "\n",
        "# --- VISUAL SETUP ---\n",
        "ncols = 3\n",
        "nrows = len(sampled)          # NON la divisione per 3\n",
        "plt.figure(figsize=(4*ncols, 4*nrows))\n",
        "\n",
        "for idx, img_path in enumerate(sampled):\n",
        "    stem = img_path.stem\n",
        "    mask_path = MASK_DIR / f\"{stem}.png\"\n",
        "\n",
        "    # -- LOAD & TF --\n",
        "    img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
        "    tf_out = val_tf(image=img, mask=np.zeros_like(img))\n",
        "    tensor_img = tf_out['image'].unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    # -- INFERENCE --\n",
        "    with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\", enabled=(DEVICE==\"cuda\")):\n",
        "        logits = model(tensor_img)\n",
        "    pred = logits.argmax(dim=1).squeeze().cpu()        # [H,W] long\n",
        "\n",
        "    # -- METRICS (se mask disponibile) --\n",
        "    if mask_path.exists():\n",
        "        gt = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
        "        gt_tf = val_tf(image=gt, mask=gt)\n",
        "        gt_t  = torch.as_tensor(gt_tf['mask'], dtype=torch.long)\n",
        "        iou, dice = _metrics(pred, gt_t)\n",
        "        ious.append(iou); dices.append(dice)\n",
        "        metric_str = f'IoU={iou:.3f}  Dice={dice:.3f}'\n",
        "    else:\n",
        "        metric_str = 'GT assente'\n",
        "\n",
        "    # -- PLOT: original | GT | pred\n",
        "    plt.subplot(nrows, ncols, idx*3 + 1)\n",
        "    plt.imshow(img, cmap='gray'); plt.axis('off'); plt.title(f'{stem} : img')\n",
        "\n",
        "    plt.subplot(nrows, ncols, idx*3 + 2)\n",
        "    if mask_path.exists():\n",
        "        plt.imshow(gt, cmap='viridis'); plt.axis('off'); plt.title('mask GT')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No GT', ha='center', va='center'); plt.axis('off')\n",
        "\n",
        "    plt.subplot(nrows, ncols, idx*3 + 3)\n",
        "    plt.imshow(pred.numpy(), cmap='viridis'); plt.axis('off'); plt.title(metric_str)\n",
        "\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# --- METRICHE AGGREGATE ---\n",
        "if ious:\n",
        "    print(f\"\\n=== METRICHE SU {len(ious)} IMMAGINI CON GT ===\")\n",
        "    print(f\"mIoU  : {np.mean(ious):.4f}\")\n",
        "    print(f\"mDice : {np.mean(dices):.4f}\")\n",
        "else:\n",
        "    print(\"\\nNessuna maschera ground-truth trovata: solo visualizzazione.\")"
      ],
      "metadata": {
        "id": "mW68SJkVYrTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ie_TC-TvYrXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WV-tRbKAF7O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELLA 7: LA PROVA DEFINITIVA - CONFRONTO VISIVO PRE vs POST TRAINING IBRIDO\n",
        "# =============================================================================\n",
        "# --- [FASE 0] SETUP COMPLETO E AUTOCONTENUTO ---\n",
        "print(\"--- [FASE 0] Setup completo ---\")\n",
        "import torch, cv2, shutil, numpy as np\n",
        "import segmentation_models_pytorch as smp\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# --- [FASE 1] CONFIGURAZIONE ---\n",
        "print(\"\\n--- [FASE 1] Configurazione del confronto ---\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ENCODER_NAME = \"efficientnet-b0\"\n",
        "\n",
        "# <<< I PERCORSI ESATTI CHE HAI FORNITO TU >>>\n",
        "MODEL_PRE_FINETUNE_PATH = \"/content/drive/MyDrive/PesiUNETPP/best_model_pre-training.pth\"\n",
        "MODEL_POST_FINETUNE_PATH = \"/content/drive/MyDrive/PesiUNETPP/best_model_finetuned.pth\" # Questo √® l'ultimo salvato\n",
        "\n",
        "# Le immagini INTERE \"d'oro\" su cui faremo il test\n",
        "SOURCE_FULL_DATA_DIR = Path(\"/content/drive/MyDrive/GoldenDataset_Clean\")\n",
        "FINETUNE_DATASET_DIR = Path(\"/content/finetune_full_images_split\") # Stessa cartella temporanea di prima\n",
        "\n",
        "# --- [FASE 2] DEFINIZIONI NECESSARIE (per essere autocontenuti) ---\n",
        "# Copiamo qui la classe Dataset e la funzione di split per non avere NameError\n",
        "class CleanDataset(Dataset):\n",
        "    def __init__(self, subset_dir, transform=None):\n",
        "        self.img_dir = subset_dir / \"images\"; self.mask_dir = subset_dir / \"masks\"\n",
        "        self.transform = transform\n",
        "        self.mask_paths = sorted(list(self.mask_dir.glob(\"*.png\")))\n",
        "        self.stems = [p.stem for p in self.mask_paths]\n",
        "        # Trova dinamicamente le immagini .jpg o .png\n",
        "        self.img_paths = [next(self.img_dir.glob(f\"{s}.*\")) for s in self.stems]\n",
        "\n",
        "    def __len__(self): return len(self.stems)\n",
        "    def __getitem__(self, idx):\n",
        "        image = cv2.imread(str(self.img_paths[idx]), cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.imread(str(self.mask_paths[idx]), cv2.IMREAD_UNCHANGED)\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
        "        return image, mask.long()\n",
        "\n",
        "def copy_files_for_split(stems, subset, source_dir, target_dir):\n",
        "    (target_dir/subset/\"images\").mkdir(parents=True, exist_ok=True); (target_dir/subset/\"masks\").mkdir(parents=True, exist_ok=True)\n",
        "    for stem in stems:\n",
        "        try:\n",
        "            img_path = next((source_dir / \"images\").glob(f\"{stem}.*\"))\n",
        "            shutil.copy(img_path, target_dir/subset/\"images\"/img_path.name)\n",
        "            shutil.copy(source_dir/\"masks\"/f\"{stem}.png\", target_dir/subset/\"masks\"/f\"{stem}.png\")\n",
        "        except StopIteration:\n",
        "            print(f\"ATTENZIONE: Immagine non trovata per la maschera '{stem}.png'. Viene saltata.\")\n",
        "\n",
        "# --- [FASE 3] PREPARAZIONE DATI E MODELLI ---\n",
        "print(\"\\n--- [FASE 3] Preparazione dati e caricamento modelli ---\")\n",
        "# Prepara il validation set su cui fare il confronto\n",
        "all_stems = sorted([p.stem for p in (SOURCE_FULL_DATA_DIR / \"masks\").glob(\"*.png\")])\n",
        "_, val_stems = train_test_split(all_stems, test_size=0.2, random_state=42)\n",
        "copy_files_for_split(val_stems, \"val\", SOURCE_FULL_DATA_DIR, FINETUNE_DATASET_DIR)\n",
        "print(f\"Creato validation set con {len(val_stems)} immagini.\")\n",
        "\n",
        "transform = A.Compose([A.Resize(512, 512), A.Normalize(mean=(0.5,), std=(0.5,)), ToTensorV2()])\n",
        "val_dataset_full = CleanDataset(FINETUNE_DATASET_DIR / \"val\", transform=transform)\n",
        "\n",
        "# Carica entrambi i modelli\n",
        "model_before = smp.UnetPlusPlus(encoder_name=ENCODER_NAME, in_channels=1, classes=3).to(DEVICE)\n",
        "model_after = smp.UnetPlusPlus(encoder_name=ENCODER_NAME, in_channels=1, classes=3).to(DEVICE)\n",
        "model_before.load_state_dict(torch.load(MODEL_PRE_FINETUNE_PATH, map_location=DEVICE)); model_before.eval()\n",
        "model_after.load_state_dict(torch.load(MODEL_POST_FINETUNE_PATH, map_location=DEVICE)); model_after.eval()\n",
        "print(\"‚úÖ Entrambi i modelli (PRE e POST fine-tuning) caricati.\")\n",
        "\n",
        "# --- [FASE 4] ESECUZIONE DEL CONFRONTO VISIVO ---\n",
        "print(\"\\n--- [FASE 4] Generazione del confronto visivo ---\")\n",
        "with torch.no_grad():\n",
        "    for i in range(len(val_dataset_full)):\n",
        "        image_tensor, mask_gt_tensor = val_dataset_full[i]\n",
        "        input_tensor = image_tensor.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        output_before = model_before(input_tensor); pred_before = torch.argmax(output_before.squeeze(), dim=0).cpu().numpy()\n",
        "        output_after = model_after(input_tensor); pred_after = torch.argmax(output_after.squeeze(), dim=0).cpu().numpy()\n",
        "\n",
        "        image_vis = image_tensor.permute(1, 2, 0).numpy() * 0.5 + 0.5; stem = val_dataset_full.stems[i]\n",
        "\n",
        "        fig, axes = plt.subplots(1, 4, figsize=(28, 7))\n",
        "        fig.suptitle(f'Confronto per: {stem}', fontsize=16)\n",
        "        axes[0].imshow(image_vis, cmap='gray'); axes[0].set_title(\"Immagine Originale\")\n",
        "        axes[1].imshow(mask_gt_tensor, cmap='jet', vmin=0, vmax=2); axes[1].set_title(\"Ground Truth\")\n",
        "        axes[2].imshow(pred_before, cmap='jet', vmin=0, vmax=2); axes[2].set_title(\"PRE-TRAINING\")\n",
        "        axes[3].imshow(pred_after, cmap='jet', vmin=0, vmax=2); axes[3].set_title(\"POST-IBRIDO\")\n",
        "        for ax in axes: ax.axis('off')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "KeI_NqZeFWZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =============================================================================\n",
        "# CELLA A: SINCRONIZZAZIONE AUTOMATICA DA CVAT\n",
        "# =============================================================================\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"--- Sincronizzazione Annotazioni da CVAT ---\")\n",
        "\n",
        "# --- 1. CONFIGURAZIONE ---\n",
        "CVAT_TASK_ID = \"YOUR_TASK_ID\"  # <<< METTI QUI L'ID NUMERICO DEL TUO TASK CVAT\n",
        "ANNOTATIONS_ZIP_PATH = Path(f\"/content/cvat_annotations_task_{CVAT_TASK_ID}.zip\")\n",
        "EXPORT_FORMAT = \"Segmentation Mask 1.1\"\n",
        "\n",
        "# La cartella di GDrive dove le maschere corrette verranno copiate\n",
        "GOLDEN_MASK_DIR_GDRIVE = Path(\"/content/drive/MyDrive/GoldenDataset/masks\")\n",
        "GOLDEN_MASK_DIR_GDRIVE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- 2. INSTALLA E CONFIGURA CVAT-CLI (se non gi√† fatto) ---\n",
        "# !pip install -q cvat-cli\n",
        "# Se √® la prima volta, esegui questa riga e inserisci le credenziali:\n",
        "# !cvat-cli core --auth\n",
        "\n",
        "# --- 3. ESPORTA E SCARICA LE ANNOTAZIONI ---\n",
        "print(f\"Esportando e scaricando annotazioni dal task CVAT ID: {CVAT_TASK_ID}...\")\n",
        "# Il comando 'dump' esporta e scarica in un solo colpo\n",
        "os.system(f'cvat-cli tasks dump --format \"{EXPORT_FORMAT}\" {CVAT_TASK_ID} \"{ANNOTATIONS_ZIP_PATH}\"')\n",
        "\n",
        "if not ANNOTATIONS_ZIP_PATH.exists():\n",
        "    print(\"‚ùå DOWNLOAD FALLITO. Controlla il tuo TASK_ID e le credenziali CVAT.\")\n",
        "else:\n",
        "    print(\"‚úÖ Annotazioni scaricate con successo.\")\n",
        "\n",
        "    # --- 4. ESTRAI E COPIA LE MASCHERE IN GDRIVE ---\n",
        "    temp_extract_dir = Path(\"/content/temp_cvat_extract\")\n",
        "    if temp_extract_dir.exists(): shutil.rmtree(temp_extract_dir)\n",
        "    \n",
        "    with zipfile.ZipFile(ANNOTATIONS_ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(temp_extract_dir)\n",
        "\n",
        "    # Il formato Segmentation Mask 1.1 crea questa sottocartella\n",
        "    source_masks_dir = temp_extract_dir / \"SegmentationClass\"\n",
        "    if source_masks_dir.exists():\n",
        "        new_masks_copied = 0\n",
        "        for mask_file in source_masks_dir.glob(\"*.png\"):\n",
        "            # Copia il file in GDrive\n",
        "            shutil.copy(mask_file, GOLDEN_MASK_DIR_GDRIVE / mask_file.name)\n",
        "            new_masks_copied += 1\n",
        "        print(f\"‚úÖ Copiate {new_masks_copied} nuove maschere in '{GOLDEN_MASK_DIR_GDRIVE}'\")\n",
        "    else:\n",
        "        print(\"‚ùå Cartella 'SegmentationClass' non trovata nello ZIP. Controlla il formato di esportazione.\")\n",
        "\n",
        "    # Pulisci i file temporanei\n",
        "    ANNOTATIONS_ZIP_PATH.unlink()\n",
        "    shutil.rmtree(temp_extract_dir)"
      ],
      "metadata": {
        "id": "RwKwVpeOLx5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1wfkzgGHLx2I"
      }
    }
  ]
}
