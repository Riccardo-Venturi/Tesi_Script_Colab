{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "9TXRxKblbKSu"
      ],
      "authorship_tag": "ABX9TyMmx8aVyARn+xmOvkaXNaIB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riccardo-Venturi/Tesi_Script_Colab/blob/main/Unet%2B%2BV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1QQFo77aFSf"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELLA 1: SETUP GLOBALE E MONTAGGIO DRIVE\n",
        "# =============================================================================\n",
        "print(\"--- [FASE 0] Setup Globale ---\")\n",
        "\n",
        "# --- 1. Installazioni ---\n",
        "!pip install -q segmentation-models-pytorch==0.3.3 albumentations==1.3.1 torchinfo==1.8.0\n",
        "\n",
        "# --- 2. Import Fondamentali ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import shutil\n",
        "import random\n",
        "import re\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELLA 2: CONFIGURAZIONE CENTRALE DEL PROGETTO\n",
        "# =============================================================================\n",
        "# Modifica i valori in questa cella per controllare l'intera pipeline.\n",
        "\n",
        "class Config:\n",
        "    # --- 1. Percorsi Principali su Google Drive ---\n",
        "    BASE_DRIVE_DIR = Path(\"/content/drive/MyDrive\")\n",
        "    CHECKPOINT_DIR = BASE_DRIVE_DIR / \"PesiUNETPP\"\n",
        "\n",
        "    # Dati per il Pre-Training (le ~600 maschere automatiche)\n",
        "    PRETRAIN_IMG_DIR = BASE_DRIVE_DIR / \"Patches_test/Radio_Patches_Normalized\"\n",
        "    PRETRAIN_MASK_DIR = BASE_DRIVE_DIR / \"Patches_test/Patches_masks\"\n",
        "\n",
        "    # Dati \"d'oro\" per il Fine-Tuning (le 50 maschere corrette a mano)\n",
        "    GOLDEN_DATA_DIR = BASE_DRIVE_DIR / \"GoldenDataset_Clean\"\n",
        "\n",
        "    # Archivio delle patch \"d'oro\" curate a mano\n",
        "    GOLDEN_PATCHES_ZIP_DIR = BASE_DRIVE_DIR / \"GoldenPatches\"\n",
        "    GOLDEN_PATCHES_ZIP_PATH = GOLDEN_PATCHES_ZIP_DIR / \"GoldenPatches_v1.zip\"\n",
        "\n",
        "    # --- 2. Percorsi di Lavoro Locali (su Colab) ---\n",
        "    # Questi di solito non vanno modificati.\n",
        "    PRETRAIN_SPLIT_DIR = Path(\"/content/dataset_radiografico_split\")\n",
        "    FINETUNE_SPLIT_DIR = Path(\"/content/finetune_full_images_split\")\n",
        "    PATCHES_RAW_DIR = Path(\"/content/patches_dataset_raw\") # Output del generatore di patch\n",
        "    PATCHES_CURATED_DIR = Path(\"/content/curated_patches_unzipped\") # Patch scompattate\n",
        "\n",
        "    # --- 3. Nomi dei File dei Pesi ---\n",
        "    PRETRAINED_MODEL_PATH = CHECKPOINT_DIR / \"best_model_pre-training.pth\"\n",
        "    FINETUNED_MODEL_PATH = CHECKPOINT_DIR / \"best_model_finetuned.pth\"\n",
        "\n",
        "    # --- 4. Parametri del Modello e Hyperparameters ---\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    ENCODER_NAME = \"efficientnet-b0\"\n",
        "    IMG_SIZE = 512\n",
        "    N_CLASSES = 3\n",
        "\n",
        "    # Parametri specifici per il PRE-TRAINING\n",
        "    PRETRAIN_EPOCHS = 25\n",
        "    PRETRAIN_LR = 1e-3\n",
        "    PRETRAIN_BATCH_SIZE = 8\n",
        "\n",
        "    # Parametri specifici per il FINE-TUNING\n",
        "    FINETUNE_EPOCHS = 20 # Aumentato a 20 per il run finale\n",
        "    FINETUNE_LR = 1e-5\n",
        "    FINETUNE_BATCH_SIZE = 4\n",
        "\n",
        "    # Parametri per la Generazione Patch (\"Blackout\" Algorithm)\n",
        "    PATCH_SIZE = 128\n",
        "    DESIRED_PATCHES_PER_IMAGE = 50 # Genera molti candidati\n",
        "    BLACKOUT_MARGIN = 90\n",
        "\n",
        "# --- Creazione automatica delle cartelle necessarie ---\n",
        "Config.CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "Config.GOLDEN_PATCHES_ZIP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"âœ… Configurazione caricata.\")\n",
        "print(f\"Dispositivo in uso: {Config.DEVICE}\")"
      ],
      "metadata": {
        "id": "ntuOuBh2aJv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "#@title CELLA 3: DEFINIZIONI CORE (CLASSI E FUNZIONI)\n",
        "# =============================================================================\n",
        "\n",
        "# --- 1. CLASSI DATASET ---\n",
        "\n",
        "class RadioDataset(Dataset):\n",
        "    \"\"\"Dataset generico per caricare immagini (jpg/png) e maschere (png) a 1 canale.\"\"\"\n",
        "    def __init__(self, subset_dir, transform=None):\n",
        "        self.img_dir = subset_dir / \"images\"\n",
        "        self.mask_dir = subset_dir / \"masks\"\n",
        "        self.transform = transform\n",
        "        self.mask_paths = sorted(list(self.mask_dir.glob(\"*.png\")))\n",
        "        self.stems = [p.stem for p in self.mask_paths]\n",
        "        try:\n",
        "            self.img_paths = [next(self.img_dir.glob(f\"{s}.*\")) for s in self.stems]\n",
        "        except StopIteration:\n",
        "            # Trova quale file sta causando l'errore per un debug piÃ¹ facile\n",
        "            for s in self.stems:\n",
        "                if not list(self.img_dir.glob(f\"{s}.*\")):\n",
        "                    raise FileNotFoundError(f\"Impossibile trovare img per maschera '{s}.png' in {self.img_dir}\")\n",
        "\n",
        "    def __len__(self): return len(self.stems)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = cv2.imread(str(self.img_paths[idx]), cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.imread(str(self.mask_paths[idx]), cv2.IMREAD_UNCHANGED)\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
        "        return image, mask.long()\n",
        "\n",
        "class HybridDataset(Dataset):\n",
        "    \"\"\"Dataset che carica un mix di immagini intere e patch curate.\"\"\"\n",
        "    def __init__(self, full_img_dir, full_mask_dir, patch_img_dir, patch_mask_dir, transform):\n",
        "        self.transform = transform\n",
        "        self.full_mask_paths = sorted(list(full_mask_dir.glob(\"*.png\")))\n",
        "        self.patch_mask_paths = sorted(list(patch_mask_dir.glob(\"*.png\")))\n",
        "        self.full_img_paths = [next(full_img_dir.glob(f\"{p.stem}.*\")) for p in self.full_mask_paths]\n",
        "        self.patch_img_paths = [next(patch_img_dir.glob(f\"{p.stem}.*\")) for p in self.patch_mask_paths]\n",
        "        self.total_size = len(self.full_img_paths) + len(self.patch_img_paths)\n",
        "        print(f\"Dataset ibrido: {len(self.full_img_paths)} immagini intere + {len(self.patch_img_paths)} patch.\")\n",
        "\n",
        "    def __len__(self): return self.total_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < len(self.full_img_paths):\n",
        "            img_path, mask_path = self.full_img_paths[idx], self.full_mask_paths[idx]\n",
        "        else:\n",
        "            patch_idx = idx - len(self.full_img_paths)\n",
        "            img_path, mask_path = self.patch_img_paths[patch_idx], self.patch_mask_paths[patch_idx]\n",
        "\n",
        "        image = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.imread(str(mask_path), cv2.IMREAD_UNCHANGED)\n",
        "        augmented = self.transform(image=image, mask=mask)\n",
        "        return augmented[\"image\"], augmented[\"mask\"].long()\n",
        "\n",
        "# --- 2. FUNZIONI HELPER ---\n",
        "\n",
        "def extract_hole_id_from_path(path):\n",
        "    match = re.search(r'H(\\d+)', path.stem)\n",
        "    return f\"H{match.group(1)}\" if match else None\n",
        "\n",
        "def copy_files_for_split(stems, subset, source_dir, target_dir):\n",
        "    \"\"\"Copia i file corrispondenti agli stems in una sottocartella train/val.\"\"\"\n",
        "    (target_dir/subset/\"images\").mkdir(parents=True, exist_ok=True)\n",
        "    (target_dir/subset/\"masks\").mkdir(parents=True, exist_ok=True)\n",
        "    for stem in stems:\n",
        "        try:\n",
        "            img_path = next((source_dir / \"images\").glob(f\"{stem}.*\"))\n",
        "            shutil.copy(img_path, target_dir/subset/\"images\"/img_path.name)\n",
        "            shutil.copy(source_dir/\"masks\"/f\"{stem}.png\", target_dir/subset/\"masks\"/f\"{stem}.png\")\n",
        "        except StopIteration:\n",
        "            print(f\"ATTENZIONE (split): Immagine non trovata per maschera '{stem}.png'. Saltata.\")\n",
        "\n",
        "print(\"âœ… Definizioni Core caricate (Dataset e Helpers).\")"
      ],
      "metadata": {
        "id": "JnyylLZNaSR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cella di creazione maschere 0,1,2\n",
        "# =============================================================================\n",
        "# WORKFLOW #0: UTILITY - IL PURIFICATORE DI MASCHERE DA CVAT\n",
        "#\n",
        "# SCOPO: Prendere le maschere a colori esportate da CVAT, convertirle nel formato\n",
        "#        numerico (0,1,2) corretto, e prepararle per essere aggiunte al dataset d'oro.\n",
        "# INPUT: Cartella con le maschere a colori.\n",
        "# OUTPUT: Cartella con le maschere numeriche pulite.\n",
        "# =============================================================================\n",
        "\n",
        "def run_mask_purification():\n",
        "    \"\"\"Converte una cartella di maschere a colori nel formato numerico standard.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\nAVVIO PURIFICAZIONE MASCHERE CVAT\\n\" + \"=\"*60)\n",
        "\n",
        "    # --- CONFIGURAZIONE ---\n",
        "    # La cartella dove hai scompattato le maschere a colori da CVAT\n",
        "    SOURCE_CVAT_COLOR_MASKS_DIR = Path(\"/content/drive/MyDrive/CVAT_Exports/SegmentationClass\")\n",
        "    # La cartella dove verranno salvate le maschere pulite\n",
        "    TARGET_CLEAN_NUMERIC_MASKS_DIR = Path(\"/content/drive/MyDrive/CVAT_Exports_Cleaned\")\n",
        "    TARGET_CLEAN_NUMERIC_MASKS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if not SOURCE_CVAT_COLOR_MASKS_DIR.exists():\n",
        "        raise FileNotFoundError(f\"Cartella sorgente non trovata: {SOURCE_CVAT_COLOR_MASKS_DIR}\")\n",
        "\n",
        "    # La mappa di conversione (BGR di OpenCV -> Indice di Classe)\n",
        "    # ATTENZIONE: Questa deve corrispondere ESATTAMENTE ai colori del tuo progetto CVAT.\n",
        "    COLOR_MAP_BGR = {\n",
        "        (0, 0, 0): 0,        # background (Nero)\n",
        "        (100, 100, 255): 1,  # hole (Rosso/Rosa in BGR) - ATTENZIONE AI VALORI\n",
        "        (220, 220, 0): 2     # damage (Ciano in BGR) - ATTENZIONE AI VALORI\n",
        "    }\n",
        "\n",
        "    print(f\"Conversione basata sulla mappa BGR: {COLOR_MAP_BGR}\")\n",
        "\n",
        "    # --- CICLO DI CONVERSIONE ---\n",
        "    mask_files = list(SOURCE_CVAT_COLOR_MASKS_DIR.glob(\"*.png\"))\n",
        "    print(f\"Trovate {len(mask_files)} maschere da convertire...\")\n",
        "\n",
        "    for mask_color_path in tqdm(mask_files, desc=\"Purificazione maschere\"):\n",
        "        mask_color = cv2.imread(str(mask_color_path), cv2.IMREAD_COLOR)\n",
        "        if mask_color is None: continue\n",
        "\n",
        "        h, w, _ = mask_color.shape\n",
        "        mask_indexed = np.full((h, w), fill_value=0, dtype=np.uint8) # Default a background\n",
        "\n",
        "        for color_bgr, class_index in COLOR_MAP_BGR.items():\n",
        "            # Crea una maschera booleana per ogni colore\n",
        "            matches = np.all(mask_color == color_bgr, axis=-1)\n",
        "            mask_indexed[matches] = class_index\n",
        "\n",
        "        save_path = TARGET_CLEAN_NUMERIC_MASKS_DIR / mask_color_path.name\n",
        "        cv2.imwrite(str(save_path), mask_indexed)\n",
        "\n",
        "    print(f\"\\nâœ… PURIFICAZIONE COMPLETATA.\")\n",
        "    print(f\"Le tue maschere numeriche sono pronte in: '{TARGET_CLEAN_NUMERIC_MASKS_DIR}'\")\n",
        "    print(\"Ora puoi copiarle manualmente nella cartella 'GoldenDataset_Clean/masks'.\")\n",
        "\n",
        "# Esempio di esecuzione (eseguire solo quando hai nuovi dati da CVAT)\n",
        "# run_mask_purification()"
      ],
      "metadata": {
        "id": "8C4sIilNg_hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "#@title WORKFLOW #1: PRE-TRAINING DEL MODELLO BASE\n",
        "#\n",
        "# SCOPO: Addestrare il modello sulle ~600 maschere automatiche per insegnargli\n",
        "#        le caratteristiche di base (forma del foro, texture). L'encoder Ã¨ congelato.\n",
        "# INPUT: Dati da Config.PRETRAIN_IMG_DIR e Config.PRETRAIN_MASK_DIR.\n",
        "# OUTPUT: File dei pesi in Config.PRETRAINED_MODEL_PATH.\n",
        "# =============================================================================\n",
        "\n",
        "def run_pre_training():\n",
        "    \"\"\"\n",
        "    Esegue l'intero processo di pre-training: preparazione dati, training con encoder\n",
        "    congelato e salvataggio del modello base.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\nINIZIO PIPELINE DI PRE-TRAINING\\n\" + \"=\"*60)\n",
        "\n",
        "    # --- 1. Preparazione e Split Dati Grezzi ---\n",
        "    print(\"\\n--- [Fase 1.1] Preparazione e Split Dati Grezzi ---\")\n",
        "    all_images = list(Config.PRETRAIN_IMG_DIR.glob(\"**/*.jpg\"))\n",
        "    all_masks = list(Config.PRETRAIN_MASK_DIR.glob(\"*.png\"))\n",
        "\n",
        "    img_map = {extract_hole_id_from_path(p): p for p in all_images if extract_hole_id_from_path(p) is not None}\n",
        "    mask_map = {extract_hole_id_from_path(p): p for p in all_masks if extract_hole_id_from_path(p) is not None}\n",
        "\n",
        "    common_ids = sorted(list(set(img_map.keys()) & set(mask_map.keys())))\n",
        "    paired_files = [(img_map[id], mask_map[id]) for id in common_ids]\n",
        "\n",
        "    if Config.PRETRAIN_SPLIT_DIR.exists(): shutil.rmtree(Config.PRETRAIN_SPLIT_DIR)\n",
        "\n",
        "    # Split 80% train, 10% val, 10% test (il test non Ã¨ usato qui, ma preparato)\n",
        "    train_end = int(len(paired_files) * 0.8)\n",
        "    val_end = train_end + int(len(paired_files) * 0.1)\n",
        "    train_files, val_files = paired_files[:train_end], paired_files[train_end:val_end]\n",
        "\n",
        "    def copy_pretrain_files(files, subset_name):\n",
        "        (Config.PRETRAIN_SPLIT_DIR/subset_name/\"images\").mkdir(parents=True)\n",
        "        (Config.PRETRAIN_SPLIT_DIR/subset_name/\"masks\").mkdir(parents=True)\n",
        "        for img_path, mask_path in tqdm(files, desc=f\"Copia in {subset_name}\"):\n",
        "            shutil.copy(img_path, Config.PRETRAIN_SPLIT_DIR/subset_name/\"images\"/mask_path.name.replace(\".png\",\".jpg\"))\n",
        "            shutil.copy(mask_path, Config.PRETRAIN_SPLIT_DIR/subset_name/\"masks\"/mask_path.name)\n",
        "\n",
        "    copy_pretrain_files(train_files, \"train\")\n",
        "    copy_pretrain_files(val_files, \"val\")\n",
        "    print(f\"âœ… Dati di pre-training splittati: {len(train_files)} train, {len(val_files)} val.\")\n",
        "\n",
        "    # --- 2. Dataloaders e Modello ---\n",
        "    print(\"\\n--- [Fase 1.2] Creazione Dataloaders e Modello ---\")\n",
        "    transform = A.Compose([A.Resize(Config.IMG_SIZE, Config.IMG_SIZE), A.Normalize(mean=(0.5,), std=(0.5,)), ToTensorV2()])\n",
        "    train_ds = RadioDataset(Config.PRETRAIN_SPLIT_DIR / \"train\", transform)\n",
        "    val_ds = RadioDataset(Config.PRETRAIN_SPLIT_DIR / \"val\", transform)\n",
        "    train_loader = DataLoader(train_ds, batch_size=Config.PRETRAIN_BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=Config.PRETRAIN_BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = smp.UnetPlusPlus(encoder_name=Config.ENCODER_NAME, encoder_weights=\"imagenet\", in_channels=1, classes=Config.N_CLASSES).to(Config.DEVICE)\n",
        "    for param in model.encoder.parameters(): param.requires_grad = False\n",
        "    print(f\"âœ… Modello UNet++ ('{Config.ENCODER_NAME}') con encoder congelato pronto.\")\n",
        "\n",
        "    # --- 3. Loop di Training ---\n",
        "    print(\"\\n--- [Fase 1.3] Inizio Loop di Training ---\")\n",
        "    loss_fn = smp.losses.DiceLoss(mode='multiclass', from_logits=True)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.PRETRAIN_LR)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.PRETRAIN_EPOCHS)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(Config.DEVICE==\"cuda\"))\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(Config.PRETRAIN_EPOCHS):\n",
        "        model.train()\n",
        "        for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.PRETRAIN_EPOCHS} Train\"):\n",
        "            images, masks = images.to(Config.DEVICE, non_blocking=True), masks.to(Config.DEVICE, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(Config.DEVICE==\"cuda\")):\n",
        "                outputs = model(images)\n",
        "                loss = loss_fn(outputs, masks)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{Config.PRETRAIN_EPOCHS} Val\"):\n",
        "                images, masks = images.to(Config.DEVICE, non_blocking=True), masks.to(Config.DEVICE, non_blocking=True)\n",
        "                with torch.cuda.amp.autocast(enabled=(Config.DEVICE==\"cuda\")):\n",
        "                    val_loss += loss_fn(model(images), masks).item()\n",
        "\n",
        "        avg_val_loss = val_loss / max(1, len(val_loader))\n",
        "        print(f\"Pre-training Epoch {epoch+1}: Val Loss: {avg_val_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), Config.PRETRAINED_MODEL_PATH)\n",
        "            print(f\"ðŸ† Checkpoint pre-training salvato (Val Loss: {avg_val_loss:.4f}).\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print(\"\\nðŸŽ‰ Pre-addestramento Finito.\")\n",
        "    print(f\"Modello base salvato in: {Config.PRETRAINED_MODEL_PATH}\")\n",
        "\n",
        "# Esempio di esecuzione (da non eseguire se non necessario)\n",
        "# run_pre_training()"
      ],
      "metadata": {
        "id": "fbMdKToKarv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Celle di craezione patches\n"
      ],
      "metadata": {
        "id": "9TXRxKblbKSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# WORKFLOW #2.A: GENERATORE DI PATCH \"OCCHIO DI FALCO\"\n",
        "#\n",
        "# SCOPO: Isolare i dettagli complessi del danno (ragnatele) creando patch mirate.\n",
        "#        Usa l'algoritmo \"Blackout\" per escludere il foro.\n",
        "# INPUT: Dati da Config.GOLDEN_DATA_DIR.\n",
        "# OUTPUT: Cartella locale '/content/patches_dataset_raw' piena di candidati.\n",
        "# =============================================================================\n",
        "\n",
        "def generate_patches():\n",
        "    \"\"\"Usa l'algoritmo 'Blackout' per generare le patch di dettagli.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\nINIZIO GENERAZIONE PATCH\\n\" + \"=\"*60)\n",
        "\n",
        "    # Pulizia delle cartelle di lavoro\n",
        "    if Path('/content/temp_finetune_split').exists(): shutil.rmtree('/content/temp_finetune_split')\n",
        "    if Config.PATCHES_RAW_DIR.exists(): shutil.rmtree(Config.PATCHES_RAW_DIR)\n",
        "\n",
        "    # Split temporaneo dei dati d'oro per processarli\n",
        "    all_stems = sorted([p.stem for p in (Config.GOLDEN_DATA_DIR / \"masks\").glob(\"*.png\")])\n",
        "    train_stems, val_stems = train_test_split(all_stems, test_size=0.2, random_state=42)\n",
        "    copy_files_for_split(train_stems, \"train\", Config.GOLDEN_DATA_DIR, Path('/content/temp_finetune_split'))\n",
        "    copy_files_for_split(val_stems, \"val\", Config.GOLDEN_DATA_DIR, Path('/content/temp_finetune_split'))\n",
        "    print(\"âœ… Dati d'oro splittati in locale per il processamento.\")\n",
        "\n",
        "    for subset in [\"train\", \"val\"]:\n",
        "        print(f\"\\nProcesso la subset: {subset}...\")\n",
        "        source_img_dir = Path('/content/temp_finetune_split')/subset/\"images\"\n",
        "        source_mask_dir = Path('/content/temp_finetune_split')/subset/\"masks\"\n",
        "        target_img_dir = Config.PATCHES_RAW_DIR/subset/\"images\"\n",
        "        target_mask_dir = Config.PATCHES_RAW_DIR/subset/\"masks\"\n",
        "        target_img_dir.mkdir(parents=True)\n",
        "        target_mask_dir.mkdir(parents=True)\n",
        "        stems = [p.stem for p in source_mask_dir.glob(\"*.png\")]\n",
        "\n",
        "        for stem in tqdm(stems, desc=f\"Applicando blackout su '{subset}'\"):\n",
        "            img_path = next(source_img_dir.glob(f\"{stem}.*\"))\n",
        "            mask_path = source_mask_dir / f\"{stem}.png\"\n",
        "            img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
        "            mask = cv2.imread(str(mask_path), cv2.IMREAD_UNCHANGED)\n",
        "            if img is None: continue\n",
        "\n",
        "            hole_mask = np.uint8(mask == 1) # Assumendo 1=hole, 2=damage. Cambiare se necessario\n",
        "            damage_mask = np.uint8(mask == 2)\n",
        "\n",
        "            # --- Logica Blackout ---\n",
        "            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (Config.BLACKOUT_MARGIN*2, Config.BLACKOUT_MARGIN*2))\n",
        "            blackout_zone = cv2.dilate(hole_mask, kernel, iterations=1)\n",
        "            surviving_damage_mask = damage_mask.copy()\n",
        "            surviving_damage_mask[blackout_zone > 0] = 0\n",
        "\n",
        "            surviving_points_yx = np.argwhere(surviving_damage_mask > 0)\n",
        "            if len(surviving_points_yx) == 0: continue\n",
        "\n",
        "            num_to_sample = min(Config.DESIRED_PATCHES_PER_IMAGE, len(surviving_points_yx))\n",
        "            sampled_indices = np.random.choice(len(surviving_points_yx), size=num_to_sample, replace=False)\n",
        "\n",
        "            for i, idx in enumerate(sampled_indices):\n",
        "                y, x = surviving_points_yx[idx]\n",
        "                y_min=max(0,y-Config.PATCH_SIZE//2); x_min=max(0,x-Config.PATCH_SIZE//2)\n",
        "                y_max=y_min+Config.PATCH_SIZE; x_max=x_min+Config.PATCH_SIZE\n",
        "                if y_max>img.shape[0] or x_max>img.shape[1]: continue\n",
        "\n",
        "                img_patch = img[y_min:y_max, x_min:x_max]\n",
        "                mask_patch = mask[y_min:y_max, x_min:x_max]\n",
        "\n",
        "                if img_patch.shape == (Config.PATCH_SIZE, Config.PATCH_SIZE):\n",
        "                    cv2.imwrite(str(target_img_dir/f\"{stem}_patch_{i}.png\"), img_patch)\n",
        "                    cv2.imwrite(str(target_mask_dir/f\"{stem}_patch_{i}.png\"), mask_patch)\n",
        "\n",
        "    shutil.rmtree('/content/temp_finetune_split')\n",
        "    print(f\"\\nðŸŽ‰ Generazione Patch Completata. I candidati sono in '{Config.PATCHES_RAW_DIR}'\")\n",
        "\n",
        "# Esempio di esecuzione (da non eseguire se non necessario)\n",
        "# generate_patches()"
      ],
      "metadata": {
        "id": "QnMz46LHa_od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# WORKFLOW #2.B: CURA MANUALE E ARCHIVIAZIONE PATCH\n",
        "#\n",
        "# SCOPO: Selezionare a mano le patch migliori dalla cartella di candidati\n",
        "#        e impacchettarle in un singolo file ZIP su Google Drive per uso futuro.\n",
        "# INPUT: Cartella '/content/patches_dataset_raw' e una lista di nomi di file.\n",
        "# OUTPUT: File ZIP in Config.GOLDEN_PATCHES_ZIP_PATH.\n",
        "# =============================================================================\n",
        "\n",
        "def curate_and_zip_patches(list_of_best_stems):\n",
        "    \"\"\"\n",
        "    Prende una lista di nomi di file di patch, li recupera, e li archivia in uno ZIP.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\nINIZIO CURA E ARCHIVIAZIONE PATCH\\n\" + \"=\"*60)\n",
        "\n",
        "    TEMP_CURATED_DIR = Path(\"/content/curated_patches_for_zip\")\n",
        "    if TEMP_CURATED_DIR.exists(): shutil.rmtree(TEMP_CURATED_DIR)\n",
        "\n",
        "    target_img_dir = TEMP_CURATED_DIR / \"images\"\n",
        "    target_mask_dir = TEMP_CURATED_DIR / \"masks\"\n",
        "    target_img_dir.mkdir(parents=True)\n",
        "    target_mask_dir.mkdir(parents=True)\n",
        "\n",
        "    found_count = 0\n",
        "    for stem in tqdm(list_of_best_stems, desc=\"Recuperando i file scelti\"):\n",
        "        found = False\n",
        "        for subset in [\"train\", \"val\"]:\n",
        "            source_img_path = Config.PATCHES_RAW_DIR / subset / \"images\" / f\"{stem}.png\"\n",
        "            source_mask_path = Config.PATCHES_RAW_DIR / subset / \"masks\" / f\"{stem}.png\"\n",
        "            if source_img_path.exists():\n",
        "                shutil.copy(source_img_path, target_img_dir / source_img_path.name)\n",
        "                shutil.copy(source_mask_path, target_mask_dir / source_mask_path.name)\n",
        "                found = True\n",
        "                break\n",
        "        if found:\n",
        "            found_count += 1\n",
        "\n",
        "    print(f\"\\nRecuperati con successo {found_count} su {len(list_of_best_stems)} file richiesti.\")\n",
        "\n",
        "    if found_count > 0:\n",
        "        # Crea il nome del file con una versione (es. v1, v2)\n",
        "        zip_name = Config.GOLDEN_PATCHES_ZIP_PATH.stem.split('_v')[0] # Prende 'GoldenPatches'\n",
        "        version = 1\n",
        "        while (Config.GOLDEN_PATCHES_ZIP_DIR / f\"{zip_name}_v{version}.zip\").exists():\n",
        "            version += 1\n",
        "\n",
        "        final_zip_path = Config.GOLDEN_PATCHES_ZIP_DIR / f\"{zip_name}_v{version}.zip\"\n",
        "\n",
        "        # Crea l'archivio\n",
        "        shutil.make_archive(str(final_zip_path.with_suffix('')), 'zip', TEMP_CURATED_DIR)\n",
        "        shutil.rmtree(TEMP_CURATED_DIR)\n",
        "\n",
        "        print(f\"\\nðŸŽ‰ ARCHIVIO CREATO: '{final_zip_path}'\")\n",
        "    else:\n",
        "        print(\"\\nNessun file recuperato. Controlla i nomi nella lista.\")\n",
        "\n",
        "# Esempio di esecuzione:\n",
        "BEST_PATCH_STEMS = [ \"H569_..._patch_3\", \"H554_..._patch_11\" ]\n",
        "curate_and_zip_patches(BEST_PATCH_STEMS)"
      ],
      "metadata": {
        "id": "bb9sMc3nbOoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##fine creazione patches\n"
      ],
      "metadata": {
        "id": "Oi3EPjYMbO6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PArte 2\n"
      ],
      "metadata": {
        "id": "oIaIr-X1bXvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# WORKFLOW #3: FINE-TUNING IBRIDO \"OCCHIO DI FALCO\"\n",
        "#\n",
        "# SCOPO: Addestrare il modello finale usando un mix di dati \"d'oro\" interi\n",
        "#        (per il contesto) e le patch curate (per i dettagli).\n",
        "#        Tutti i layer del modello sono sbloccati e il learning rate Ã¨ basso.\n",
        "# INPUT: Checkpoint dal pre-training (o precedente fine-tuning) e ZIP delle patch curate.\n",
        "# OUTPUT: File dei pesi finale in Config.FINETUNED_MODEL_PATH.\n",
        "# =============================================================================\n",
        "\n",
        "def run_fine_tuning():\n",
        "    \"\"\"\n",
        "    Esegue il training ibrido, caricando le immagini intere e le patch curate.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\nINIZIO PIPELINE DI FINE-TUNING IBRIDO\\n\" + \"=\"*60)\n",
        "\n",
        "    # --- 1. Preparazione Dati ---\n",
        "    print(\"\\n--- [Fase 3.1] Preparazione Dati per il Fine-Tuning ---\")\n",
        "    if Config.FINETUNE_SPLIT_DIR.exists(): shutil.rmtree(Config.FINETUNE_SPLIT_DIR)\n",
        "    if Config.PATCHES_CURATED_DIR.exists(): shutil.rmtree(Config.PATCHES_CURATED_DIR)\n",
        "\n",
        "    if not Config.GOLDEN_PATCHES_ZIP_PATH.exists():\n",
        "      raise FileNotFoundError(f\"Archivio ZIP delle patch non trovato in {Config.GOLDEN_PATCHES_ZIP_PATH}. Esegui prima la cura delle patch.\")\n",
        "\n",
        "    with zipfile.ZipFile(Config.GOLDEN_PATCHES_ZIP_PATH, 'r') as zf:\n",
        "        zf.extractall(Config.PATCHES_CURATED_DIR)\n",
        "    print(f\"âœ… Patch d'oro estratte in '{Config.PATCHES_CURATED_DIR}'\")\n",
        "\n",
        "    # Split 80/20 delle immagini \"d'oro\" INTERE\n",
        "    all_full_stems = sorted([p.stem for p in (Config.GOLDEN_DATA_DIR / \"masks\").glob(\"*.png\")])\n",
        "    train_stems, val_stems = train_test_split(all_full_stems, test_size=0.2, random_state=42)\n",
        "    copy_files_for_split(train_stems, \"train\", Config.GOLDEN_DATA_DIR, Config.FINETUNE_SPLIT_DIR)\n",
        "    copy_files_for_split(val_stems, \"val\", Config.GOLDEN_DATA_DIR, Config.FINETUNE_SPLIT_DIR)\n",
        "    print(f\"âœ… Immagini d'oro intere splittate: {len(train_stems)} train, {len(val_stems)} val.\")\n",
        "\n",
        "    # --- 2. Dataloaders Ibridi ---\n",
        "    print(\"\\n--- [Fase 3.2] Creazione Dataloaders Ibridi ---\")\n",
        "    transform = A.Compose([A.Resize(Config.IMG_SIZE, Config.IMG_SIZE), A.HorizontalFlip(), A.VerticalFlip(), A.Normalize(mean=(0.5,), std=(0.5,)), ToTensorV2()])\n",
        "\n",
        "    train_ds = HybridDataset(\n",
        "        Config.FINETUNE_SPLIT_DIR/\"train\"/\"images\", Config.FINETUNE_SPLIT_DIR/\"train\"/\"masks\",\n",
        "        Config.PATCHES_CURATED_DIR/\"images\", Config.PATCHES_CURATED_DIR/\"masks\",\n",
        "        transform\n",
        "    )\n",
        "    # Nota: Usiamo un approccio semplificato dove il val set ibrido contiene anche le patch di training\n",
        "    # Per una valutazione purista, le patch dovrebbero essere splittate a loro volta.\n",
        "    val_ds = HybridDataset(\n",
        "        Config.FINETUNE_SPLIT_DIR/\"val\"/\"images\", Config.FINETUNE_SPLIT_DIR/\"val\"/\"masks\",\n",
        "        Config.PATCHES_CURATED_DIR/\"images\", Config.PATCHES_CURATED_DIR/\"masks\",\n",
        "        transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=Config.FINETUNE_BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=Config.FINETUNE_BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # --- 3. Modello e Loop di Training ---\n",
        "    print(\"\\n--- [Fase 3.3] Inizio Loop di Fine-Tuning ---\")\n",
        "    model = smp.UnetPlusPlus(encoder_name=Config.ENCODER_NAME, in_channels=1, classes=Config.N_CLASSES).to(Config.DEVICE)\n",
        "\n",
        "    # Parte dall'ultimo fine-tuning se esiste, altrimenti dal pre-training\n",
        "    model_to_load = Config.FINETUNED_MODEL_PATH if Config.FINETUNED_MODEL_PATH.exists() else Config.PRETRAINED_MODEL_PATH\n",
        "    print(f\"Caricamento pesi da: {model_to_load}\")\n",
        "    model.load_state_dict(torch.load(model_to_load, map_location=Config.DEVICE))\n",
        "\n",
        "    # Sblocca tutti i parametri per il fine-tuning\n",
        "    for p in model.parameters(): p.requires_grad = True\n",
        "    print(\"âœ… Modello sbloccato per il fine-tuning.\")\n",
        "\n",
        "    loss_fn = smp.losses.DiceLoss(mode='multiclass', from_logits=True)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.FINETUNE_LR)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(Config.DEVICE==\"cuda\"))\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(Config.FINETUNE_EPOCHS):\n",
        "        model.train()\n",
        "        for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.FINETUNE_EPOCHS} Train\"):\n",
        "            images, masks = images.to(Config.DEVICE, non_blocking=True), masks.to(Config.DEVICE, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(Config.DEVICE==\"cuda\")):\n",
        "                outputs = model(images)\n",
        "                loss = loss_fn(outputs, masks)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{Config.FINETUNE_EPOCHS} Val\"):\n",
        "                images, masks = images.to(Config.DEVICE, non_blocking=True), masks.to(Config.DEVICE, non_blocking=True)\n",
        "                with torch.cuda.amp.autocast(enabled=(Config.DEVICE==\"cuda\")):\n",
        "                    val_loss += loss_fn(model(images), masks).item()\n",
        "\n",
        "        avg_val_loss = val_loss / max(1, len(val_loader))\n",
        "        print(f\"Fine-Tuning Epoch {epoch+1}: Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), Config.FINETUNED_MODEL_PATH)\n",
        "            # Salviamo una copia locale per scaricarla subito se serve\n",
        "            shutil.copy(Config.FINETUNED_MODEL_PATH, \"/content/best_model_finetuned_local.pth\")\n",
        "            print(f\"ðŸ† Checkpoint fine-tuning salvato (Val Loss: {avg_val_loss:.4f}).\")\n",
        "\n",
        "    print(\"\\nðŸŽ‰ Fine-Tuning Ibrido Terminato.\")\n",
        "\n",
        "# Esempio di esecuzione (da non eseguire se non necessario)\n",
        "# run_fine_tuning()"
      ],
      "metadata": {
        "id": "qUc9mCQGbR3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# WORKFLOW #4: VALUTAZIONE FINALE E CONFRONTO VISIVO\n",
        "#\n",
        "# SCOPO: Confrontare visivamente le performance del modello pre-training\n",
        "#        contro il modello finale post-fine-tuning ibrido.\n",
        "# INPUT: Pesi da Config.PRETRAINED_MODEL_PATH e Config.FINETUNED_MODEL_PATH.\n",
        "#        Dati dal validation set del tuo dataset d'oro.\n",
        "# OUTPUT: Plot di confronto a 4 pannelli.\n",
        "# =============================================================================\n",
        "\n",
        "def run_evaluation():\n",
        "    \"\"\"\n",
        "    Esegue il confronto visivo finale tra il modello di pre-training e quello\n",
        "    dopo il fine-tuning ibrido, usando il validation set dei dati d'oro.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\nINIZIO PIPELINE DI VALUTAZIONE COMPARATIVA\\n\" + \"=\"*60)\n",
        "\n",
        "    # --- 1. Preparazione Dati e Modelli ---\n",
        "    print(\"\\n--- [Fase 4.1] Preparazione dati e caricamento modelli ---\")\n",
        "    transform = A.Compose([A.Resize(Config.IMG_SIZE, Config.IMG_SIZE), A.Normalize(mean=(0.5,), std=(0.5,)), ToTensorV2()])\n",
        "\n",
        "    # Usa il validation set delle immagini INTERE per un confronto equo \"nel mondo reale\"\n",
        "    val_dataset_full = RadioDataset(Config.FINETUNE_SPLIT_DIR / \"val\", transform=transform)\n",
        "    print(f\"Valutazione su {len(val_dataset_full)} campioni del validation set 'd'oro'.\")\n",
        "\n",
        "    # Carica entrambi i modelli\n",
        "    model_before = smp.UnetPlusPlus(encoder_name=Config.ENCODER_NAME, in_channels=1, classes=Config.N_CLASSES).to(Config.DEVICE)\n",
        "    model_after = smp.UnetPlusPlus(encoder_name=Config.ENCODER_NAME, in_channels=1, classes=Config.N_CLASSES).to(Config.DEVICE)\n",
        "\n",
        "    if not Config.PRETRAINED_MODEL_PATH.exists() or not Config.FINETUNED_MODEL_PATH.exists():\n",
        "        raise FileNotFoundError(\"Uno o entrambi i file dei pesi (pre-trained e fine-tuned) non sono stati trovati.\")\n",
        "\n",
        "    model_before.load_state_dict(torch.load(Config.PRETRAINED_MODEL_PATH, map_location=Config.DEVICE)); model_before.eval()\n",
        "    model_after.load_state_dict(torch.load(Config.FINETUNED_MODEL_PATH, map_location=Config.DEVICE)); model_after.eval()\n",
        "    print(\"âœ… Modelli PRE e POST caricati.\")\n",
        "\n",
        "    # --- 2. Esecuzione del Confronto Visivo ---\n",
        "    print(\"\\n--- [Fase 4.2] Generazione del confronto visivo ---\")\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(val_dataset_full)):\n",
        "            image_tensor, mask_gt_tensor = val_dataset_full[i]\n",
        "            input_tensor = image_tensor.unsqueeze(0).to(Config.DEVICE)\n",
        "\n",
        "            # Inferenza con entrambi i modelli\n",
        "            output_before = model_before(input_tensor)\n",
        "            pred_before = torch.argmax(output_before.squeeze(), dim=0).cpu().numpy()\n",
        "\n",
        "            output_after = model_after(input_tensor)\n",
        "            pred_after = torch.argmax(output_after.squeeze(), dim=0).cpu().numpy()\n",
        "\n",
        "            # De-normalizza l'immagine per una visualizzazione corretta\n",
        "            image_vis = image_tensor.permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
        "            stem = val_dataset_full.stems[i]\n",
        "\n",
        "            fig, axes = plt.subplots(1, 4, figsize=(28, 7))\n",
        "            fig.suptitle(f'Confronto per: {stem}', fontsize=16, y=0.95)\n",
        "            axes[0].imshow(image_vis, cmap='gray'); axes[0].set_title(\"Immagine Originale\")\n",
        "            axes[1].imshow(mask_gt_tensor.numpy(), cmap='viridis', vmin=0, vmax=2); axes[1].set_title(\"Ground Truth\")\n",
        "            axes[2].imshow(pred_before, cmap='viridis', vmin=0, vmax=2); axes[2].set_title(\"PRE-TRAINING\")\n",
        "            axes[3].imshow(pred_after, cmap='viridis', vmin=0, vmax=2); axes[3].set_title(\"POST-IBRIDO\")\n",
        "            for ax in axes: ax.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "    print(\"\\nðŸŽ‰ Valutazione Completata.\")\n",
        "\n",
        "# Esempio di esecuzione (da non eseguire se non necessario)\n",
        "# run_evaluation()"
      ],
      "metadata": {
        "id": "DIgmQuWNcImj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPorxQI6ctP-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
