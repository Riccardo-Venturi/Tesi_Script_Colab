{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "x09_LO3w5x3Z"
      ],
      "mount_file_id": "1TDmXfcm8MK6ndUybSpdnCQ57mpqqwFH4",
      "authorship_tag": "ABX9TyMAjE/0sq+aNPNeHF6V6e+a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riccardo-Venturi/Tesi_Script_Colab/blob/main/sequential_finding_of_forces_from_time_series__geometry_hu_moments_and_geometry_factors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#METRICHE\n",
        "<details>\n",
        "    Metriche di \"Grandezza\" (Quanto √® grosso il danno?):\n",
        "\n",
        "        AreaDelaminata: L'area totale dei pixel di classe \"danno\". Fondamentale.\n",
        "\n",
        "        Dmax: Il diametro massimo che circoscrive tutto il danno.\n",
        "\n",
        "        Damage-to-Hole Area Ratio (DHAR): Come hai detto tu, Area_Danno / Area_Foro. Semplice, adimensionale, potente. Ci dice se il danno √® trascurabile o esteso.\n",
        "\n",
        "    Metriche di \"Forma\" (Com'√® fatto il danno?):\n",
        "\n",
        "        Momenti di Hu: I tuoi 7 assi nella manica. Catturano l'essenza della forma in modo invariante. Essenziali per distinguere un danno compatto da uno fibroso.\n",
        "\n",
        "        Elongation / Aspect Ratio: Come hai scritto, indica la direzionalit√†. Un valore alto suggerisce che il danno si sta propagando lungo una direzione di fibra preferenziale, che √® un indicatore di pericolo.\n",
        "\n",
        "        Solidity / Circularity: Metriche che hai gi√† identificato. Ci dicono quanto il danno √® \"frastagliato\" vs \"compatto\". Un danno a bassa solidit√† (molto frastagliato) ha pi√π \"punte\" che agiscono da concentratori di stress, aumentando il rischio di propagazione delle cricche.\n",
        "\n",
        "    Metriche \"Radiali\" (Come si distribuisce il danno attorno al foro?):\n",
        "\n",
        "        Centroid Offset: Se il baricentro del danno √® molto spostato rispetto al centro del foro, indica un danno asimmetrico e, di nuovo, una concentrazione di stress non uniforme.\n",
        "\n",
        "        Copertura Angolare (Coverage): Delle 360¬∞ circonferenza del foro, quale percentuale √® affetta da danno? Questo distingue un piccolo danno localizzato (bassa copertura) da una delaminazione perimetrale completa (alta copertura)."
      ],
      "metadata": {
        "id": "N-PJnYVgNeZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUesta nuova cella dimostra che lgbm ha overfittato, quindi si preferisce prendere il modello MLP come utput con dati_forze_imputate"
      ],
      "metadata": {
        "id": "W0gZ7frDuazP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q lightgbm pandas matplotlib keras tensorflow scikit-learn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. CARICAMENTO E PREPARAZIONE DATASET ---\n",
        "FILE_INPUT = 'LSTM_master_dataset.csv'\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(FILE_INPUT)\n",
        "    print(f\"Dataset '{FILE_INPUT}' caricato. Shape: {data.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERRORE: File '{FILE_INPUT}' non trovato.\")\n",
        "    exit()\n",
        "\n",
        "# Selezione delle feature rilevanti per la sequenza.\n",
        "features_to_use = [\n",
        "    'Forza_N', 'AreaDelaminata_mm2', 'Dmax_mm', 'DF_diametro',\n",
        "    'DHAR_Area_Ratio', 'ShapeFactor', 'Hu_1', 'Hu_2', 'Hu_3', 'Hu_4', 'Hu_5', 'Hu_6', 'Hu_7'\n",
        "]\n",
        "target_features = ['AreaDelaminata_mm2', 'Dmax_mm', 'DF_diametro']\n",
        "\n",
        "# Verifica che tutte le feature esistano nel DataFrame e crea il subset\n",
        "features_to_use = [f for f in features_to_use if f in data.columns]\n",
        "data_subset = data[features_to_use].astype('float32')\n",
        "\n",
        "# --- 2. NORMALIZZAZIONE E CREAZIONE SEQUENZE ---\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(data_subset)\n",
        "\n",
        "# --- CORREZIONE: Ottieni gli indici delle colonne target PRIMA di creare le sequenze ---\n",
        "all_columns = list(data_subset.columns)\n",
        "target_indices = [all_columns.index(feature) for feature in target_features]\n",
        "\n",
        "\n",
        "# Funzione aggiornata per lavorare solo con array NumPy e indici\n",
        "def create_sequences(input_data, n_steps_in, target_col_indices):\n",
        "    X, y = [], []\n",
        "    for i in range(len(input_data) - n_steps_in):\n",
        "        # La sequenza di input usa tutte le feature\n",
        "        X.append(input_data[i:(i + n_steps_in), :])\n",
        "        # L'output (y) prende solo le colonne target al passo temporale successivo\n",
        "        y.append(input_data[i + n_steps_in, target_col_indices])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "n_steps = 15\n",
        "X, y = create_sequences(scaled_data, n_steps, target_indices)\n",
        "\n",
        "\n",
        "# --- 3. SUDDIVISIONE CRONOLOGICA TRAIN/TEST ---\n",
        "test_size = 150\n",
        "train_size = X.shape[0] - test_size\n",
        "\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "print(f\"\\nShape X_train: {X_train.shape}\")\n",
        "print(f\"Shape y_train: {y_train.shape}\")\n",
        "print(f\"Shape X_test: {X_test.shape}\")\n",
        "print(f\"Shape y_test: {y_test.shape}\")\n",
        "\n",
        "\n",
        "# --- 4. COSTRUZIONE E ADDESTRAMENTO MODELLO LSTM ---\n",
        "print(\"\\nCostruzione e addestramento del modello LSTM...\")\n",
        "model = Sequential([\n",
        "    LSTM(units=100, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(units=50, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(units=y_train.shape[1])\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.summary()\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Addestramento completato.\")\n",
        "\n",
        "\n",
        "# --- 5. VALUTAZIONE MODELLO ---\n",
        "# Predizioni\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "\n",
        "# Inversione della normalizzazione\n",
        "# Creiamo un array \"dummy\" con la stessa shape dei dati originali scalati\n",
        "num_total_features = scaled_data.shape[1]\n",
        "\n",
        "# Per le predizioni\n",
        "dummy_pred = np.zeros((len(y_pred_scaled), num_total_features))\n",
        "dummy_pred[:, target_indices] = y_pred_scaled\n",
        "y_pred = scaler.inverse_transform(dummy_pred)[:, target_indices]\n",
        "\n",
        "# Per i valori reali del test set\n",
        "dummy_test = np.zeros((len(y_test), num_total_features))\n",
        "dummy_test[:, target_indices] = y_test\n",
        "y_test_orig = scaler.inverse_transform(dummy_test)[:, target_indices]\n",
        "\n",
        "\n",
        "# --- 6. VISUALIZZAZIONE RISULTATI ---\n",
        "print(\"\\nGenerazione grafici di valutazione...\")\n",
        "\n",
        "# Grafico Loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss del Modello (MSE)')\n",
        "plt.xlabel('Epoca'); plt.ylabel('Loss'); plt.legend(); plt.grid(True); plt.show()\n",
        "\n",
        "# Grafici Predetto vs Reale\n",
        "for i, feature_name in enumerate(target_features):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.plot(y_test_orig[:, i], label='Valori Reali', color='blue', marker='.')\n",
        "    plt.plot(y_pred[:, i], label='Valori Predetti', color='red', linestyle='--')\n",
        "    plt.title(f'Predizione vs Valori Reali - {feature_name}')\n",
        "    plt.xlabel('Time Step (nel test set)')\n",
        "    plt.ylabel(feature_name)\n",
        "    plt.legend(); plt.grid(True); plt.show()"
      ],
      "metadata": {
        "id": "Yst-LNfyltI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CODICE AGGIUNTIVO PER L'INFERENZA SEQUENZIALE ---\n",
        "\n",
        "# Impostazioni per l'inferenza\n",
        "START_HOLE_NUMBER = 300\n",
        "PREDICTION_HORIZON = 50 # Quanti passi futuri predire\n",
        "\n",
        "# Trova l'indice nel DataFrame originale corrispondente al foro di partenza\n",
        "start_index = data[data['NumeroForo'] == START_HOLE_NUMBER].index[0]\n",
        "\n",
        "# Prendi l'ultima sequenza di dati reali disponibili come punto di partenza\n",
        "# (dobbiamo prenderla dai dati scalati)\n",
        "last_known_sequence = scaled_data[start_index - n_steps : start_index].reshape(1, n_steps, X.shape[2])\n",
        "\n",
        "# Lista per salvare le predizioni fatte un passo alla volta\n",
        "walk_forward_predictions_scaled = []\n",
        "\n",
        "current_sequence = last_known_sequence.copy()\n",
        "\n",
        "print(f\"\\nInizio inferenza Walk-Forward per {PREDICTION_HORIZON} passi dal foro {START_HOLE_NUMBER}...\")\n",
        "\n",
        "for _ in range(PREDICTION_HORIZON):\n",
        "    # 1. Predici il prossimo passo\n",
        "    next_step_pred_scaled = model.predict(current_sequence, verbose=0)\n",
        "    walk_forward_predictions_scaled.append(next_step_pred_scaled.flatten())\n",
        "\n",
        "    # 2. Crea un \"passo temporale\" completo per la prossima previsione.\n",
        "    # Dobbiamo creare una riga con tutte le 13 feature.\n",
        "    # Le feature target (es. Area, Dmax) vengono aggiornate con la predizione.\n",
        "    # Le altre feature (es. Forza, Hu) devono essere \"trasportate\" dall'ultimo passo noto.\n",
        "    # Per semplicit√†, in questa simulazione, replichiamo l'ultimo vettore di input e aggiorniamo solo le colonne predette.\n",
        "    next_input_vector = current_sequence[0, -1, :].copy()\n",
        "    next_input_vector[target_indices] = next_step_pred_scaled\n",
        "\n",
        "    # 3. Aggiorna la sequenza: rimuovi il primo passo e aggiungi quello nuovo\n",
        "    current_sequence = np.append(current_sequence[:, 1:, :], next_input_vector.reshape(1, 1, X.shape[2]), axis=1)\n",
        "\n",
        "print(\"Inferenza completata.\")\n",
        "\n",
        "\n",
        "# --- Inversione della Normalizzazione per l'inferenza ---\n",
        "walk_forward_predictions_scaled = np.array(walk_forward_predictions_scaled)\n",
        "\n",
        "dummy_walk_forward = np.zeros((len(walk_forward_predictions_scaled), num_total_features))\n",
        "dummy_walk_forward[:, target_indices] = walk_forward_predictions_scaled\n",
        "y_pred_walk_forward = scaler.inverse_transform(dummy_walk_forward)[:, target_indices]\n",
        "\n",
        "\n",
        "# --- Dati reali per confronto ---\n",
        "real_data_for_comparison = data.iloc[start_index : start_index + PREDICTION_HORIZON]\n",
        "\n",
        "# --- Grafico di confronto ---\n",
        "print(\"Generazione grafico inferenza Walk-Forward...\")\n",
        "for i, feature_name in enumerate(target_features):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    # Dati reali\n",
        "    plt.plot(real_data_for_comparison['NumeroForo'], real_data_for_comparison[feature_name], label='Valori Reali', color='blue', marker='.')\n",
        "    # Predizioni sequenziali\n",
        "    plt.plot(real_data_for_comparison['NumeroForo'], y_pred_walk_forward[:, i], label='Predizioni Walk-Forward', color='green', linestyle='--')\n",
        "    plt.title(f'Inferenza Sequenziale vs. Reale dal Foro {START_HOLE_NUMBER} - {feature_name}')\n",
        "    plt.xlabel('NumeroForo')\n",
        "    plt.ylabel(feature_name)\n",
        "    plt.legend(); plt.grid(True); plt.show()"
      ],
      "metadata": {
        "id": "UOHv2jIq35j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#vecchio metodo con imputazione\n"
      ],
      "metadata": {
        "id": "AgjbkEeilpJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title questo predispone si sia fatto tutto il resto sotto come test; risulta un diario scientifico sperimentale\n",
        "##prende il csv creato in precedenza con i valori mancanti delle forze e applica la lgbm che si √® dimostrata quella con errore sperimentale\n",
        "#pi√π basso e migiore aderenza ai dati\n",
        "# =============================================================================\n",
        "# SNIPPET FINALE PER IMPUTAZIONE FORZE MANCANTI CON MODELLO OTTIMALE\n",
        "# Metodo: LightGBM addestrato solo su 'NumeroForo'\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. Caricamento del dataset con i buchi ---\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/LSTM_master_dataset.csv\")\n",
        "    print(\"Dataset 'dataset_master_finalissimo.csv' caricato.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERRORE: Assicurati che il file 'dataset_master_finalissimo.csv' sia presente.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Preparazione Dati per l'Imputazione ---\n",
        "# Converti colonna in numerico, gestendo i valori mancanti\n",
        "df['Forza_N'] = pd.to_numeric(df['Forza_N'], errors='coerce')\n",
        "\n",
        "# Separa i dati per addestrare il nostro modello di imputazione\n",
        "df_known = df[df['Forza_N'].notna()].copy()\n",
        "\n",
        "# Dati di addestramento: usiamo TUTTI i dati noti per addestrare il modello finale\n",
        "X_train_final = df_known[['NumeroForo']]\n",
        "y_train_final = df_known['Forza_N']\n",
        "\n",
        "# Dati da imputare: le righe dove la Forza_N √® mancante\n",
        "X_to_impute = df[df['Forza_N'].isna()][['NumeroForo']]\n",
        "\n",
        "# --- 3. Addestramento del Modello Vincente e Imputazione ---\n",
        "print(\"\\nAddestramento del modello di imputazione finale (LGBM-Base)...\")\n",
        "# Usiamo i parametri di default, che si sono dimostrati efficaci\n",
        "imputation_model = lgb.LGBMRegressor(random_state=42, verbosity=-1)\n",
        "\n",
        "# Addestriamo il modello su TUTTI i dati di forza disponibili\n",
        "imputation_model.fit(X_train_final, y_train_final)\n",
        "\n",
        "print(\"Predizione dei valori mancanti...\")\n",
        "# Eseguiamo la predizione\n",
        "predicted_forces = imputation_model.predict(X_to_impute)\n",
        "\n",
        "# --- 4. Creazione e Salvataggio del Dataset Definitivo ---\n",
        "# Crea una copia del dataframe originale per non modificarlo\n",
        "df_imputed_lgbm = df.copy()\n",
        "\n",
        "# Riempi i valori NaN con le nostre predizioni\n",
        "df_imputed_lgbm.loc[df_imputed_lgbm['Forza_N'].isna(), 'Forza_N'] = predicted_forces\n",
        "\n",
        "# Salva il file CSV che userai per tutte le analisi future\n",
        "output_filename = 'dataset_LGBM_imputato.csv'\n",
        "df_imputed_lgbm.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset definitivo salvato come '{output_filename}'\")\n",
        "print(\"\\nPrime righe con i valori imputati:\")\n",
        "display(df_imputed_lgbm.loc[df['Forza_N'].isna()].head())\n",
        "\n",
        "# --- 5. Visualizzazione del Risultato (Highlight per la tesi) ---\n",
        "print(\"\\nGenerazione del grafico di confronto finale...\")\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "# Dati reali originali\n",
        "ax.scatter(df_known['NumeroForo'], df_known['Forza_N'],\n",
        "           label='Dato Reale', s=15, c='royalblue', zorder=3)\n",
        "\n",
        "# Dati che abbiamo appena imputato con il modello LGBM-Base\n",
        "ax.scatter(X_to_impute['NumeroForo'], predicted_forces,\n",
        "           label='Dato Imputato (LGBM-Base)', s=60, c='red', marker='X', zorder=5, edgecolor='black')\n",
        "\n",
        "ax.set_title(\"Dataset Finale con Forze Imputate tramite Regressione su Usura\", fontsize=18)\n",
        "ax.set_xlabel(\"Numero del Foro (Sequenza Esperimento)\", fontsize=14)\n",
        "ax.set_ylabel(\"Forza (N)\", fontsize=14)\n",
        "ax.legend(fontsize=12, title=\"Stato del Dato\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k70yGCVqkyAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cella di confronto visuale rapido\n",
        "\n",
        "df_mlp = pd.read_csv('dati_con_forze_imputate.csv')\n",
        "df_lgbm = pd.read_csv('dataset_LGBM_imputato.csv')\n",
        "\n",
        "# Estrai solo i valori imputati da entrambi\n",
        "indici_mancanti = df_lgbm['NumeroForo'].isin(range(169, 177)) # Esempio per il primo blocco\n",
        "forze_imputate_mlp = df_mlp[indici_mancanti]\n",
        "forze_imputate_lgbm = df_lgbm[indici_mancanti]\n",
        "\n",
        "# Grafico\n",
        "plt.figure(figsize=(20, 10))\n",
        "# Dati reali circostanti per contesto\n",
        "plt.plot(df_lgbm['NumeroForo'], df_lgbm['Forza_N'], 'o', color='lightgray', label='Dati Reali')\n",
        "\n",
        "# Dati imputati\n",
        "plt.plot(forze_imputate_mlp['NumeroForo'], forze_imputate_mlp['Forza_N'], 's-r', label='Imputato con MLP (complesso)')\n",
        "plt.plot(forze_imputate_lgbm['NumeroForo'], forze_imputate_lgbm['Forza_N'], 'X-g', label='Imputato con LGBM-Base (semplice/ottimale)')\n",
        "\n",
        "plt.title('Confronto Visivo delle Imputazioni', fontsize=16)\n",
        "plt.xlabel('NumeroForo')\n",
        "plt.ylabel('Forza (N)')\n",
        "plt.legend()\n",
        "plt.xlim(160, 185) # Zoom sulla zona di interesse\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L9v4vV7doknY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tabella dei Dati Essenziali e Costanti del Progetto**\n",
        "\n",
        "| Parametro / Dato | Valore | Unit√† | Fonte / Scopo | Note |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **PARAMETRI FISICI & GEOMETRICI (Ground Truth)** | | | | |\n",
        "| Diametro Nominale Foro (`D_nom`) | **6.0** | mm | Tesi / **Calibrazione** | La nostra \"stele di Rosetta\". √à la misura reale che useremo per calibrare ogni patch. |\n",
        "| Area Nominale Foro (`A_nom`) | **28.274** | mm¬≤ | Calcolata (`œÄ*r¬≤`) | Serve per calcolare `Adel` dai dati `Atot` umani e per lo `Shape Factor`. |\n",
        "| Spessore Laminato | ~2.0 | mm | Tesi | Non usato nei modelli attuali, ma fondamentale per future simulazioni FEM. |\n",
        "| **PARAMETRI DELLA SCANSIONE ORIGINALE** | | | | |\n",
        "| Risoluzione Scansione | **600** | DPI | Tesi / Specifiche | Utilizzata per il calcolo della scala teorica. |\n",
        "| Scala Teorica Originale | **23.62** | pixel/mm | Calcolata (`600/25.4`) | **Valida SOLO sulla scansione intera, non sulle patch ridimensionate.** |\n",
        "| Diametro Foro in Pixel (Teorico) | **~142** | pixel | Calcolato (`6 * 23.62`) | Utile come controllo di sanit√† sui primi script di rilevamento. |\n",
        "| **PARAMETRI DELLA PIPELINE ATTUALE (YOLO -> UNet)** | | | | |\n",
        "| Dimensione Patch di Input (UNet) | **512 x 512** | pixel | Tuo script di crop | **Questo ridimensionamento ha causato la perdita della scala originale.** |\n",
        "| **Scala Empirica Misurata su Patch** | **~35.8** | pixel/mm | Nostra misura manuale | **Valore indicativo** che conferma la discrepanza e la necessit√† di una calibrazione per patch. |\n",
        "| **CLASSI NELLE MASCHERE UNet** | | | | |\n",
        "| Sfondo | **0** | (valore pixel) | UNet script | Classe da ignorare nei calcoli. |\n",
        "| Foro | **1** | (valore pixel) | UNet script | Maschera del foro. La UNet la separa dal danno. |\n",
        "| Delaminazione | **2** | (valore pixel) | UNet script | Maschera del danno. **√à questa che dobbiamo misurare.** |\n",
        "\n",
        "---\n",
        "\n",
        "### **Confronto Dati: Umano vs. Nostro (Il Problema da Risolvere)**\n",
        "\n",
        "Questa tabella evidenzia la discrepanza che dobbiamo risolvere. Prendo il Foro 1 come esempio lampante:\n",
        "\n",
        "| Metrica | Dato Umano (Tesi) | Dato Nostro (CSV sballato) | Stato |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Adel** | **3.08 mm¬≤** | **6.14 mm¬≤** | üî¥ **Sbagliato!** (Errore del ~100%) |\n",
        "| **DMAX** | **6.22 mm** | **15.74 mm** | üî¥ **Sbagliato!** (Errore del ~150%) |\n",
        "| **Forza** | **89.50 N** | (da predire) | üîú Obiettivo Futuro |\n",
        "\n",
        "### **Cosa Facciamo Adesso (Il Piano Pratico Immediato)**\n",
        "\n",
        "Come hai detto tu, lavoriamo ripartendo dalle patch e dalle maschere che hai gi√†.\n",
        "\n",
        "1.  **Azione #1: Calibrazione Automatica (ORA).**\n",
        "    *   Prendiamo la cartella con le tue **patch radiografiche** (`.jpg`, 512x512).\n",
        "    *   Eseguiamo lo **script di calibrazione automatica** che ti ho fornito. Questo script misurer√† il diametro del foro in pixel in ogni patch e creer√† il file `calibrazione_scale_patch.csv`.\n",
        "    *   **Risultato:** Un file che associa ogni patch alla sua scala `px/mm` corretta.\n",
        "\n",
        "2.  **Azione #2: Estrazione Features Corretta.**\n",
        "    *   Modifichiamo lo script di estrazione features. Per ogni maschera:\n",
        "        *   Legge il nome del file (es. `H001_..._pred.png`).\n",
        "        *   Cerca la scala per `H001` nel file `calibrazione_scale_patch.csv`.\n",
        "        *   Misura area e DMAX in pixel dalla maschera.\n",
        "        *   **Usa la scala corretta per convertire i valori in mm e mm¬≤.**\n",
        "    *   **Risultato:** Un nuovo file `features_corrette_e_verificate.csv` con dati finalmente confrontabili con quelli del tesista.\n",
        "\n",
        "Iniziamo con l'**Azione #1**. Prepara la cartella con le patch `.jpg` e il codice di calibrazione che ti ho passato. √à il passo pi√π importante per sbloccare tutto il resto.\n"
      ],
      "metadata": {
        "id": "mPWnKqfmeLf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Fase 1: La Scansione Originale (Il Mondo Reale in Pixel)**\n",
        "<details>\n",
        "*   **Immagine di Riferimento:** `T0_90_1A_ingresso.jpg` (e le altre scansioni complete).\n",
        "*   **Scala Teorica (DPI):** 600 DPI (dots per inch).\n",
        "    *   Conversione: 1 pollice = 25.4 mm\n",
        "    *   **Scala Originale (px/mm):** `600 / 25.4 = **23.62 pixel per mm**`.\n",
        "*   **Dimensione Fisica Riferimento:** Diametro nominale del foro `D_nom = **6.0 mm**`.\n",
        "*   **Dimensione in Pixel (Teorica):** Sulla scansione originale, un foro da 6 mm dovrebbe avere un diametro di `6.0 mm * 23.62 px/mm ‚âà **142 pixel**`.\n",
        "\n",
        "#### **Fase 2: Dal Rilevamento YOLO al Ritaglio della Patch**\n",
        "\n",
        "*   **Script di Riferimento:** `yolorilevazionefori.ipynb` e gli script successivi di ordinamento e crop.\n",
        "*   **Processo:**\n",
        "    1.  YOLO rileva i fori sulla scansione intera (`imgsz=1280` o `(H,W)` nativa).\n",
        "    2.  Le coordinate delle bounding box vengono salvate e ordinate con K-Means (l'ordinamento bustrofedico).\n",
        "    3.  Uno script di crop prende il centro `(cx, cy)` di ogni foro rilevato.\n",
        "    4.  Viene ritagliata una patch quadrata attorno a quel centro. Hai usato `HS = 350` (patch 700x700) e `HS = 256` (patch 512x512).\n",
        "    5.  **IL PASSAGGIO CRUCIALE:** In uno degli script, c'√® `patch = cv2.resize(patch, (TARGET_SIZE, TARGET_SIZE), ...)` dove `TARGET_SIZE` √® `512`.\n",
        "\n",
        "#### **Fase 3: La Patch Radiografica (L'Input per la UNet)**\n",
        "\n",
        "*   **Immagine di Riferimento:** L'immagine della patch singola che mi hai mostrato.\n",
        "*   **Dimensione:** **512x512 pixel**.\n",
        "*   **Il Problema:** Questa immagine √® il risultato del `resize` della Fase 2. La sua scala in `pixel/mm` **NON √® pi√π 23.62**. √à una nuova scala che dobbiamo calcolare.\n",
        "\n",
        "#### **Fase 4: La Maschera Predetta (L'Output della UNet)**\n",
        "\n",
        "*   **Immagine di Riferimento:** L'immagine della maschera in bianco e nero che mi hai mostrato.\n",
        "*   **Dimensione:** **512x512 pixel** (la stessa della patch di input).\n",
        "*   **Contenuto:** I valori dei pixel rappresentano le classi (es. 0=Sfondo, 1=Foro, 2=Danno).\n",
        "*   **Obiettivo:** Misurare l'area e il DMAX del danno (pixel di classe 2) su questa maschera e convertirli in `mm¬≤` e `mm` usando la scala corretta.\n"
      ],
      "metadata": {
        "id": "eQnAoHCEIt-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    *   **Mediana vs Media:** La tua intuizione sulla mediana √® acuta. Se la distribuzione delle forze per un dato `NumeroForo` non √® simmetrica, predire la media (come fa la MSE loss) potrebbe non essere la scelta migliore. I punti potrebbero essere \"spiaccicati\" (con una maggiore dispersione), e il nostro modello non lo cattura.\n"
      ],
      "metadata": {
        "id": "50zKc6tz3u_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analisi pi√π Rigorosa e Piano d'Azione Rivisto\n",
        "\n",
        "Dato che l'obiettivo finale √® definire un limite di danno accettabile, dobbiamo essere pi√π critici. Invece di accettare ciecamente l'output della MLP, usiamolo come uno strumento esplorativo per rispondere a una domanda pi√π profonda.\n",
        "\n",
        "**Nuovo Obiettivo:** Stabilire se le *caratteristiche della forma del danno* (i momenti di Hu) contengono abbastanza informazione per distinguere tra un processo a \"bassa forza\" e uno ad \"alta forza\", al netto dell'effetto dominante dell'usura (`NumeroForo`).\n",
        "<details>\n",
        "Ecco un approccio pi√π solido e un codice rivisto che si concentra su questa domanda.\n",
        "\n",
        "#### Piano d'Azione (Versione 2.0 - pi√π mirata)\n",
        "1.  **Analisi di Correlazione:** Prima di addestrare un modello complesso, visualizziamo la correlazione tra le nostre features (soprattutto i momenti di Hu) e la `Forza_N`. Questo ci dir√† quali descrittori sono pi√π promettenti.\n",
        "2.  **Feature Importance con un Modello Robusto:** Invece di una MLP (che √® una \"scatola nera\"), usiamo un modello come **Gradient Boosting (es. LightGBM o XGBoost)**. Questi modelli ci forniscono una metrica diretta di **\"Feature Importance\"**, dicendoci quali variabili hanno usato di pi√π per fare le loro predizioni. Questo √® un passo diagnostico fondamentale.\n",
        "3.  **Confronto Modelli:** Addestriamo due modelli:\n",
        "    *   **Modello A (Baseline):** Predice la `Forza_N` usando **solo** `NumeroForo`. Questo ci dice qual √® la performance che otteniamo basandoci solo sull'usura.\n",
        "    *   **Modello B (Completo):** Predice la `Forza_N` usando `NumeroForo` + tutte le features del danno.\n",
        "4.  **Valutazione:** Se il Modello B √® **significativamente migliore** del Modello A, allora abbiamo la prova che **le forme del danno contengono informazione utile sulla forza, al di l√† del semplice effetto di usura.**\n"
      ],
      "metadata": {
        "id": "3x_LYEGC5BSS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZAJqEP8IrAF"
      },
      "outputs": [],
      "source": [
        "### **Script di Calibrazione Automatica della Scala**\n",
        "'''\n",
        "Questo script fa una sola cosa, ma la fa bene:\n",
        "1.  Legge ogni immagine `.jpg` da una cartella di input (le tue patch radiografiche).\n",
        "2.  Usa l'algoritmo di visione artificiale `HoughCircles` per trovare il foro da 6 mm.\n",
        "3.  Calcola la scala `pixel/mm` per quella specifica immagine.\n",
        "4.  Salva tutti i risultati in un file CSV chiamato `calibrazione_scale_patch.csv`.\n",
        "\n",
        "```python\n",
        "'''# =============================================================================\n",
        "# SCRIPT DI CALIBRAZIONE AUTOMATICA DELLA SCALA PER OGNI PATCH\n",
        "# =============================================================================\n",
        "\n",
        "# --- 1. INSTALLAZIONI E IMPORT NECESSARI ---\n",
        "# (Esegui questa cella una sola volta all'inizio)\n",
        "!pip install -q opencv-python pandas matplotlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files # Per scaricare il file CSV alla fine\n",
        "\n",
        "# --- 2. IMPOSTAZIONI (LE UNICHE COSE DA MODIFICARE) ---\n",
        "\n",
        "# Diametro reale del foro in millimetri. Questa √® la nostra \"verit√†\" per la calibrazione.\n",
        "DIAMETRO_REALE_FORO_MM = 6.0\n",
        "\n",
        "# Imposta il percorso alla cartella che contiene le TUE patch radiografiche (.jpg).\n",
        "# Esempio per Google Drive:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# RADIO_PATCHES_DIR = Path(\"/content/drive/MyDrive/PATCHES_RADIO_Dataset\")\n",
        "\n",
        "# Per un test iniziale, puoi caricare le immagini manualmente in una cartella in Colab.\n",
        "RADIO_PATCHES_DIR = Path(\"/content/drive/MyDrive/PATCHES_RADIO_Dataset\")#\"/content/immagini_da_calibrare\")\n",
        "RADIO_PATCHES_DIR.mkdir(exist_ok=True) # Crea la cartella se non esiste\n",
        "print(f\"ATTENZIONE: Lo script cercher√† le immagini in '{RADIO_PATCHES_DIR}'\")\n",
        "print(\"Carica le tue patch .jpg in quella cartella.\")\n",
        "\n",
        "\n",
        "# --- 3. FUNZIONE DI CALIBRAZIONE ---\n",
        "\n",
        "def get_scale_from_radiograph(radiograph_path: Path, visualize=False):\n",
        "    \"\"\"\n",
        "    Carica un'immagine radiografica, trova il foro centrale e calcola la scala pixel/mm.\n",
        "    \"\"\"\n",
        "    if not radiograph_path.exists():\n",
        "        print(f\"Attenzione: Immagine non trovata a {radiograph_path}\")\n",
        "        return None, None\n",
        "\n",
        "    # Carica l'immagine in scala di grigi\n",
        "    image = cv2.imread(str(radiograph_path), cv2.IMREAD_GRAYSCALE)\n",
        "    if image is None:\n",
        "        return None, None\n",
        "\n",
        "    # Applica un leggero blur per ridurre il rumore e aiutare l'algoritmo\n",
        "    blurred_image = cv2.GaussianBlur(image, (9, 9), 2)\n",
        "\n",
        "    # Parametri per l'algoritmo HoughCircles. Potrebbero richiedere aggiustamenti.\n",
        "    # dp: Rapporto inverso della risoluzione dell'accumulatore. 1.2 √® un buon punto di partenza.\n",
        "    # minDist: Distanza minima tra i centri dei cerchi. Mettiamola grande per trovare solo il foro principale.\n",
        "    # param1: Soglia superiore per l'edge detector di Canny interno.\n",
        "    # param2: Soglia per il centro del cerchio. Pi√π √® basso, pi√π \"falsi\" cerchi trova.\n",
        "    # minRadius, maxRadius: Limiti per il raggio del cerchio. FONDAMENTALI!\n",
        "\n",
        "    # Stimiamo un range ragionevole per il raggio basandoci sulle dimensioni dell'immagine\n",
        "    img_height, img_width = image.shape\n",
        "    min_r = int(img_height * 0.15) # min raggio 15% dell'altezza\n",
        "    max_r = int(img_height * 0.40) # max raggio 40% dell'altezza (pi√π flessibile)\n",
        "\n",
        "    circles = cv2.HoughCircles(blurred_image, cv2.HOUGH_GRADIENT, dp=1.2, minDist=img_width,\n",
        "                               param1=70, param2=45, minRadius=min_r, maxRadius=max_r)\n",
        "\n",
        "    if circles is not None:\n",
        "        # Trovato almeno un cerchio!\n",
        "        circles = np.round(circles[0, :]).astype(\"int\")\n",
        "        (x, y, r) = circles[0] # Prendiamo il primo (e unico) cerchio trovato\n",
        "\n",
        "        diametro_pixel = r * 2\n",
        "        scala_px_per_mm = diametro_pixel / DIAMETRO_REALE_FORO_MM\n",
        "\n",
        "        if visualize:\n",
        "            output_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "            cv2.circle(output_image, (x, y), r, (0, 255, 0), 4) # Cerchio verde\n",
        "            cv2.circle(output_image, (x, y), 5, (0, 0, 255), -1) # Punto rosso al centro\n",
        "            testo = f\"Scala: {scala_px_per_mm:.2f} px/mm\"\n",
        "            cv2.putText(output_image, testo, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "            return scala_px_per_mm, output_image\n",
        "\n",
        "        return scala_px_per_mm, None\n",
        "    else:\n",
        "        # Nessun cerchio trovato automaticamente\n",
        "        return None, image if visualize else None\n",
        "\n",
        "# --- 4. BLOCCO DI ESECUZIONE ---\n",
        "\n",
        "# Prendi tutte le immagini .jpg nella cartella\n",
        "all_radio_paths = sorted(list(RADIO_PATCHES_DIR.glob(\"*.jpg\")))\n",
        "if not all_radio_paths:\n",
        "    all_radio_paths = sorted(list(RADIO_PATCHES_DIR.glob(\"*.png\"))) # Prova anche i png\n",
        "\n",
        "if not all_radio_paths:\n",
        "    print(f\"‚ùå ERRORE: Nessuna immagine .jpg o .png trovata nella cartella '{RADIO_PATCHES_DIR}'.\")\n",
        "    print(\"Assicurati di aver caricato le immagini prima di eseguire questa cella.\")\n",
        "else:\n",
        "    print(f\"Trovate {len(all_radio_paths)} patch. Inizio calibrazione...\")\n",
        "\n",
        "    # Visualizziamo un esempio per un controllo di qualit√†\n",
        "    print(\"\\n--- TEST DI VISUALIZZAZIONE SULLA PRIMA IMMAGINE ---\")\n",
        "    scala_test, img_test = get_scale_from_radiograph(all_radio_paths[0], visualize=True)\n",
        "    if scala_test:\n",
        "        print(f\"Scala calcolata per '{all_radio_paths[0].name}': {scala_test:.2f} px/mm\")\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(cv2.cvtColor(img_test, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(\"Visualizzazione Rilevamento Cerchio\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Rilevamento fallito sull'immagine di test. Prova a modificare i parametri di HoughCircles (param2, minRadius, maxRadius).\")\n",
        "\n",
        "    # Eseguiamolo su tutte le immagini per creare il nostro file di calibrazione\n",
        "    print(\"\\n--- CALIBRAZIONE DI MASSA SULL'INTERO DATASET ---\")\n",
        "    calibration_data = []\n",
        "    for path in tqdm(all_radio_paths, desc=\"Calibrazione Immagini\"):\n",
        "        scala, _ = get_scale_from_radiograph(path, visualize=False)\n",
        "\n",
        "        entry = {'filename': path.name}\n",
        "        if scala:\n",
        "            entry['scala_px_per_mm'] = scala\n",
        "        else:\n",
        "            entry['scala_px_per_mm'] = np.nan # Mettiamo NaN se il rilevamento fallisce\n",
        "        calibration_data.append(entry)\n",
        "\n",
        "    # Converti i risultati in un DataFrame di Pandas\n",
        "    df_calibrazione = pd.DataFrame(calibration_data)\n",
        "\n",
        "    output_filename = \"calibrazione_scale_patch.csv\"\n",
        "    df_calibrazione.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(\"\\n--- RISULTATI CALIBRAZIONE ---\")\n",
        "    print(df_calibrazione.head())\n",
        "    print(f\"\\n‚úÖ File di calibrazione '{output_filename}' creato con successo!\")\n",
        "\n",
        "    # Controlliamo se ci sono stati fallimenti\n",
        "    fallimenti = df_calibrazione['scala_px_per_mm'].isna().sum()\n",
        "    if fallimenti > 0:\n",
        "        print(f\"\\n‚ö†Ô∏è ATTENZIONE: {fallimenti} immagini non sono state calibrate correttamente.\")\n",
        "    else:\n",
        "        print(\"\\nüéâ OTTIMO: Tutte le immagini sono state calibrate con successo.\")\n",
        "\n",
        "    # Analizziamo la distribuzione delle scale\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    df_calibrazione['scala_px_per_mm'].hist(bins=30, edgecolor='black')\n",
        "    plt.title(\"Distribuzione delle Scale Calcolate (px/mm)\")\n",
        "    plt.xlabel(\"Scala (pixel/mm)\")\n",
        "    plt.ylabel(\"Numero di Patch\")\n",
        "    plt.show()\n",
        "\n",
        "    # Offri il download del file CSV\n",
        "    files.download(output_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ora si applica alle maschere della unet"
      ],
      "metadata": {
        "id": "r9XhTzIZhXsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SCRIPT DI ESTRAZIONE FEATURES \"INTELLIGENTE\" CON SCALA CORRETTA\n",
        "# =============================================================================\n",
        "\n",
        "# --- 1. INSTALLAZIONI E IMPORT NECESSARI ---\n",
        "!pip install -q opencv-python pandas\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# --- 2. IMPOSTAZIONI (CONTROLLA QUESTI PERCORSI) ---\n",
        "\n",
        "# Percorso al file CSV con le scale che hai appena generato\n",
        "CALIBRATION_FILE_PATH = Path(\"/content/calibrazione_scale_patch.csv\")\n",
        "\n",
        "# Percorso alla cartella che contiene le TUE maschere predette dalla UNet (.png)\n",
        "MASKS_DIR = Path(\"/content/drive/MyDrive/PATCHES_RADIO_Dataset\")\n",
        "MASKS_DIR.mkdir(exist_ok=True) # Crea la cartella se non esiste\n",
        "print(f\"ATTENZIONE: Lo script cercher√† le maschere in '{MASKS_DIR}'\")\n",
        "print(\"Carica le tue maschere .png in quella cartella.\")\n",
        "\n",
        "# --- 3. CARICAMENTO DATI DI CALIBRAZIONE ---\n",
        "try:\n",
        "    df_calibrazione = pd.read_csv(CALIBRATION_FILE_PATH)\n",
        "    # Creiamo un dizionario per una ricerca super veloce della scala\n",
        "    # Assumiamo che il filename della maschera derivi da quello della patch\n",
        "    # Es: H001_h001_..._pred.png -> H001_h001_....jpg\n",
        "    # Quindi creiamo una chiave pulita\n",
        "    def get_clean_key(filename):\n",
        "        return filename.replace('.png', '.jpg')\n",
        "\n",
        "    scale_dict = {row['filename']: row['scala_px_per_mm'] for _, row in df_calibrazione.iterrows()}\n",
        "    print(f\"‚úÖ File di calibrazione caricato con successo. Contiene {len(scale_dict)} scale.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå ERRORE: File di calibrazione non trovato in '{CALIBRATION_FILE_PATH}'. Assicurati che sia presente.\")\n",
        "    raise\n",
        "\n",
        "# --- 4. FUNZIONE DI ESTRAZIONE FEATURES (AGGIORNATA) ---\n",
        "\n",
        "def extract_calibrated_features(mask_path, scale_lookup):\n",
        "    \"\"\"\n",
        "    Estrae le features da una maschera usando la scala di calibrazione corretta.\n",
        "    \"\"\"\n",
        "    # Ricava il nome del file della radiografia originale per cercare la scala\n",
        "    original_radio_filename = mask_path.name.replace('.png', '.jpg')\n",
        "\n",
        "    # Prendi la scala corretta dal nostro dizionario\n",
        "    scala = scale_lookup.get(original_radio_filename)\n",
        "\n",
        "    if scala is None or pd.isna(scala):\n",
        "        # Se non troviamo una scala per questa immagine, la saltiamo\n",
        "        return None\n",
        "\n",
        "    # Carica la maschera\n",
        "    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None:\n",
        "        return None\n",
        "\n",
        "    # Isola solo la maschera della delaminazione (classe 2)\n",
        "    delam_mask = (mask == 2).astype(np.uint8)\n",
        "\n",
        "    # --- Calcolo Area ---\n",
        "    area_delam_px = cv2.countNonZero(delam_mask)\n",
        "    area_delam_mm2 = area_delam_px / (scala ** 2)\n",
        "\n",
        "    # --- Calcolo DMAX ---\n",
        "    contours, _ = cv2.findContours(delam_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    d_max_px = 0\n",
        "    if contours:\n",
        "        # Unisci tutti i punti di tutti i contorni in un unico array\n",
        "        all_points = np.concatenate(contours, axis=0)\n",
        "        # Troviamo il rettangolo rotato con area minima che contiene tutti i punti\n",
        "        rect = cv2.minAreaRect(all_points)\n",
        "        # La diagonale di questo rettangolo √® una buona stima di DMAX\n",
        "        box = cv2.boxPoints(rect)\n",
        "        dist1 = np.linalg.norm(box[0] - box[2])\n",
        "        dist2 = np.linalg.norm(box[1] - box[3])\n",
        "        d_max_px = max(dist1, dist2)\n",
        "\n",
        "    d_max_mm = d_max_px / scala\n",
        "\n",
        "    # (Opzionale, puoi aggiungere qui i Momenti di Hu o altri calcoli se servono)\n",
        "\n",
        "    return {\n",
        "        'FileMaschera': mask_path.name,\n",
        "        'AreaDelaminata_mm2': area_delam_mm2,\n",
        "        'Dmax_mm': d_max_mm,\n",
        "        'Scala_Usata': scala\n",
        "    }\n",
        "\n",
        "# --- 5. BLOCCO DI ESECUZIONE ---\n",
        "all_mask_paths = sorted({\n",
        "    *MASKS_DIR.rglob(\"*.png\"),\n",
        "    *MASKS_DIR.rglob(\"*.jpg\"),\n",
        "    *MASKS_DIR.rglob(\"*.jpeg\"),\n",
        "})\n",
        "if not all_mask_paths:\n",
        "    print(f\"‚ùå ERRORE: Nessuna maschera .png trovata in '{MASKS_DIR}'.\")\n",
        "else:\n",
        "    print(f\"\\nTrovate {len(all_mask_paths)} maschere. Inizio estrazione features corrette...\")\n",
        "\n",
        "    features_list = []\n",
        "    for path in tqdm(all_mask_paths, desc=\"Estrazione Features Calibrate\"):\n",
        "        feats = extract_calibrated_features(path, scale_dict)\n",
        "        if feats:\n",
        "            features_list.append(feats)\n",
        "\n",
        "    df_features_corrette = pd.DataFrame(features_list)\n",
        "\n",
        "    output_filename = \"features_corrette_e_verificate.csv\"\n",
        "    df_features_corrette.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(\"\\n--- RISULTATI ESTRAZIONE CORRETTA ---\")\n",
        "    print(df_features_corrette.head())\n",
        "    print(f\"\\n‚úÖ File con features corrette '{output_filename}' creato con successo!\")\n",
        "\n",
        "    # Offri il download\n",
        "    files.download(output_filename)"
      ],
      "metadata": {
        "id": "ZW0yem-WhbZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crezione del file csv dei dati di lavoro definitivo, mi fa merge di quelle maschere unet e del file di origine di Melis"
      ],
      "metadata": {
        "id": "I84eyFhTmNaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### **Notebook Definitivo: Creazione del Dataset Master Comparativo**\n",
        "\n",
        "#### **CELLA 1: Setup, Caricamento Dati e Costanti**\n",
        "'''*Questa cella imposta l'ambiente, definisce le costanti del nostro universo e carica tutti i file di input necessari.*\n",
        "\n",
        "```python'''\n",
        "# =============================================================================\n",
        "# CELLA 1: SETUP, CARICAMENTO DATI E COSTANTI\n",
        "# =============================================================================\n",
        "\n",
        "# --- 1. Installazioni e Import ---\n",
        "!pip install -q opencv-python pandas\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import re\n",
        "from google.colab import files\n",
        "\n",
        "print(\"--- Ambiente Pronto ---\")\n",
        "\n",
        "# --- 2. Costanti Fisiche e di Progetto ---\n",
        "D_NOMINALE_MM = 6.0\n",
        "AREA_NOMINALE_MM2 = np.pi * (D_NOMINALE_MM / 2)**2\n",
        "print(f\"Costanti definite: D_nom = {D_NOMINALE_MM} mm, A_nom = {AREA_NOMINALE_MM2:.3f} mm^2\")\n",
        "\n",
        "\n",
        "# --- 3. Percorsi dei File di Input (CORRETTI) ---\n",
        "CALIBRATION_FILE_PATH = Path(\"/content/calibrazione_scale_patch.csv\")\n",
        "\n",
        "\n",
        "# QUESTI SONO I LINK AI FILE \"RAW\" (Grezzi)\n",
        "FORZA_URL_RAW = \"https://github.com/Riccardo-Venturi/DatiBuchi/raw/main/forze%20T_0_90.xlsx\"\n",
        "GEOMETRIA_MELIS_URL_RAW = \"https://github.com/Riccardo-Venturi/DatiBuchi/raw/main/Aree%20Delaminazioni%20T_0_90_rev1.xlsx\"\n",
        "\n",
        "\n",
        "# --- 4. Caricamento e Preparazione dei Dati di Input ---\n",
        "try:\n",
        "    # Dati di Calibrazione (da file locale)\n",
        "    df_calibrazione = pd.read_csv(CALIBRATION_FILE_PATH)\n",
        "    scale_dict = {row['filename']: row['scala_px_per_mm'] for _, row in df_calibrazione.iterrows()}\n",
        "    print(f\"‚úÖ File di calibrazione caricato ({len(scale_dict)} voci).\")\n",
        "\n",
        "    # Dati di Forza di Maurizio (da URL raw usando pd.read_excel)\n",
        "    df_forza_maurizio = pd.read_excel(FORZA_URL_RAW, engine='openpyxl',header=1)\n",
        "    # Puliamo i dati, prendendo solo le colonne che ci servono\n",
        "    df_forza_maurizio = df_forza_maurizio[['Foro', 'Fcorretta']].rename(columns={'Foro': 'NumeroForo', 'Fcorretta': 'Forza_N'})\n",
        "    print(f\"‚úÖ File delle forze di Maurizio caricato ({len(df_forza_maurizio)} voci).\")\n",
        "#    # Dati Geometrici di Maurizio\n",
        "#    df_geometria_maurizio = pd.read_excel(GEOMETRIA_MELIS_URL_RAW,engine='openpyxl')\n",
        "#    print(f\"‚úÖ File di geometria di Maurizio caricato ({len(df_geometria_maurizio)} voci).\")\n",
        "#\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERRORE: Qualcosa √® andato storto nel caricamento dei file. Dettagli: {e}\")\n",
        "    raise\n"
      ],
      "metadata": {
        "id": "z5emrEnrmVtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(MASKS_DIR)"
      ],
      "metadata": {
        "id": "H3MBpQwbKoJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# CELLA 3: ESTRAZIONE FEATURES GEOMETRICHE CALIBRATE (UNET)\n",
        "# =============================================================================\n",
        "\n",
        "def extract_hole_number(path):\n",
        "    match = re.search(r'H(\\d+)', path.stem)\n",
        "    return int(match.group(1)) if match else -1\n",
        "\n",
        "def extract_geometric_features(mask_path, scale_lookup):\n",
        "    original_radio_filename = mask_path.name.replace('.png', '.jpg')\n",
        "    scala = scale_lookup.get(original_radio_filename)\n",
        "    if scala is None or pd.isna(scala): return None\n",
        "\n",
        "    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None: return None\n",
        "\n",
        "    numero_foro = extract_hole_number(mask_path)\n",
        "    delam_mask = (mask == 2).astype(np.uint8)\n",
        "\n",
        "    # Calcoli geometrici\n",
        "    area_delam_px = cv2.countNonZero(delam_mask)\n",
        "    area_delam_mm2 = area_delam_px / (scala ** 2)\n",
        "\n",
        "    d_max_mm = 0\n",
        "    contours, _ = cv2.findContours(delam_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if contours:\n",
        "        all_points = np.concatenate(contours, axis=0)\n",
        "        if len(all_points) > 0:\n",
        "            rect = cv2.minAreaRect(all_points)\n",
        "            box = cv2.boxPoints(rect)\n",
        "            d_max_px = max(np.linalg.norm(box[0] - box[2]), np.linalg.norm(box[1] - box[3]))\n",
        "            d_max_mm = d_max_px / scala\n",
        "\n",
        "    # Feature ingegnerizzate\n",
        "    df_diametro = d_max_mm / D_NOMINALE_MM\n",
        "    area_cerchio_max_mm2 = np.pi * (d_max_mm / 2)**2\n",
        "    denominatore_sf = area_cerchio_max_mm2 - AREA_NOMINALE_MM2\n",
        "    shape_factor = area_delam_mm2 / denominatore_sf if denominatore_sf > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'NumeroForo': numero_foro,\n",
        "        'AreaDelaminata_unet': area_delam_mm2,\n",
        "        'Dmax_unet': d_max_mm,\n",
        "        'DF_unet': df_diametro,\n",
        "        'ShapeFactor_unet': shape_factor,\n",
        "        'FileMaschera': mask_path.name,\n",
        "    }\n",
        "\n",
        "# Eseguiamo l'estrazione\n",
        "all_mask_paths = sorted({\n",
        "    *MASKS_DIR.rglob(\"*.png\"),\n",
        "    *MASKS_DIR.rglob(\"*.jpg\"),\n",
        "    *MASKS_DIR.rglob(\"*.jpeg\"),\n",
        "})\n",
        "if not all_mask_paths:\n",
        "    print(f\"‚ùå ERRORE: Nessuna maschera trovata in '{MASKS_DIR}'.\")\n",
        "else:\n",
        "    geometric_features_list = []\n",
        "    for path in tqdm(all_mask_paths, desc=\"Estraggo Geometria UNet\"):\n",
        "        feats = extract_geometric_features(path, scale_dict)\n",
        "        if feats:\n",
        "            geometric_features_list.append(feats)\n",
        "\n",
        "    df_unet_geometric = pd.DataFrame(geometric_features_list)\n",
        "    print(\"\\n--- Dataset Geometrico (UNet) Calibrato ---\")\n",
        "    display(df_unet_geometric.head())\n",
        "\n",
        "#### **CELLA 4: Calcolo Momenti di Hu (Cella Separata)**\n",
        "'''*Come richiesto, una cella dedicata a calcolare i 7 Momenti di Hu, che sono ottimi descrittori di forma.*\n",
        "\n",
        "```python'''\n",
        "# =============================================================================\n",
        "# CELLA 4: CALCOLO MOMENTI DI HU\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_hu_moments(mask_path):\n",
        "    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None: return None\n",
        "\n",
        "    numero_foro = extract_hole_number(mask_path)\n",
        "    delam_mask = (mask == 2).astype(np.uint8)\n",
        "\n",
        "    moments = cv2.moments(delam_mask)\n",
        "    hu_moments = cv2.HuMoments(moments).flatten()\n",
        "    hu_log = np.sign(hu_moments) * np.log10(np.abs(hu_moments) + 1e-7) # Stabilizzazione logaritmica\n",
        "\n",
        "    return {\n",
        "        'NumeroForo': numero_foro,\n",
        "        'Hu_1': hu_log[0], 'Hu_2': hu_log[1], 'Hu_3': hu_log[2], 'Hu_4': hu_log[3],\n",
        "        'Hu_5': hu_log[4], 'Hu_6': hu_log[5], 'Hu_7': hu_log[6],\n",
        "    }\n",
        "\n",
        "# Eseguiamo l'estrazione\n",
        "hu_features_list = []\n",
        "for path in tqdm(all_mask_paths, desc=\"Calcolo Momenti di Hu\"):\n",
        "    hu_feats = calculate_hu_moments(path)\n",
        "    if hu_feats:\n",
        "        hu_features_list.append(hu_feats)\n",
        "\n",
        "df_unet_hu = pd.DataFrame(hu_features_list)\n",
        "print(\"\\n--- Dataset Momenti di Hu (UNet) ---\")\n",
        "display(df_unet_hu.head())"
      ],
      "metadata": {
        "id": "XEozVj7l3QQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELLA 5: UNIONE FINALE E SALVATAGGIO (senza geometrie umane)  ‚úÖ\n",
        "# =============================================================================\n",
        "print(\"--- Unione di UNet (geometria+Hu) con le Forze ---\")\n",
        "\n",
        "# 0) Protezione: tieni solo fori riconosciuti (>0) dalle maschere\n",
        "df_unet_geometric = df_unet_geometric[df_unet_geometric[\"NumeroForo\"].fillna(-1).astype(int) > 0]\n",
        "df_unet_hu        = df_unet_hu[df_unet_hu[\"NumeroForo\"].fillna(-1).astype(int) > 0]\n",
        "\n",
        "# 1) UNET: merge interno tra geometria e Hu\n",
        "df_unet_completo = df_unet_geometric.merge(df_unet_hu, on=\"NumeroForo\", how=\"inner\")\n",
        "\n",
        "# 2) FORZE: normalizza chiave e deduplica\n",
        "df_forza_maurizio = (\n",
        "    df_forza_maurizio\n",
        "      .assign(NumeroForo=(df_forza_maurizio[\"NumeroForo\"]\n",
        "                          .astype(str).str.extract(r\"(\\d+)\").squeeze()))\n",
        "      .dropna(subset=[\"NumeroForo\"])\n",
        ")\n",
        "df_forza_maurizio[\"NumeroForo\"] = df_forza_maurizio[\"NumeroForo\"].astype(\"int64\")\n",
        "df_forza_maurizio = df_forza_maurizio.drop_duplicates(subset=[\"NumeroForo\"], keep=\"first\")\n",
        "\n",
        "# 3) MERGE finale: tieni tutti i fori che hanno features UNet, attacca la Forza se c‚Äô√®\n",
        "#    (on deve essere colonna presente in ENTRAMBI i DataFrame)\n",
        "df_master = df_unet_completo.merge(\n",
        "    df_forza_maurizio[[\"NumeroForo\",\"Forza_N\"]],\n",
        "    on=\"NumeroForo\", how=\"left\"\n",
        ")\n",
        "\n",
        "# 4) Ordina/riordina colonne e salva\n",
        "df_master = df_master.sort_values(\"NumeroForo\").reset_index(drop=True)\n",
        "colonne_ordinate = [\n",
        "    \"NumeroForo\", \"Forza_N\",\n",
        "    \"AreaDelaminata_unet\", \"Dmax_unet\", \"DF_unet\", \"ShapeFactor_unet\",\n",
        "    \"Hu_1\",\"Hu_2\",\"Hu_3\",\"Hu_4\",\"Hu_5\",\"Hu_6\",\"Hu_7\",\n",
        "    \"FileMaschera\"\n",
        "]\n",
        "df_master = df_master[[c for c in colonne_ordinate if c in df_master.columns]]\n",
        "output_filename = \"dataset_master_finalissimo.csv\"\n",
        "df_master.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Salvato: {output_filename}\")\n",
        "print(f\"Righe: {len(df_master)} | Forze mancanti: {df_master['Forza_N'].isna().sum()}\")\n",
        "\n",
        "display(df_master.head(10))\n"
      ],
      "metadata": {
        "id": "h27-DF06-NcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questo √® un classico problema di **regressione** e di **imputazione dei dati tramite modello**. Procediamo passo dopo passo.\n",
        "si passa alla parte di chiussura del dataset\n",
        "1) attiva mlp per forze da geometria\n",
        "2) attiva ltsm per danno"
      ],
      "metadata": {
        "id": "HgCNC37bboCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ecco il codice Python che realizza esattamente questo. Utilizzeremo `pandas` per la manipolazione dei dati, `scikit-learn` per il pre-processing e la divisione dei dati, e `tensorflow/keras` per costruire e addestrare la nostra MLP.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import re # Per estrarre il tipo di materiale dal nome del file\n"
      ],
      "metadata": {
        "id": "0WUD4CHwb1Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# 1. Caricamento e Preparazione dei Dati\n",
        "# --------------------------------------------\n",
        "# Assicurati che il file si chiami 'dati_completi.csv' o modifica il nome\n",
        "# Ho ipotizzato che il file sia nella stessa cartella dello script.\n",
        "try:\n",
        "    df = pd.read_csv('/content/dataset_master_finalissimo.csv') # Sostituisci con il nome del tuo file\n",
        "except FileNotFoundError:\n",
        "    print(\"Errore: File non trovato. Assicurati che il nome del file sia corretto e che si trovi nella giusta directory.\")\n",
        "    exit()\n",
        "\n",
        "print(\"Dataset caricato con successo.\")\n",
        "print(f\"Dimensioni del dataset: {df.shape}\")\n",
        "\n",
        "# Funzione per estrarre il tipo di materiale\n",
        "def get_material_type(filename):\n",
        "    # Cerca un pattern tipo \"Carbon Textile XY\"\n",
        "    match = re.search(r'Carbon Textile \\d[A-Z]', str(filename))\n",
        "    if match:\n",
        "        return match.group(0)\n",
        "    return \"Unknown\" # In caso non trovi il pattern\n",
        "\n",
        "# Crea la colonna 'TipoMateriale' che √® una feature FONDAMENTALE\n",
        "df['TipoMateriale'] = df['FileMaschera'].apply(get_material_type)\n",
        "\n",
        "print(\"\\nValori unici in 'TipoMateriale':\")\n",
        "print(df['TipoMateriale'].value_counts())\n"
      ],
      "metadata": {
        "id": "wscL4jHYb2zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Divisione del Dataset\n",
        "# --------------------------------------------\n",
        "# Dividiamo il dataframe in due: uno con le forze note (per addestrare)\n",
        "# e uno con le forze mancanti (su cui predire).\n",
        "\n",
        "df_con_forza = df[df['Forza_N'].notna()].copy()\n",
        "df_senza_forza = df[df['Forza_N'].isna()].copy()\n",
        "\n",
        "print(f\"\\nNumero di campioni con forza nota (per training/validation): {len(df_con_forza)}\")\n",
        "print(f\"Numero di campioni con forza mancante (per predizione): {len(df_senza_forza)}\")\n"
      ],
      "metadata": {
        "id": "d9sP4DUAb2_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Definizione delle Feature e del Target\n",
        "# --------------------------------------------\n",
        "# Definiamo le colonne che useremo come input (features) e output (target)\n",
        "# Il target √® la forza, le features sono tutto il resto che descrive il danno.\n",
        "\n",
        "features_numeriche = [\n",
        "    'AreaDelaminata_unet', 'Dmax_unet', 'DF_unet', 'ShapeFactor_unet',\n",
        "    'Hu_1', 'Hu_2', 'Hu_3', 'Hu_4', 'Hu_5', 'Hu_6', 'Hu_7'\n",
        "]\n",
        "features_categoriche = ['TipoMateriale']\n",
        "\n",
        "# Variabili indipendenti (X) e dipendente (y)\n",
        "X = df_con_forza[features_numeriche + features_categoriche]\n",
        "y = df_con_forza['Forza_N']\n"
      ],
      "metadata": {
        "id": "PTT2WLHMcw59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Creazione della Pipeline di Pre-processing\n",
        "# --------------------------------------------\n",
        "# Questa pipeline gestir√† la standardizzazione delle feature numeriche\n",
        "# e la codifica one-hot di quelle categoriche in modo automatico e sicuro.\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), features_numeriche),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), features_categoriche)\n",
        "    ])"
      ],
      "metadata": {
        "id": "rpvN_IMdc09G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Divisione del set di dati noti in Training e Validation\n",
        "# -------------------------------------------------------------\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nDimensioni del set di addestramento: {X_train.shape}\")\n",
        "print(f\"Dimensioni del set di validazione: {X_val.shape}\")\n"
      ],
      "metadata": {
        "id": "Lnfr6xRQc8c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verifica Quantitativa dell'Importanza delle Features (il test decisivo):**\n",
        "    *   **Testo:** \"Per quantificare oggettivamente l'importanza predittiva di ciascuna variabile, sono stati confrontati due modelli Gradient Boosting (LightGBM): un modello 'Baseline' addestrato solo su `NumeroForo` e un modello 'Completo' addestrato su tutte le features.\"\n",
        "    <details>*   **Snippet di Codice:**\n",
        "        ```python\n",
        "        # Snippet 2: Confronto Modelli LGBM\n",
        "        import lightgbm as lgb\n",
        "        from sklearn.metrics import mean_squared_error\n",
        "        import numpy as np\n",
        "\n",
        "        df_known_force = df[df['Forza_N'].notna()]\n",
        "        X = df_known_force[features]\n",
        "        y = df_known_force[target]\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Modello Completo\n",
        "        lgbm_full = lgb.LGBMRegressor(random_state=42, verbosity=-1) # verbosity=-1 per pulire i warning\n",
        "        lgbm_full.fit(X_train, y_train)\n",
        "        rmse_full = np.sqrt(mean_squared_error(y_test, lgbm_full.predict(X_test)))\n",
        "\n",
        "        # Modello Baseline\n",
        "        lgbm_base = lgb.LGBMRegressor(random_state=42, verbosity=-1)\n",
        "        lgbm_base.fit(X_train[['NumeroForo']], y_train)\n",
        "        rmse_base = np.sqrt(mean_squared_error(y_test, lgbm_base.predict(X_test[['NumeroForo']])))\n",
        "\n",
        "        print(f\"RMSE Modello Completo: {rmse_full:.2f} N\")\n",
        "        print(f\"RMSE Modello Baseline: {rmse_base:.2f} N\")\n",
        "        ```\n",
        "    *   **Risultato:** Inserisci il grafico della **Feature Importance del modello completo**.\n",
        "    *   **Commento:** \"I risultati sono stati inequivocabili. Il modello Baseline ha ottenuto un RMSE di 6.11 N, mentre il modello Completo ha ottenuto un RMSE di 6.61 N, indicando un **peggioramento della performance del 8.13%**. Il grafico di Feature Importance (Figura X) conferma che le features geometriche del danno forniscono un contributo predittivo trascurabile una volta che l'effetto dell'usura √® noto.\"\n"
      ],
      "metadata": {
        "id": "bPPpJOleiMUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Assumo che 'full_data.csv' sia disponibile con i tuoi dati\n",
        "try:\n",
        "    df = pd.read_csv('/content/dataset_master_finalissimo.csv', sep=',')\n",
        "except FileNotFoundError:\n",
        "    print(\"ERRORE: Assicurati che il file '/content/dataset_master_finalissimo.csv' sia nella stessa cartella.\")\n",
        "    # Fallback su dati di esempio se non trova il file\n",
        "    df = pd.DataFrame({\n",
        "        'NumeroForo': range(1, 11),\n",
        "        'Forza_N': np.linspace(90, 110, 10),\n",
        "        'AreaDelaminata_unet': np.random.rand(10) * 0.1,\n",
        "        'Dmax_unet': np.random.rand(10) * 2 + 8,\n",
        "        'DF_unet': np.random.rand(10) * 0.5 + 1,\n",
        "        'ShapeFactor_unet': np.random.rand(10) * 0.005,\n",
        "        'Hu_1': np.random.rand(10) * 2, 'Hu_2': np.random.rand(10) * 4, 'Hu_3': np.random.rand(10) * 6,\n",
        "        'Hu_4': np.random.rand(10) * 6, 'Hu_5': np.random.rand(10) * 20 - 10, 'Hu_6': np.random.rand(10) * 20 - 10,\n",
        "        'Hu_7': np.random.rand(10) * 20 - 10\n",
        "    })\n",
        "\n",
        "# --- PASSO 0: Preparazione ---\n",
        "df['Forza_N'] = pd.to_numeric(df['Forza_N'], errors='coerce')\n",
        "df_known_force = df[df['Forza_N'].notna()].copy()\n",
        "\n",
        "features = [\n",
        "    'NumeroForo',\n",
        "    'AreaDelaminata_unet', 'Dmax_unet', 'DF_unet', 'ShapeFactor_unet',\n",
        "    'Hu_1', 'Hu_2', 'Hu_3', 'Hu_4', 'Hu_5', 'Hu_6', 'Hu_7'\n",
        "]\n",
        "target = 'Forza_N'\n",
        "\n",
        "X = df_known_force[features]\n",
        "y = df_known_force[target]\n",
        "\n",
        "# --- PASSO 1: Analisi di Correlazione ---\n",
        "correlation_matrix = df_known_force[features + [target]].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\",\n",
        "            annot_kws={\"size\": 8})\n",
        "plt.title('Matrice di Correlazione tra Features e Forza', fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCorrelazione delle features con la Forza:\")\n",
        "print(correlation_matrix[target].sort_values(ascending=False))\n",
        "\n",
        "\n",
        "# --- PASSO 2: Feature Importance con LightGBM ---\n",
        "\n",
        "# Divisione dati\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Modello B (Completo)\n",
        "lgbm_full = lgb.LGBMRegressor(random_state=42)\n",
        "lgbm_full.fit(X_train, y_train)\n",
        "\n",
        "# Valutazione Modello B\n",
        "preds_full = lgbm_full.predict(X_test)\n",
        "rmse_full = np.sqrt(mean_squared_error(y_test, preds_full))\n",
        "print(f\"\\nRMSE del Modello Completo (con tutte le features): {rmse_full:.4f} N\")\n",
        "\n",
        "# Visualizzazione Feature Importance\n",
        "lgb.plot_importance(lgbm_full, figsize=(10, 8), title=\"Feature Importance (Modello Completo)\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- PASSO 3: Confronto con Modello Baseline ---\n",
        "\n",
        "# Modello A (Baseline - solo usura)\n",
        "X_train_base = X_train[['NumeroForo']]\n",
        "X_test_base = X_test[['NumeroForo']]\n",
        "\n",
        "lgbm_base = lgb.LGBMRegressor(random_state=42)\n",
        "lgbm_base.fit(X_train_base, y_train)\n",
        "\n",
        "# Valutazione Modello A\n",
        "preds_base = lgbm_base.predict(X_test_base)\n",
        "rmse_base = np.sqrt(mean_squared_error(y_test, preds_base))\n",
        "print(f\"RMSE del Modello Baseline (solo con NumeroForo): {rmse_base:.4f} N\")\n",
        "\n",
        "# --- PASSO 4: Discussione ---\n",
        "improvement = (rmse_base - rmse_full) / rmse_base * 100\n",
        "print(f\"\\nAggiungere le features del danno ha migliorato la predizione del {improvement:.2f}%.\")\n",
        "\n",
        "if improvement > 5: # Soglia arbitraria\n",
        "    print(\"\\nCONCLUSIONE: S√¨, le caratteristiche della forma del danno (momenti, area, etc.)\")\n",
        "    print(\"contengono informazioni preziose per stimare la forza, anche dopo aver considerato l'usura.\")\n",
        "    print(\"Questo supporta l'idea di poter definire un 'limite di danno accettabile' basato sulla sua forma.\")\n",
        "else:\n",
        "    print(\"\\nCONCLUSIONE: L'effetto dominante √® l'usura ('NumeroForo').\")\n",
        "    print(\"Le caratteristiche del danno aggiungono poche informazioni aggiuntive per predire la forza.\")\n",
        "    print(\"Sar√† pi√π difficile definire un limite di danno basandosi solo sulla sua forma, indipendentemente dall'usura.\")"
      ],
      "metadata": {
        "id": "UgF4PZ-x58UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##:fine sopra preparazione dati\n",
        "\n",
        "# **Sott::o inizio settaggio modello**!!:"
      ],
      "metadata": {
        "id": "DThyIJjgdGau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#modello MLP usato all'inizio, ma mancano picchi, sembra seguire medie, usaimo un modello diverso che √® pi√π centrato sulla relazione fra metriche per capire le correlazioni fra forza e danno"
      ],
      "metadata": {
        "id": "x09_LO3w5x3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Costruzione del Modello MLP (Multi-Layer Perceptron)\n",
        "# --------------------------------------------------------\n",
        "# Creiamo una pipeline che prima processa i dati e poi li passa al modello\n",
        "# Definiamo un'architettura MLP semplice ma efficace.\n",
        "\n",
        "# Trasformiamo i dati di training e validation usando il preprocessor\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_val_processed = preprocessor.transform(X_val)\n",
        "\n",
        "# Definiamo il modello Keras\n",
        "model = tf.keras.Sequential([\n",
        "    # Il numero di neuroni nel primo strato √® basato sulla forma dei dati processati\n",
        "    tf.keras.layers.Input(shape=(X_train_processed.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2), # Dropout per ridurre l'overfitting\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1) # Strato di output con 1 neurone per la regressione\n",
        "])\n",
        "\n",
        "# Compiliamo il modello\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='mean_squared_error', # Ottima loss per la regressione\n",
        "    metrics=['mean_absolute_error'] # Metrica pi√π interpretabile (errore medio in N)\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Aggiungiamo un early stopping per fermare l'addestramento quando il modello smette di migliorare\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=15, restore_best_weights=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "5Lr14Sk5dLQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Addestramento del Modello\n",
        "# --------------------------------------------------------\n",
        "print(\"\\nInizio addestramento MLP...\")\n",
        "history = model.fit(\n",
        "    X_train_processed, y_train,\n",
        "    epochs=200, # Numero massimo di epoche\n",
        "    validation_data=(X_val_processed, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Addestramento completato.\")\n"
      ],
      "metadata": {
        "id": "Uwp7pQSGdlmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Valutazione del Modello\n",
        "# --------------------------------------------------------\n",
        "# Visualizziamo le curve di apprendimento per assicurarci che non ci sia overfitting\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss del Modello')\n",
        "plt.xlabel('Epoca')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mean_absolute_error'], label='Training MAE')\n",
        "plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n",
        "plt.title('Errore Assoluto Medio (MAE)')\n",
        "plt.xlabel('Epoca')\n",
        "plt.ylabel('Forza (N)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uCGEwgfLdmq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Predizione delle Forze Mancanti\n",
        "# --------------------------------------------------------\n",
        "# Usiamo il modello addestrato per predire i valori sul set df_senza_forza\n",
        "X_da_predire = df_senza_forza[features_numeriche + features_categoriche]\n",
        "\n",
        "# Processiamo i dati da predire con lo stesso preprocessor\n",
        "X_da_predire_processed = preprocessor.transform(X_da_predire)\n",
        "\n",
        "# Eseguiamo la predizione\n",
        "forze_predette = model.predict(X_da_predire_processed).flatten() # .flatten() per avere un array 1D\n"
      ],
      "metadata": {
        "id": "P3q_C6n0dtE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Creazione del DataFrame Finale\n",
        "# ---------------------------------------------------------\n",
        "# Riempiamo i valori NaN nel dataframe originale con le nostre predizioni\n",
        "df_finale = df.copy()\n",
        "df_finale.loc[df_finale['Forza_N'].isna(), 'Forza_N'] = forze_predette\n",
        "\n",
        "# Visualizziamo alcuni dei valori imputati per controllo\n",
        "print(\"\\nDataFrame originale con forze mancanti (prime 5 righe affette):\")\n",
        "print(df[df['Forza_N'].isna()].head())\n",
        "\n",
        "print(\"\\nDataFrame finale con forze imputate (stesse 5 righe):\")\n",
        "print(df_finale.loc[df_senza_forza.index].head())\n",
        "\n",
        "# Salviamo il nuovo CSV completo\n",
        "output_filename = 'dati_con_forze_imputate.csv'\n",
        "df_finale.to_csv(output_filename, index=False)\n",
        "print(f\"\\nDataset completo salvato come '{output_filename}'\")"
      ],
      "metadata": {
        "id": "ZBYhGiUId25r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##NEW\n",
        "# =============================================================================\n",
        "# CELLA 5: STUDIO ESPLORATIVO + ESPORTAZIONE/DOWNLOAD\n",
        "# =============================================================================\n",
        "# Requisiti: df_unet_geometric, df_unet_hu gi√† creati (CELLE 3‚Äì4)\n",
        "# Opzionale (se hai gi√† fatto il merge forza altrove): df_forza_maurizio\n",
        "\n",
        "import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "\n",
        "plt.close('all')\n",
        "sns.set(context=\"notebook\", style=\"whitegrid\")\n",
        "\n",
        "# ---- 1) Build preview dataframe (join geom + Hu; attacca Forza se disponibile) ----\n",
        "df_unet = pd.merge(df_unet_geometric, df_unet_hu, on=\"NumeroForo\", how=\"inner\")\n",
        "\n",
        "if \"df_forza_maurizio\" in globals() and isinstance(df_forza_maurizio, pd.DataFrame):\n",
        "    tmp_forze = df_forza_maurizio.copy()\n",
        "    if \"NumeroForo\" in tmp_forze.columns:\n",
        "        tmp_forze[\"NumeroForo\"] = (\n",
        "            tmp_forze[\"NumeroForo\"].astype(str).str.extract(r\"(\\d+)\").astype(float)\n",
        "        )\n",
        "        tmp_forze[\"NumeroForo\"] = tmp_forze[\"NumeroForo\"].astype(\"Int64\")\n",
        "    df_master_preview = df_unet.merge(\n",
        "        tmp_forze[[\"NumeroForo\",\"Forza_N\"]].drop_duplicates(\"NumeroForo\"),\n",
        "        on=\"NumeroForo\", how=\"left\"\n",
        "    )\n",
        "else:\n",
        "    df_master_preview = df_unet.copy()\n",
        "    if \"Forza_N\" not in df_master_preview.columns:\n",
        "        df_master_preview[\"Forza_N\"] = np.nan  # slot per coerenza viste\n",
        "\n",
        "# Colonne numeriche principali da studiare (adatta liberamente)\n",
        "num_cols = [c for c in [\n",
        "    \"AreaDelaminata_unet\",\"Dmax_unet\",\"DF_unet\",\"ShapeFactor_unet\",\n",
        "    \"Hu_1\",\"Hu_2\",\"Hu_3\",\"Hu_4\",\"Hu_5\",\"Hu_6\",\"Hu_7\",\"Forza_N\"\n",
        "] if c in df_master_preview.columns]\n",
        "\n",
        "print(f\"Rows in preview: {len(df_master_preview)} | numeric cols: {len(num_cols)}\")\n",
        "\n",
        "# ---- 2) Distributions (hist+KDE) ----\n",
        "for c in num_cols:\n",
        "    fig = plt.figure(figsize=(6,4))\n",
        "    ax = sns.histplot(df_master_preview[c].dropna(), kde=True)\n",
        "    ax.set_title(f\"Distribution ‚Äî {c}\")\n",
        "    ax.set_xlabel(c); ax.set_ylabel(\"count\")\n",
        "    plt.show()\n",
        "\n",
        "# ---- 3) 2-D pairwise (quick) ----\n",
        "# Hint: subset per velocit√† se vuoi: vars=num_cols[:6]\n",
        "if len(num_cols) >= 2:\n",
        "    g = sns.pairplot(df_master_preview[num_cols].dropna(), corner=True, diag_kind=\"hist\")\n",
        "    g.fig.suptitle(\"Pairwise (subset numeric)\", y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "# ---- 4) Correlation heatmap ----\n",
        "if len(num_cols) >= 2:\n",
        "    corr = df_master_preview[num_cols].corr(numeric_only=True)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    ax = sns.heatmap(corr, annot=False, cmap=\"viridis\", square=True)\n",
        "    ax.set_title(\"Correlation (numeric)\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# ---- 5) ‚ÄúTime series‚Äù semplice indicizzata per foro (giusto per scorrere valori) ----\n",
        "if \"Forza_N\" in df_master_preview.columns:\n",
        "    df_plot = df_master_preview.sort_values(\"NumeroForo\")\n",
        "    plt.figure(figsize=(8,3.5))\n",
        "    plt.plot(df_plot[\"NumeroForo\"], df_plot[\"Forza_N\"], marker=\".\", linestyle=\"-\")\n",
        "    plt.title(\"Forza_N vs NumeroForo\")\n",
        "    plt.xlabel(\"NumeroForo\"); plt.ylabel(\"Forza_N\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# ---- 6) Salvataggi + download ----\n",
        "OUT_DIR = Path(\"/content\"); OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "p_geom = OUT_DIR/\"unet_geometria.csv\"\n",
        "p_hu   = OUT_DIR/\"unet_hu.csv\"\n",
        "p_prev = OUT_DIR/\"dataset_master_preview.csv\"\n",
        "\n",
        "df_unet_geometric.to_csv(p_geom, index=False)\n",
        "df_unet_hu.to_csv(p_hu, index=False)\n",
        "df_master_preview.to_csv(p_prev, index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" -\", p_geom)\n",
        "print(\" -\", p_hu)\n",
        "print(\" -\", p_prev)\n",
        "\n",
        "# trigger download (Colab)\n",
        "for p in [p_geom, p_hu, p_prev]:\n",
        "    try:\n",
        "        files.download(str(p))\n",
        "    except Exception as e:\n",
        "        print(f\"(Info) Download programmatico non riuscito per {p.name}. Scaricalo dal pannello Files. Dettagli: {e}\")\n",
        "\n",
        "# ---- 7) (Opzionale) Report HTML EDA con ydata-profiling ----\n",
        "# NB: generare il report pu√≤ richiedere un po' di tempo per molte colonne/righe\n",
        "try:\n",
        "    %pip -q install ydata-profiling\n",
        "    from ydata_profiling import ProfileReport\n",
        "    profile = ProfileReport(df_master_preview[num_cols + [\"NumeroForo\"]], title=\"UNet + Forze ‚Äî Profiling\")\n",
        "    html_path = OUT_DIR/\"eda_report_unet_forze.html\"\n",
        "    profile.to_file(html_path)\n",
        "    print(\"EDA report:\", html_path)\n",
        "    try:\n",
        "        files.download(str(html_path))\n",
        "    except Exception as e:\n",
        "        print(f\"(Info) Download HTML non auto-avviato: {e}\")\n",
        "except Exception as e:\n",
        "    print(\"(Opzionale) ydata-profiling non installato/errore:\", e)\n"
      ],
      "metadata": {
        "id": "uQfdfF6gfMtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Analisi Grafica della Coerenza dei Valori di Forza Imputati\n",
        "'''\n",
        "Per questa analisi, utilizzer√≤ il file originale e quello nuovo, `/content/dati_con_forze_imputate.csv.csv`.\n",
        "\n",
        "```python'''\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Impostazioni grafiche per una migliore leggibilit√†\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# 1. Caricamento dei due dataset per il confronto\n",
        "# ----------------------------------------------------\n",
        "try:\n",
        "    df_original = pd.read_csv('/content/dataset_master_finalissimo.csv')\n",
        "    df_imputato = pd.read_csv('/content/dati_con_forze_imputate.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Assicurati che entrambi i file '/content/dataset_master_finalissimo.csv' e '/content/dati_con_forze_imputate.csv.csv' siano presenti.\")\n",
        "    exit()\n",
        "\n",
        "# Aggiungiamo un flag per distinguere i dati originali, imputati e reali\n",
        "df_imputato['Stato'] = 'Reale'\n",
        "# Identifichiamo gli indici dove la forza era mancante nel file originale\n",
        "indici_mancanti = df_original[df_original['Forza_N'].isna()].index\n",
        "df_imputato.loc[indici_mancanti, 'Stato'] = 'Imputato'\n",
        "\n",
        "print(\"Dati caricati e pronti per la visualizzazione.\")\n",
        "\n",
        "# 2. Grafico 1: Andamento Generale della Forza\n",
        "# ----------------------------------------------------\n",
        "# Questo grafico ci mostra la sequenza completa delle forze, evidenziando i punti che abbiamo predetto.\n",
        "# Ci aspettiamo che i punti rossi (imputati) si inseriscano in modo fluido nell'andamento generale.\n",
        "\n",
        "plt.figure(figsize=(18, 8))\n",
        "sns.scatterplot(\n",
        "    data=df_imputato,\n",
        "    x='NumeroForo',\n",
        "    y='Forza_N',\n",
        "    hue='Stato',\n",
        "    palette={'Reale': 'blue', 'Imputato': 'red'},\n",
        "    s=20, # Dimensione dei punti\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title('Andamento Generale della Forza (Valori Reali vs Imputati)', fontsize=16)\n",
        "plt.xlabel('Numero del Foro (Sequenza Esperimento)', fontsize=12)\n",
        "plt.ylabel('Forza (N)', fontsize=12)\n",
        "plt.legend(title='Stato del Dato')\n",
        "plt.show()\n",
        "\n",
        "# 3. Grafico 2: Confronto delle Distribuzioni (Violin Plot)\n",
        "# -----------------------------------------------------------------\n",
        "# Questo grafico √® FONDAMENTALE. Mostra la distribuzione dei valori di forza per ogni tipo di materiale.\n",
        "# Vogliamo vedere se la distribuzione dei valori imputati (in rosso) si sovrappone bene a quella\n",
        "# dei valori reali (in blu) all'interno dello stesso gruppo di materiale.\n",
        "\n",
        "# Ordiniamo i materiali per una visualizzazione logica\n",
        "material_order = sorted(df_imputato['TipoMateriale'].unique())\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "sns.violinplot(\n",
        "    data=df_imputato,\n",
        "    x='TipoMateriale',\n",
        "    y='Forza_N',\n",
        "    hue='Stato',\n",
        "    split=True, # Divide il violino a met√† per un confronto diretto\n",
        "    inner='quart', # Mostra i quartili all'interno\n",
        "    palette={'Reale': 'skyblue', 'Imputato': 'salmon'},\n",
        "    order=material_order\n",
        ")\n",
        "plt.title('Distribuzione delle Forze per Tipo di Materiale', fontsize=16)\n",
        "plt.xlabel('Tipo di Materiale', fontsize=12)\n",
        "plt.ylabel('Forza (N)', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Stato del Dato', loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Grafico 3: Relazione Forza vs. Area Delaminata\n",
        "# -----------------------------------------------------------------\n",
        "# Verifichiamo se i punti imputati seguono la stessa \"nuvola\" di punti dei dati reali quando\n",
        "# plottiamo la forza contro una delle feature pi√π importanti.\n",
        "\n",
        "plt.figure(figsize=(18, 8))\n",
        "sns.scatterplot(\n",
        "    data=df_imputato,\n",
        "    x='AreaDelaminata_unet',\n",
        "    y='Forza_N',\n",
        "    hue='Stato',\n",
        "    style='TipoMateriale', # Usiamo stili diversi per ogni materiale\n",
        "    palette={'Reale': 'black', 'Imputato': 'red'},\n",
        "    s=50,\n",
        "    alpha=0.8\n",
        ")\n",
        "plt.title('Correlazione Forza vs. Area Delaminata (Valori Reali vs Imputati)', fontsize=16)\n",
        "plt.xlabel('Area Delaminata (mm¬≤)', fontsize=12)\n",
        "plt.ylabel('Forza (N)', fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9UFLWmQEfMYR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
